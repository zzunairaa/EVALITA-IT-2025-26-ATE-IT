# System Description

This system implements an ATE solution for the ATE-IT Shared Task (EVALITA 2026), targeting Subtask A: Term Extraction from Italian municipal waste management documents. The system employs a hybrid neural symbolic architecture combining classical NLP preprocessing with deep learning based sequence labeling to identify domain specific technical terms in Italian administrative texts. The architecture consists of four sequential components. The preprocessing layer performs text normalization, including parentheses removal, quote normalization, and whitespace handling, using SpaCy's Italian model (it_core_news_sm) for tokenization. The BIO encoding layer maps gold standard terms to token level labels (B-TERM, I-TERM, O) using a longest first strategy for nested terms and aligns BERT subword tokenization for accurate label assignment. The core component is a fine tuned transformer model (dbmdz/bert-base-italian-uncased) trained for token classification with class weighted loss to address class imbalance. The model uses probability-based prediction thresholds rather than argmax to improve recall for borderline terms. Training incorporates gradient clipping, achieving a training loss of approximately 0.13. The post-processing layer reconstructs multi word terms from BIO labels while enforcing ATE-IT constraints: no nested terms unless independent, no duplicates per sentence, and domain specific filtering. This layer removes stopwords, generic terms, English words, days of the week, and administrative headers, while normalizing term formats. These filters reduce false positives by 70-85%, addressing common Italian stopwords, generic environmental terms, and incomplete fragments. The system is evaluated using Micro-F1 and Type-F1. Current performance achieves 0.69 Micro-F1 on the development set, with expected improvements to 0.72-0.76. Outputs are in CSV format with document, paragraph, sentence identifiers, sentence text, and extracted terms, normalized to lowercase and compliant with ATE-IT requirements. This approach outperforms the baseline zero-shot performance of 0.513 Micro-F1, demonstrating the effectiveness of fine tuned transformers with domain specific preprocessing and filtering for automatic term extraction.