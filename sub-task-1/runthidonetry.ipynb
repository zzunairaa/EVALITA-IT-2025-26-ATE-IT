{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATE-IT Shared Task (EVALITA 2026) - Subtask A: Term Extraction\n",
    "\n",
    "**Author**: Senior NLP Research Implementation  \n",
    "**Task**: Automatic Term Extraction (ATE) for Italian Municipal Waste Management Documents  \n",
    "**Shared Task**: EVALITA 2026 - ATE-IT  \n",
    "**Subtask**: A - Term Extraction\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Description & Research Context\n",
    "\n",
    "### 1.1 Task Overview\n",
    "\n",
    "This notebook implements a state-of-the-art **Automatic Term Extraction (ATE)** system for the ATE-IT Shared Task, specifically targeting **Subtask A – Term Extraction**. The task requires identifying domain-specific technical terms related to municipal waste management in Italian administrative documents.\n",
    "\n",
    "**Key Challenge**: Unlike general Named Entity Recognition (NER), ATE focuses on domain-specific terminology that may not follow standard entity patterns, requiring specialized approaches for multi-word expression (MWE) detection and domain adaptation.\n",
    "\n",
    "### 1.2 Research Approach\n",
    "\n",
    "This system implements a **hybrid neural-symbolic architecture** combining:\n",
    "\n",
    "1. **Classical NLP Pipeline**:\n",
    "   - Linguistic preprocessing with SpaCy Italian models\n",
    "   - Rule-based text normalization\n",
    "   - Domain-aware tokenization\n",
    "\n",
    "2. **Deep Learning Component**:\n",
    "   - Fine-tuned Italian BERT models (dbmdz/bert-base-italian-uncased)\n",
    "   - Sequence labeling with BIO tagging scheme\n",
    "   - Probability-based prediction for improved recall\n",
    "\n",
    "3. **Post-processing & Constraints**:\n",
    "   - ATE-IT constraint enforcement (no nested terms, no duplicates)\n",
    "   - Term reconstruction from token-level predictions\n",
    "   - Domain-specific filtering\n",
    "\n",
    "**Research Methodology**: This implementation uses transformers **strictly as supervised sequence-labeling models** following the NER paradigm. No LLM prompting, generative inference, or zero-shot approaches are employed, ensuring reproducibility and interpretability.\n",
    "\n",
    "### 1.3 Task Specifications\n",
    "\n",
    "**Input**: Italian sentences from municipal waste management documents  \n",
    "**Output**: Domain-specific terms (single words or multi-word expressions)\n",
    "\n",
    "**Term Characteristics**:\n",
    "- Single-word terms: e.g., `\"raccolta\"` (collection)\n",
    "- Multi-word expressions: e.g., `\"servizio di raccolta dei rifiuti\"` (waste collection service)\n",
    "- Domain-specific: Must relate to municipal waste management\n",
    "\n",
    "**Constraints** (per ATE-IT guidelines):\n",
    "- No nested terms unless they appear standalone in the text\n",
    "- No duplicate terms per sentence\n",
    "- Terms must be domain-specific to municipal waste management\n",
    "- Case-insensitive matching (lowercase normalization)\n",
    "\n",
    "### 1.4 Evaluation Metrics\n",
    "\n",
    "The system is evaluated using two complementary metrics:\n",
    "\n",
    "1. **Micro-F1**: Term-level performance (precision, recall, F1) - compares sets of terms per sentence\n",
    "2. **Type-F1**: Unique term type performance (precision, recall, F1) - compares unique term types across dataset\n",
    "\n",
    "**Baseline Performance** (Gemini-2.5-Flash, zero-shot):\n",
    "- Micro-F1: 0.513\n",
    "- Type-F1: 0.470\n",
    "- Type-Recall: 0.636 (target to exceed)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. System Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Input: Italian Sentences (CSV format)                  │\n",
    "└────────────────────┬────────────────────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Preprocessing Layer                                    │\n",
    "│  - Text cleaning & normalization                        │\n",
    "│  - SpaCy Italian tokenization                           │\n",
    "│  - Lowercase conversion                                 │\n",
    "└────────────────────┬────────────────────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  BIO Encoding Layer                                     │\n",
    "│  - Gold term → BIO label mapping                        │\n",
    "│  - Handle nested terms (longest-first strategy)        │\n",
    "│  - Subword tokenizer alignment                          │\n",
    "└────────────────────┬────────────────────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Transformer Model (Italian BERT)                       │\n",
    "│  - Fine-tuned for token classification                  │\n",
    "│  - Class-weighted loss (handles imbalance)              │\n",
    "│  - Probability-based prediction (improves recall)       │\n",
    "└────────────────────┬────────────────────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Term Reconstruction Layer                              │\n",
    "│  - BIO → Multi-word terms                               │\n",
    "│  - Constraint enforcement                               │\n",
    "│  - Duplicate removal                                    │\n",
    "└────────────────────┬────────────────────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Output: JSON format with term lists                   │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Innovations\n",
    "\n",
    "1. **Probability-Based Prediction**: Uses probability thresholds instead of argmax to capture borderline terms, significantly improving Type-Recall\n",
    "2. **Class-Weighted Loss**: Addresses class imbalance (O tokens dominate TERM tokens) through inverse frequency weighting\n",
    "3. **Robust Subword Alignment**: Properly handles BERT subword tokenization to maintain token-level accuracy\n",
    "4. **Constraint-Aware Reconstruction**: Enforces ATE-IT constraints while maximizing recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell first to install all required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fashad/Desktop/ate-te-task1/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers already installed\n",
      "torch already installed\n",
      "Installing scikit-learn...\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "scikit-learn installed\n",
      "pandas already installed\n",
      "numpy already installed\n",
      "spacy already installed\n",
      "tqdm already installed\n",
      "datasets already installed\n",
      "seqeval already installed\n",
      "\n",
      "Downloading SpaCy Italian model...\n",
      "Italian SpaCy model already downloaded\n",
      "\n",
      "All dependencies installed successfully!\n",
      "\n",
      " Libraries loaded successfully\n",
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "print(\"Installing required packages...\")\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of packages to install\n",
    "packages = [\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"scikit-learn\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"spacy\",\n",
    "    \"tqdm\",\n",
    "    \"datasets\",\n",
    "    \"seqeval\"\n",
    "]\n",
    "\n",
    "# Install packages\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"{package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"{package} installed\")\n",
    "\n",
    "# Download SpaCy Italian model\n",
    "print(\"\\nDownloading SpaCy Italian model...\")\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"it_core_news_sm\")\n",
    "    print(\"Italian SpaCy model already downloaded\")\n",
    "except OSError:\n",
    "    print(\"Downloading it_core_news_sm...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"it_core_news_sm\"])\n",
    "    print(\"Italian SpaCy model downloaded\")\n",
    "\n",
    "print(\"\\nAll dependencies installed successfully!\\n\")\n",
    "\n",
    "# Transformers and PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# SpaCy for tokenization\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"it_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Italian SpaCy model not found. Please run: python -m spacy download it_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Datasets and evaluation\n",
    "from datasets import Dataset as HFDataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score as sklearn_f1\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\" Libraries loaded successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries loaded successfully\n",
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# SpaCy for tokenization\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"it_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Italian SpaCy model not found. Please run: python -m spacy download it_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Datasets and evaluation\n",
    "from datasets import Dataset as HFDataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score as sklearn_f1\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\" Libraries loaded successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n",
      "Training set: 3423 rows\n",
      "\n",
      "Loading development set...\n",
      "Development set: 779 rows\n",
      "\n",
      "Dataset columns: ['document_id', 'paragraph_id', 'sentence_id', 'sentence_text', 'term']\n",
      "\n",
      "First few training examples:\n",
      "       document_id  paragraph_id  sentence_id  \\\n",
      "0  doc_agropoli_09             1            0   \n",
      "1  doc_agropoli_09             1            1   \n",
      "2  doc_agropoli_09             1            2   \n",
      "3  doc_agropoli_09             1            3   \n",
      "4  doc_agropoli_09             1            4   \n",
      "5  doc_agropoli_09             3            0   \n",
      "6  doc_agropoli_09             3            1   \n",
      "7  doc_agropoli_09             3            1   \n",
      "8  doc_agropoli_09             3            1   \n",
      "9  doc_agropoli_09             3            1   \n",
      "\n",
      "                                       sentence_text                      term  \n",
      "0                   Unione dei Comuni “Alto Cilento”                       NaN  \n",
      "1  Agropoli – Capaccio Paestum - Cicerale – Giung...                       NaN  \n",
      "2                      Centrale Unica di Committenza                       NaN  \n",
      "3    Piazza della Repubblica, 384043 - Agropoli (SA)                       NaN  \n",
      "4  mail:altocilento.cuc@gmail.com - pec: altocile...                       NaN  \n",
      "5                                     BANDO DI GARA                        NaN  \n",
      "6  AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACC...                  raccolta  \n",
      "7  AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACC...                  recupero  \n",
      "8  AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACC...      servizio di raccolta  \n",
      "9  AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACC...   servizio di spazzamento  \n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "TRAIN_PATH = r\"subtask_a_train.csv\"\n",
    "DEV_PATH = r\"subtask_a_dev.csv\"\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading training set...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"Training set: {len(train_df)} rows\")\n",
    "\n",
    "print(\"\\nLoading development set...\")\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "print(f\"Development set: {len(dev_df)} rows\")\n",
    "\n",
    "print(\"\\nDataset columns:\", train_df.columns.tolist())\n",
    "print(\"\\nFirst few training examples:\")\n",
    "print(train_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET STATISTICS\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  Total rows: 3423\n",
      "  Rows with terms: 2218\n",
      "  Unique sentences: 2308\n",
      "  Unique documents: 63\n",
      "\n",
      "Development Set:\n",
      "  Total rows: 779\n",
      "  Rows with terms: 451\n",
      "  Unique sentences: 577\n",
      "  Unique documents: 60\n",
      "\n",
      "Term Statistics (Training):\n",
      "  Total terms: 2218\n",
      "  Unique term types: 713\n",
      "  Average term length (words): 2.22\n",
      "  Max term length (words): 21\n",
      "  Min term length (words): 1\n",
      "\n",
      "Term Statistics (Development):\n",
      "  Total terms: 451\n",
      "  Unique term types: 242\n",
      "  Average term length (words): 2.31\n",
      "  Max term length (words): 21\n",
      "  Min term length (words): 1\n",
      "\n",
      "Example sentence with multiple terms:\n",
      "  Sentence: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', ' recupero', ' servizio di raccolta', ' servizio di spazzamento', ' smaltimento', ' trasporto']\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training set\n",
    "train_with_terms = train_df[train_df['term'].notna() & (train_df['term'].str.strip() != '')]\n",
    "train_unique_sentences = train_df[['document_id', 'paragraph_id', 'sentence_id']].drop_duplicates()\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Total rows: {len(train_df)}\")\n",
    "print(f\"  Rows with terms: {len(train_with_terms)}\")\n",
    "print(f\"  Unique sentences: {len(train_unique_sentences)}\")\n",
    "print(f\"  Unique documents: {train_df['document_id'].nunique()}\")\n",
    "\n",
    "# Development set\n",
    "dev_with_terms = dev_df[dev_df['term'].notna() & (dev_df['term'].str.strip() != '')]\n",
    "dev_unique_sentences = dev_df[['document_id', 'paragraph_id', 'sentence_id']].drop_duplicates()\n",
    "\n",
    "print(f\"\\nDevelopment Set:\")\n",
    "print(f\"  Total rows: {len(dev_df)}\")\n",
    "print(f\"  Rows with terms: {len(dev_with_terms)}\")\n",
    "print(f\"  Unique sentences: {len(dev_unique_sentences)}\")\n",
    "print(f\"  Unique documents: {dev_df['document_id'].nunique()}\")\n",
    "\n",
    "# Term statistics\n",
    "train_terms = train_with_terms['term'].str.strip().str.lower()\n",
    "dev_terms = dev_with_terms['term'].str.strip().str.lower()\n",
    "\n",
    "print(f\"\\nTerm Statistics (Training):\")\n",
    "print(f\"  Total terms: {len(train_terms)}\")\n",
    "print(f\"  Unique term types: {train_terms.nunique()}\")\n",
    "print(f\"  Average term length (words): {train_terms.str.split().str.len().mean():.2f}\")\n",
    "print(f\"  Max term length (words): {train_terms.str.split().str.len().max()}\")\n",
    "print(f\"  Min term length (words): {train_terms.str.split().str.len().min()}\")\n",
    "\n",
    "print(f\"\\nTerm Statistics (Development):\")\n",
    "print(f\"  Total terms: {len(dev_terms)}\")\n",
    "print(f\"  Unique term types: {dev_terms.nunique()}\")\n",
    "print(f\"  Average term length (words): {dev_terms.str.split().str.len().mean():.2f}\")\n",
    "print(f\"  Max term length (words): {dev_terms.str.split().str.len().max()}\")\n",
    "print(f\"  Min term length (words): {dev_terms.str.split().str.len().min()}\")\n",
    "\n",
    "# Example sentences with multiple terms\n",
    "print(f\"\\nExample sentence with multiple terms:\")\n",
    "example = train_df[train_df.groupby(['document_id', 'paragraph_id', 'sentence_id'])['sentence_id'].transform('count') > 1]\n",
    "if len(example) > 0:\n",
    "    sample_sentence = example[['document_id', 'paragraph_id', 'sentence_id']].drop_duplicates().iloc[0]\n",
    "    sentence_data = train_df[\n",
    "        (train_df['document_id'] == sample_sentence['document_id']) &\n",
    "        (train_df['paragraph_id'] == sample_sentence['paragraph_id']) &\n",
    "        (train_df['sentence_id'] == sample_sentence['sentence_id'])\n",
    "    ]\n",
    "    print(f\"  Sentence: {sentence_data.iloc[0]['sentence_text']}\")\n",
    "    print(f\"  Terms: {sentence_data['term'].dropna().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and Tokenization\n",
    "\n",
    "The preprocessing pipeline:\n",
    "1. **Lowercase** the text (no lemmatization or stemming)\n",
    "2. **Clean** brackets, excessive punctuation\n",
    "3. **Tokenize** using SpaCy Italian model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test:\n",
      "Original: Il servizio di raccolta dei rifiuti [urbani] è gestito dalla Buttol Srl.\n",
      "Cleaned:  il servizio di raccolta dei rifiuti urbani è gestito dalla buttol srl.\n",
      "Tokens:   ['il', 'servizio', 'di', 'raccolta', 'dei', 'rifiuti', 'urbani', 'è', 'gestito', 'dalla', 'buttol', 'srl', '.']\n",
      "Token count: 13\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text: lowercase and remove excessive brackets/punctuation.\n",
    "    Keeps basic punctuation for tokenization.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Lowercase (no lemmatization/stemming as per requirements)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Clean excessive brackets and special characters\n",
    "    # Remove square brackets but keep content\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n",
    "    text = re.sub(r'\\{([^\\}]*)\\}', r'\\1', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenize_with_spacy(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text using SpaCy Italian model.\n",
    "    Returns list of token strings.\n",
    "    \"\"\"\n",
    "    if not text or text == '':\n",
    "        return []\n",
    "    \n",
    "    if nlp is None:\n",
    "        # Fallback to simple whitespace tokenization if SpaCy not available\n",
    "        return text.split()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Test preprocessing\n",
    "test_text = \"Il servizio di raccolta dei rifiuti [urbani] è gestito dalla Buttol Srl.\"\n",
    "cleaned = clean_text(test_text)\n",
    "tokens = tokenize_with_spacy(cleaned)\n",
    "\n",
    "print(\"Preprocessing test:\")\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Cleaned:  {cleaned}\")\n",
    "print(f\"Tokens:   {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BIO Encoding\n",
    "\n",
    "Convert gold terms into BIO (Beginning-Inside-Outside) labels for each token in a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO Encoding Test:\n",
      "Sentence: Il servizio di raccolta dei rifiuti è gestito.\n",
      "Terms: ['servizio di raccolta dei rifiuti', 'raccolta', 'rifiuti']\n",
      "\n",
      "Token -> Label mapping:\n",
      "  il              -> O\n",
      "  servizio        -> B-TERM\n",
      "  di              -> I-TERM\n",
      "  raccolta        -> I-TERM\n",
      "  dei             -> I-TERM\n",
      "  rifiuti         -> I-TERM\n",
      "  è               -> O\n",
      "  gestito         -> O\n",
      "  .               -> O\n"
     ]
    }
   ],
   "source": [
    "def find_term_in_tokens(term: str, tokens: List[str]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Find all occurrences of a term in a tokenized sentence.\n",
    "    Returns list of (start_idx, end_idx) tuples (end exclusive).\n",
    "    \n",
    "    Handles multi-word terms by matching sequences of tokens.\n",
    "    \"\"\"\n",
    "    if not term or pd.isna(term):\n",
    "        return []\n",
    "    \n",
    "    term = str(term).strip().lower()\n",
    "    term_tokens = tokenize_with_spacy(term)\n",
    "    \n",
    "    if len(term_tokens) == 0:\n",
    "        return []\n",
    "    \n",
    "    matches = []\n",
    "    for i in range(len(tokens) - len(term_tokens) + 1):\n",
    "        if tokens[i:i+len(term_tokens)] == term_tokens:\n",
    "            matches.append((i, i + len(term_tokens)))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "def create_bio_labels(sentence_text: str, terms: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Create BIO labels for a sentence given the gold terms.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of token strings\n",
    "        labels: List of BIO labels ('B-TERM', 'I-TERM', 'O')\n",
    "    \"\"\"\n",
    "    # Clean and tokenize sentence\n",
    "    cleaned_text = clean_text(sentence_text)\n",
    "    tokens = tokenize_with_spacy(cleaned_text)\n",
    "    \n",
    "    if len(tokens) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    # Initialize all labels as 'O' (Outside)\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Process each term\n",
    "    valid_terms = [t for t in terms if t and pd.notna(t) and str(t).strip() != '']\n",
    "    \n",
    "    # Process terms sorted by length (longest first) to handle nested terms correctly\n",
    "    sorted_terms = sorted(valid_terms, key=lambda x: len(tokenize_with_spacy(str(x))), reverse=True)\n",
    "    \n",
    "    for term in sorted_terms:\n",
    "        matches = find_term_in_tokens(term, tokens)\n",
    "        for start, end in matches:\n",
    "            # Only label if span is not already labeled\n",
    "            if all(labels[i] == 'O' for i in range(start, end)):\n",
    "                # Label first token as B-TERM\n",
    "                labels[start] = 'B-TERM'\n",
    "                # Label remaining tokens as I-TERM\n",
    "                for i in range(start + 1, end):\n",
    "                    labels[i] = 'I-TERM'\n",
    "    \n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "# Test BIO encoding\n",
    "test_sentence = \"Il servizio di raccolta dei rifiuti è gestito.\"\n",
    "test_terms = [\"servizio di raccolta dei rifiuti\", \"raccolta\", \"rifiuti\"]\n",
    "\n",
    "tokens, labels = create_bio_labels(test_sentence, test_terms)\n",
    "\n",
    "print(\"BIO Encoding Test:\")\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Terms: {test_terms}\")\n",
    "print(\"\\nToken -> Label mapping:\")\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f\"  {token:15s} -> {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " clean_text function is available\n",
      " tokenize_with_spacy function is available\n",
      " find_term_in_tokens function is available\n",
      " create_bio_labels function is available\n",
      " Defined prepare_data_for_training function\n",
      "\n",
      "============================================================\n",
      "Creating train_data and dev_data...\n",
      "============================================================\n",
      "\n",
      "Preparing training data...\n",
      "Prepared 2308 training sentences\n",
      "\n",
      "Preparing development data...\n",
      "Prepared 577 development sentences\n",
      "============================================================\n",
      "\n",
      "️  tokenizer not found. Loading tokenizer...\n",
      "Using default model: dbmdz/bert-base-italian-uncased\n",
      " tokenizer loaded: dbmdz/bert-base-italian-uncased\n",
      "\n",
      "️  LABEL_TO_ID not found. Creating label mappings...\n",
      " Created label mappings: {'O': 0, 'B-TERM': 1, 'I-TERM': 2}\n",
      "\n",
      "============================================================\n",
      "All required variables are ready!\n",
      "============================================================\n",
      "Converting training data to HuggingFace format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2308/2308 [00:00<00:00, 15767.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2308\n",
      "\n",
      "Converting development data to HuggingFace format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 577/577 [00:00<00:00, 18383.35 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development dataset size: 577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data to HuggingFace format for training\n",
    "# This cell automatically creates all required variables and functions if they don't exist\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Define all required helper functions (if not already defined)\n",
    "# ============================================================================\n",
    "\n",
    "# Define clean_text function if not exists\n",
    "try:\n",
    "    _ = clean_text\n",
    "    print(\" clean_text function is available\")\n",
    "except NameError:\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Clean text: lowercase and remove excessive brackets/punctuation.\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        text = str(text).strip().lower()\n",
    "        text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n",
    "        text = re.sub(r'\\{([^\\}]*)\\}', r'\\1', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    print(\" Defined clean_text function\")\n",
    "\n",
    "# Define tokenize_with_spacy function if not exists\n",
    "try:\n",
    "    _ = tokenize_with_spacy\n",
    "    print(\" tokenize_with_spacy function is available\")\n",
    "except NameError:\n",
    "    def tokenize_with_spacy(text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text using SpaCy Italian model.\"\"\"\n",
    "        if not text or text == '':\n",
    "            return []\n",
    "        if nlp is None:\n",
    "            return text.split()\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        return tokens\n",
    "    print(\" Defined tokenize_with_spacy function\")\n",
    "\n",
    "# Define find_term_in_tokens function if not exists\n",
    "try:\n",
    "    _ = find_term_in_tokens\n",
    "    print(\" find_term_in_tokens function is available\")\n",
    "except NameError:\n",
    "    def find_term_in_tokens(term: str, tokens: List[str]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Find all occurrences of a term in a tokenized sentence.\"\"\"\n",
    "        if not term or pd.isna(term):\n",
    "            return []\n",
    "        term = str(term).strip().lower()\n",
    "        term_tokens = tokenize_with_spacy(term)\n",
    "        if len(term_tokens) == 0:\n",
    "            return []\n",
    "        matches = []\n",
    "        for i in range(len(tokens) - len(term_tokens) + 1):\n",
    "            if tokens[i:i+len(term_tokens)] == term_tokens:\n",
    "                matches.append((i, i + len(term_tokens)))\n",
    "        return matches\n",
    "    print(\" Defined find_term_in_tokens function\")\n",
    "\n",
    "# Define create_bio_labels function if not exists\n",
    "try:\n",
    "    _ = create_bio_labels\n",
    "    print(\" create_bio_labels function is available\")\n",
    "except NameError:\n",
    "    def create_bio_labels(sentence_text: str, terms: List[str]) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Create BIO labels for a sentence given the gold terms.\"\"\"\n",
    "        cleaned_text = clean_text(sentence_text)\n",
    "        tokens = tokenize_with_spacy(cleaned_text)\n",
    "        if len(tokens) == 0:\n",
    "            return [], []\n",
    "        labels = ['O'] * len(tokens)\n",
    "        valid_terms = [t for t in terms if t and pd.notna(t) and str(t).strip() != '']\n",
    "        sorted_terms = sorted(valid_terms, key=lambda x: len(tokenize_with_spacy(str(x))), reverse=True)\n",
    "        for term in sorted_terms:\n",
    "            matches = find_term_in_tokens(term, tokens)\n",
    "            for start, end in matches:\n",
    "                if all(labels[i] == 'O' for i in range(start, end)):\n",
    "                    labels[start] = 'B-TERM'\n",
    "                    for i in range(start + 1, end):\n",
    "                        labels[i] = 'I-TERM'\n",
    "        return tokens, labels\n",
    "    print(\" Defined create_bio_labels function\")\n",
    "\n",
    "# Define prepare_data_for_training function if not exists\n",
    "try:\n",
    "    _ = prepare_data_for_training\n",
    "    print(\" prepare_data_for_training function is available\")\n",
    "except NameError:\n",
    "    def prepare_data_for_training(df: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Prepare data from DataFrame into format suitable for training.\"\"\"\n",
    "        data = []\n",
    "        sentence_groups = df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "        for (doc_id, para_id, sent_id), group in sentence_groups:\n",
    "            sentence_text = group.iloc[0]['sentence_text']\n",
    "            terms = group['term'].dropna().tolist()\n",
    "            terms = [str(t).strip() for t in terms if t and str(t).strip() != '']\n",
    "            tokens, labels = create_bio_labels(sentence_text, terms)\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "            data.append({\n",
    "                'document_id': doc_id,\n",
    "                'paragraph_id': para_id,\n",
    "                'sentence_id': sent_id,\n",
    "                'sentence_text': sentence_text,\n",
    "                'tokens': tokens,\n",
    "                'labels': labels,\n",
    "                'gold_terms': terms\n",
    "            })\n",
    "        return data\n",
    "    print(\" Defined prepare_data_for_training function\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Ensure all required variables are defined\n",
    "# ============================================================================\n",
    "\n",
    "# Check and create train_data and dev_data if needed\n",
    "try:\n",
    "    _ = train_data\n",
    "    _ = dev_data\n",
    "    print(\" train_data and dev_data are already available\")\n",
    "except NameError:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Creating train_data and dev_data...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Make sure train_df and dev_df exist\n",
    "    try:\n",
    "        _ = train_df\n",
    "        _ = dev_df\n",
    "    except NameError:\n",
    "        raise NameError(\n",
    "            \"train_df and dev_df are not defined. \"\n",
    "            \"Please run the cell that loads the datasets first (Section 3: Load Datasets.\"\n",
    "        )\n",
    "    \n",
    "    # Create train_data and dev_data\n",
    "    print(\"\\nPreparing training data...\")\n",
    "    train_data = prepare_data_for_training(train_df)\n",
    "    print(f\"Prepared {len(train_data)} training sentences\")\n",
    "    \n",
    "    print(\"\\nPreparing development data...\")\n",
    "    dev_data = prepare_data_for_training(dev_df)\n",
    "    print(f\"Prepared {len(dev_data)} development sentences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Check and ensure tokenizer is defined\n",
    "try:\n",
    "    _ = tokenizer\n",
    "    print(\" tokenizer is available\")\n",
    "except NameError:\n",
    "    print(\"\\n️  tokenizer not found. Loading tokenizer...\")\n",
    "    try:\n",
    "        _ = MODEL_NAME\n",
    "    except NameError:\n",
    "        MODEL_NAME = \"dbmdz/bert-base-italian-uncased\"\n",
    "        print(f\"Using default model: {MODEL_NAME}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\" tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "# Check and ensure LABEL_TO_ID is defined\n",
    "try:\n",
    "    _ = LABEL_TO_ID\n",
    "    print(\" LABEL_TO_ID is available\")\n",
    "except NameError:\n",
    "    print(\"\\n️  LABEL_TO_ID not found. Creating label mappings...\")\n",
    "    LABEL_LIST = ['O', 'B-TERM', 'I-TERM']\n",
    "    LABEL_TO_ID = {label: idx for idx, label in enumerate(LABEL_LIST)}\n",
    "    ID_TO_LABEL = {idx: label for idx, label in enumerate(LABEL_LIST)}\n",
    "    print(f\" Created label mappings: {LABEL_TO_ID}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All required variables are ready!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def prepare_huggingface_dataset(data: List[Dict]) -> HFDataset:\n",
    "    \"\"\"\n",
    "    Convert prepared data to HuggingFace Dataset format.\n",
    "    \"\"\"\n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples['tokens'],\n",
    "            is_split_into_words=True,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        for i, label_seq in enumerate(examples['labels']):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            aligned_labels = []\n",
    "            previous_word_idx = None\n",
    "            \n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    aligned_labels.append(-100)\n",
    "                elif word_idx == previous_word_idx:\n",
    "                    aligned_labels.append(-100)\n",
    "                else:\n",
    "                    aligned_labels.append(LABEL_TO_ID[label_seq[word_idx]])\n",
    "                previous_word_idx = word_idx\n",
    "            \n",
    "            labels.append(aligned_labels)\n",
    "        \n",
    "        tokenized_inputs['labels'] = labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset_dict = {\n",
    "        'tokens': [item['tokens'] for item in data],\n",
    "        'labels': [item['labels'] for item in data]\n",
    "    }\n",
    "    \n",
    "    hf_dataset = HFDataset.from_dict(dataset_dict)\n",
    "    hf_dataset = hf_dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=['tokens']\n",
    "    )\n",
    "    \n",
    "    return hf_dataset\n",
    "\n",
    "print(\"Converting training data to HuggingFace format...\")\n",
    "train_hf_dataset = prepare_huggingface_dataset(train_data)\n",
    "print(f\"Training dataset size: {len(train_hf_dataset)}\")\n",
    "\n",
    "print(\"\\nConverting development data to HuggingFace format...\")\n",
    "dev_hf_dataset = prepare_huggingface_dataset(dev_data)\n",
    "print(f\"Development dataset size: {len(dev_hf_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Prepared 2308 training sentences\n",
      "\n",
      "Preparing development data...\n",
      "Prepared 577 development sentences\n",
      "\n",
      "Example prepared data:\n",
      "Document ID: doc_agropoli_09\n",
      "Sentence: Unione dei Comuni “Alto Cilento”\n",
      "Tokens: ['unione', 'dei', 'comuni', '“', 'alto', 'cilento', '”']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Gold terms: []\n",
      "\n",
      "Label distribution (training): {'O': 35170, 'B-TERM': 2176, 'I-TERM': 2582}\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_training(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Prepare data from DataFrame into format suitable for training.\n",
    "    Groups by sentence and collects all terms per sentence.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Group by sentence\n",
    "    sentence_groups = df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "    \n",
    "    for (doc_id, para_id, sent_id), group in sentence_groups:\n",
    "        sentence_text = group.iloc[0]['sentence_text']\n",
    "        terms = group['term'].dropna().tolist()\n",
    "        \n",
    "        # Clean terms\n",
    "        terms = [str(t).strip() for t in terms if t and str(t).strip() != '']\n",
    "        \n",
    "        # Create tokens and BIO labels\n",
    "        tokens, labels = create_bio_labels(sentence_text, terms)\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        \n",
    "        data.append({\n",
    "            'document_id': doc_id,\n",
    "            'paragraph_id': para_id,\n",
    "            'sentence_id': sent_id,\n",
    "            'sentence_text': sentence_text,\n",
    "            'tokens': tokens,\n",
    "            'labels': labels,\n",
    "            'gold_terms': terms\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Preparing training data...\")\n",
    "train_data = prepare_data_for_training(train_df)\n",
    "print(f\"Prepared {len(train_data)} training sentences\")\n",
    "\n",
    "print(\"\\nPreparing development data...\")\n",
    "dev_data = prepare_data_for_training(dev_df)\n",
    "print(f\"Prepared {len(dev_data)} development sentences\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample prepared data:\")\n",
    "example = train_data[0]\n",
    "print(f\"Document ID: {example['document_id']}\")\n",
    "print(f\"Sentence: {example['sentence_text']}\")\n",
    "print(f\"Tokens: {example['tokens']}\")\n",
    "print(f\"Labels: {example['labels']}\")\n",
    "print(f\"Gold terms: {example['gold_terms']}\")\n",
    "\n",
    "# Check label distribution\n",
    "all_labels = [label for item in train_data for label in item['labels']]\n",
    "label_counts = Counter(all_labels)\n",
    "print(f\"\\nLabel distribution (training): {dict(label_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition\n",
    "\n",
    "We'll use an Italian BERT model (UmBERTo or AlBERTo) fine-tuned for token classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: dbmdz/bert-base-italian-uncased\n",
      "Labels: ['O', 'B-TERM', 'I-TERM']\n",
      "Label mappings: {'O': 0, 'B-TERM': 1, 'I-TERM': 2}\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "# Using dbmdz/bert-base-italian-uncased - a well-established Italian BERT model\n",
    "MODEL_NAME = \"dbmdz/bert-base-italian-uncased\"\n",
    "# Alternative options (uncomment to use):\n",
    "# MODEL_NAME = \"Musixmatch/umberto-commoncrawl-cased-v1\"  # UmBERTo model\n",
    "# MODEL_NAME = \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\"  # AlBERTo model\n",
    "\n",
    "# Label configuration\n",
    "LABEL_LIST = ['O', 'B-TERM', 'I-TERM']\n",
    "NUM_LABELS = len(LABEL_LIST)\n",
    "LABEL_TO_ID = {label: idx for idx, label in enumerate(LABEL_LIST)}\n",
    "ID_TO_LABEL = {idx: label for idx, label in enumerate(LABEL_LIST)}\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Labels: {LABEL_LIST}\")\n",
    "print(f\"Label mappings: {LABEL_TO_ID}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer test:\n",
      "Original tokens: ['il', 'servizio', 'di', 'raccolta', 'dei', 'rifiuti', 'è', 'gestito', '.']\n",
      "Tokenizer input_ids length: 11\n",
      "Tokenizer attention_mask length: 11\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"Il servizio di raccolta dei rifiuti è gestito.\"\n",
    "test_tokens = tokenize_with_spacy(clean_text(test_text))\n",
    "\n",
    "encoded = tokenizer(\n",
    "    test_tokens,\n",
    "    is_split_into_words=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizer test:\")\n",
    "print(f\"Original tokens: {test_tokens}\")\n",
    "print(f\"Tokenizer input_ids length: {len(encoded['input_ids'][0])}\")\n",
    "print(f\"Tokenizer attention_mask length: {len(encoded['attention_mask'][0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label alignment test:\n",
      "Tokens: ['Il', 'servizio', 'di', 'raccolta']\n",
      "Original labels: ['O', 'B-TERM', 'I-TERM', 'I-TERM']\n",
      "Aligned label IDs: [-100, 0, 1, 2, 2, -100]\n",
      "Tokenizer word_ids: [None, 0, 1, 2, 3, None]\n",
      "Mapped labels: ['IGNORE', 'O', 'B-TERM', 'I-TERM', 'I-TERM', 'IGNORE']\n"
     ]
    }
   ],
   "source": [
    "def align_labels_with_tokenizer(tokens: List[str], labels: List[str], tokenizer) -> List[int]:\n",
    "    \"\"\"\n",
    "    Align BIO labels with tokenizer subword tokens.\n",
    "    Uses word_ids to map each subword token to its original word.\n",
    "    \"\"\"\n",
    "    # Tokenize with the transformer tokenizer\n",
    "    encoded = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    \n",
    "    word_ids = encoded.word_ids()\n",
    "    \n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    for word_idx in word_ids:\n",
    "        # Special tokens (CLS, SEP, PAD) get -100 (ignored in loss)\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        # Same word as previous token\n",
    "        elif word_idx == previous_word_idx:\n",
    "            aligned_labels.append(-100)  # Only first subword gets label\n",
    "        else:\n",
    "            # New word - get label from original label list\n",
    "            aligned_labels.append(LABEL_TO_ID.get(labels[word_idx], LABEL_TO_ID['O']))\n",
    "        \n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "    return aligned_labels\n",
    "\n",
    "\n",
    "# Test alignment\n",
    "test_tokens = [\"Il\", \"servizio\", \"di\", \"raccolta\"]\n",
    "test_labels = [\"O\", \"B-TERM\", \"I-TERM\", \"I-TERM\"]\n",
    "\n",
    "aligned = align_labels_with_tokenizer(test_tokens, test_labels, tokenizer)\n",
    "\n",
    "print(\"Label alignment test:\")\n",
    "print(f\"Tokens: {test_tokens}\")\n",
    "print(f\"Original labels: {test_labels}\")\n",
    "print(f\"Aligned label IDs: {aligned}\")\n",
    "encoded_test = tokenizer(test_tokens, is_split_into_words=True, padding=False, return_tensors=\"pt\")\n",
    "print(f\"Tokenizer word_ids: {encoded_test.word_ids()}\")\n",
    "print(f\"Mapped labels: {[ID_TO_LABEL.get(id, 'IGNORE') if id != -100 else 'IGNORE' for id in aligned]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: dbmdz/bert-base-italian-uncased\n",
      "Model parameters: 109,339,395\n",
      "Trainable parameters: 109,339,395\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=ID_TO_LABEL,\n",
    "    label2id=LABEL_TO_ID,\n",
    "    ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured:\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Output directory: ./ate_it_model_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ate_it_model_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy (deprecated in newer transformers)\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized and ready for training.\n"
     ]
    }
   ],
   "source": [
    "# Metrics computation for training\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics during evaluation.\n",
    "    Note: This computes token-level F1, which may differ from term-level F1 used in final evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted labels (ignoring -100)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Flatten and remove ignored tokens\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        for pred, label in zip(pred_seq, label_seq):\n",
    "            if label != -100:\n",
    "                true_labels.append(ID_TO_LABEL[label])\n",
    "                pred_labels.append(ID_TO_LABEL[pred])\n",
    "    \n",
    "    # Calculate metrics using seqeval\n",
    "    precision = precision_score([true_labels], [pred_labels])\n",
    "    recall = recall_score([true_labels], [pred_labels])\n",
    "    f1 = f1_score([true_labels], [pred_labels])\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Optional: Add class-weighted loss to handle class imbalance\n",
    "# Calculate class weights from training data\n",
    "from collections import Counter\n",
    "all_train_labels = [label for item in train_data for label in item['labels']]\n",
    "label_counts = Counter(all_train_labels)\n",
    "total_labels = sum(label_counts.values())\n",
    "\n",
    "# Calculate inverse frequency weights (optional - uncomment to use)\n",
    "# class_weights = [\n",
    "#     1.0 / (label_counts.get('O', 1) / total_labels),      # O class weight\n",
    "#     1.0 / (label_counts.get('B-TERM', 1) / total_labels), # B-TERM class weight  \n",
    "#     1.0 / (label_counts.get('I-TERM', 1) / total_labels)   # I-TERM class weight\n",
    "# ]\n",
    "# Normalize weights\n",
    "# max_weight = max(class_weights)\n",
    "# class_weights = [w / max_weight for w in class_weights]\n",
    "# print(f\"Class weights: {dict(zip(LABEL_LIST, class_weights))}\")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf_dataset,\n",
    "    eval_dataset=dev_hf_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized and ready for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 05:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.184304</td>\n",
       "      <td>0.490842</td>\n",
       "      <td>0.607710</td>\n",
       "      <td>0.543060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>0.680751</td>\n",
       "      <td>0.657596</td>\n",
       "      <td>0.668973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.119890</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.679712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.129228</td>\n",
       "      <td>0.699561</td>\n",
       "      <td>0.723356</td>\n",
       "      <td>0.711260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.122148</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.713514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "Training loss: 0.1349\n",
      "\n",
      "Model saved to ./ate_it_final_model\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"./ate_it_final_model\")\n",
    "tokenizer.save_pretrained(\"./ate_it_final_model\")\n",
    "\n",
    "print(\"\\nModel saved to ./ate_it_final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ate_it_final_model/tokenizer_config.json',\n",
       " './ate_it_final_model/special_tokens_map.json',\n",
       " './ate_it_final_model/vocab.txt',\n",
       " './ate_it_final_model/added_tokens.json',\n",
       " './ate_it_final_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./ate_it_final_model\")\n",
    "tokenizer.save_pretrained(\"./ate_it_final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Saved Model for Evaluation\n",
    "\n",
    "After training, load the saved model for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model for evaluation...\n",
      "Loading model from: ./ate_it_final_model\n",
      " Model loaded successfully from ./ate_it_final_model\n",
      "Model ready for evaluation on cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model for evaluation\n",
    "print(\"Loading saved model for evaluation...\")\n",
    "\n",
    "# Try to load from local project path first, then default path\n",
    "model_paths = [\n",
    "    \"./ate_it_final_model\",\n",
    "    \"./ate_it_final_model/\"\n",
    "]\n",
    "\n",
    "model_loaded = False\n",
    "for model_path in model_paths:\n",
    "    try:\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Loading model from: {model_path}\")\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_path,\n",
    "                num_labels=NUM_LABELS,\n",
    "                id2label=ID_TO_LABEL,\n",
    "                label2id=LABEL_TO_ID\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model_loaded = True\n",
    "            print(f\" Model loaded successfully from {model_path}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from {model_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "if not model_loaded:\n",
    "    print(\"Warning: Could not load saved model. Using the model from training.\")\n",
    "    print(\"Make sure the model was saved during training.\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model ready for evaluation on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Functions\n",
    "\n",
    "Implementing the exact ATE-IT evaluation metrics: **Micro-F1** and **Type-F1**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-F1 Formula\n",
    "\n",
    "Micro-F1 is calculated at the **term level** over all sentences (matching official ATE-IT evaluation):\n",
    "\n",
    "$$\n",
    "\\text{Micro-Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Micro-Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Micro-F1} = \\frac{2 \\times \\text{Micro-Precision} \\times \\text{Micro-Recall}}{\\text{Micro-Precision} + \\text{Micro-Recall}}\n",
    "$$\n",
    "\n",
    "Where (per official requirements):\n",
    "- **TP** (True Positives): Number of terms extracted from sentence *i* that match the gold standard\n",
    "- **FP** (False Positives): Number of terms extracted from sentence *i* that do not match the gold standard\n",
    "- **FN** (False Negatives): Number of gold standard terms in sentence *i* that were not extracted\n",
    "\n",
    "**Note**: Micro-F1 compares **sets of terms per sentence**, not individual tokens. This matches the official evaluation script which compares term lists.\n",
    "\n",
    "### Type-F1 Formula\n",
    "\n",
    "Type-F1 is calculated over **unique term types** (case-insensitive):\n",
    "\n",
    "$$\n",
    "\\text{Type-Precision} = \\frac{|\\text{Predicted Terms} \\cap \\text{Gold Terms}|}{|\\text{Predicted Terms}|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Type-Recall} = \\frac{|\\text{Predicted Terms} \\cap \\text{Gold Terms}|}{|\\text{Gold Terms}|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Type-F1} = \\frac{2 \\times \\text{Type-Precision} \\times \\text{Type-Recall}}{\\text{Type-Precision} + \\text{Type-Recall}}\n",
    "$$\n",
    "\n",
    "Where terms are normalized to lowercase for comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5. COMPREHENSIVE OPTIMIZATION: Multi-Objective Performance Improvement\n",
    "\n",
    "**CRITICAL**: This cell implements a **comprehensive optimization strategy** to exceed baseline performance across ALL metrics.\n",
    "\n",
    "### Optimization Strategy:\n",
    "\n",
    "1. **Probability-Based Predictions**: Instead of argmax (hard predictions), we use probability thresholds to capture borderline terms, significantly improving Type-Recall.\n",
    "\n",
    "2. **Multi-Objective Optimization**: \n",
    "   - Tests a wide range of probability thresholds (0.01 to 0.30)\n",
    "   - Balances Type-Recall (50% weight), Type-F1 (30% weight), and Micro-F1 (20% weight)\n",
    "   - Ensures Type-Precision remains reasonable (>0.30) to avoid excessive false positives\n",
    "\n",
    "3. **Baseline Comparison**: \n",
    "   - Compares all metrics against baseline (Micro-Precision, Micro-Recall, Micro-F1, Type-Precision, Type-Recall, Type-F1)\n",
    "   - Shows percentage improvements for each metric\n",
    "   - Identifies which metrics exceed baseline\n",
    "\n",
    "**Research Rationale**: Argmax only selects the highest-confidence prediction, missing borderline terms. Probability thresholds allow us to capture terms with lower but still significant confidence. By testing multiple thresholds and using a weighted scoring function, we optimize for overall performance rather than a single metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function test:\n",
      "Tokens: ['Il', 'servizio', 'di', 'raccolta', 'dei', 'rifiuti', 'è', 'gestito']\n",
      "Gold labels: ['O', 'B-TERM', 'I-TERM', 'I-TERM', 'I-TERM', 'I-TERM', 'O', 'O']\n",
      "Pred labels: ['O', 'B-TERM', 'I-TERM', 'I-TERM', 'I-TERM', 'I-TERM', 'O', 'O']\n",
      "Gold terms: ['servizio di raccolta dei rifiuti']\n",
      "Pred terms: ['servizio di raccolta dei rifiuti']\n",
      "\n",
      "Micro-F1: {'micro_precision': 1.0, 'micro_recall': 1.0, 'micro_f1': 1.0}\n",
      "Type-F1: {'type_precision': 1.0, 'type_recall': 1.0, 'type_f1': 1.0, 'num_gold_types': 1, 'num_pred_types': 1, 'num_intersection': 1}\n"
     ]
    }
   ],
   "source": [
    "def extract_terms_from_bio(tokens: List[str], labels: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reconstruct multi-word terms from BIO labels.\n",
    "    Combines adjacent B/I tokens into terms.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    current_term = []\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == 'B-TERM':\n",
    "            # Save previous term if exists\n",
    "            if current_term:\n",
    "                terms.append(' '.join(current_term))\n",
    "            # Start new term\n",
    "            current_term = [token.lower()]\n",
    "        elif label == 'I-TERM':\n",
    "            # Continue current term\n",
    "            if current_term:\n",
    "                current_term.append(token.lower())\n",
    "            else:\n",
    "                # I without B - treat as B (shouldn't happen but handle gracefully)\n",
    "                current_term = [token.lower()]\n",
    "        else:  # 'O'\n",
    "            # Save previous term if exists\n",
    "            if current_term:\n",
    "                terms.append(' '.join(current_term))\n",
    "                current_term = []\n",
    "    \n",
    "    # Don't forget last term\n",
    "    if current_term:\n",
    "        terms.append(' '.join(current_term))\n",
    "    \n",
    "    return terms\n",
    "\n",
    "\n",
    "def compute_micro_f1(gold_terms_list: List[List[str]], pred_terms_list: List[List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute Micro-F1 at term level (matching evaluation script).\n",
    "    \n",
    "    According to ATE-IT evaluation: Micro-F1 compares sets of terms per sentence.\n",
    "    This matches the official evaluation script which compares term sets.\n",
    "    \n",
    "    Args:\n",
    "        gold_terms_list: List of term lists (gold) - one list per sentence\n",
    "        pred_terms_list: List of term lists (predictions) - one list per sentence\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # Iterate through each sentence's gold standard and system output terms\n",
    "    for gold_terms, pred_terms in zip(gold_terms_list, pred_terms_list):\n",
    "        # Normalize terms to lowercase and convert to sets for comparison\n",
    "        gold_set = set(term.strip().lower() for term in gold_terms if term and term.strip())\n",
    "        pred_set = set(term.strip().lower() for term in pred_terms if term and term.strip())\n",
    "        \n",
    "        # Calculate True Positives, False Positives, and False Negatives for the current sentence\n",
    "        true_positives = len(gold_set.intersection(pred_set))\n",
    "        false_positives = len(pred_set - gold_set)\n",
    "        false_negatives = len(gold_set - pred_set)\n",
    "        \n",
    "        # Accumulate totals across all sentences\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0.0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'micro_precision': precision,\n",
    "        'micro_recall': recall,\n",
    "        'micro_f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_type_f1(gold_terms_list: List[List[str]], pred_terms_list: List[List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute Type-F1 over unique term types.\n",
    "    \n",
    "    Args:\n",
    "        gold_terms_list: List of term lists (gold)\n",
    "        pred_terms_list: List of term lists (predictions)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1\n",
    "    \"\"\"\n",
    "    # Flatten and normalize to lowercase\n",
    "    gold_terms_set = set()\n",
    "    for term_list in gold_terms_list:\n",
    "        for term in term_list:\n",
    "            if term and term.strip():\n",
    "                gold_terms_set.add(term.strip().lower())\n",
    "    \n",
    "    pred_terms_set = set()\n",
    "    for term_list in pred_terms_list:\n",
    "        for term in term_list:\n",
    "            if term and term.strip():\n",
    "                pred_terms_set.add(term.strip().lower())\n",
    "    \n",
    "    # Calculate intersection\n",
    "    intersection = gold_terms_set & pred_terms_set\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = len(intersection) / len(pred_terms_set) if len(pred_terms_set) > 0 else 0.0\n",
    "    recall = len(intersection) / len(gold_terms_set) if len(gold_terms_set) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'type_precision': precision,\n",
    "        'type_recall': recall,\n",
    "        'type_f1': f1,\n",
    "        'num_gold_types': len(gold_terms_set),\n",
    "        'num_pred_types': len(pred_terms_set),\n",
    "        'num_intersection': len(intersection)\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation functions\n",
    "test_tokens = [\"Il\", \"servizio\", \"di\", \"raccolta\", \"dei\", \"rifiuti\", \"è\", \"gestito\"]\n",
    "test_gold_labels = [\"O\", \"B-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"O\", \"O\"]\n",
    "test_pred_labels = [\"O\", \"B-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"O\", \"O\"]\n",
    "\n",
    "test_gold_terms = extract_terms_from_bio(test_tokens, test_gold_labels)\n",
    "test_pred_terms = extract_terms_from_bio(test_tokens, test_pred_labels)\n",
    "\n",
    "print(\"Evaluation function test:\")\n",
    "print(f\"Tokens: {test_tokens}\")\n",
    "print(f\"Gold labels: {test_gold_labels}\")\n",
    "print(f\"Pred labels: {test_pred_labels}\")\n",
    "print(f\"Gold terms: {test_gold_terms}\")\n",
    "print(f\"Pred terms: {test_pred_terms}\")\n",
    "\n",
    "micro_metrics = compute_micro_f1([test_gold_terms], [test_pred_terms])\n",
    "type_metrics = compute_type_f1([test_gold_terms], [test_pred_terms])\n",
    "\n",
    "print(f\"\\nMicro-F1: {micro_metrics}\")\n",
    "print(f\"Type-F1: {type_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term reconstruction test:\n",
      "Tokens: ['Il', 'servizio', 'di', 'raccolta', 'dei', 'rifiuti', 'è', 'gestito']\n",
      "Labels: ['O', 'B-TERM', 'I-TERM', 'I-TERM', 'I-TERM', 'I-TERM', 'O', 'O']\n",
      "Extracted terms: ['servizio di raccolta dei rifiuti']\n",
      "\n",
      "Testing nested term handling:\n",
      "If both 'servizio di raccolta dei rifiuti urbani' and 'raccolta' are detected,\n",
      "only the longer one should be kept (unless 'raccolta' appears standalone elsewhere)\n"
     ]
    }
   ],
   "source": [
    "# Domain-specific filters\n",
    "ITALIAN_STOPWORDS = {\n",
    "    'del', 'di', 'a', 'e', 'essere', 'conferito', 'portare', 'buttare', \n",
    "    'esponi', 'esporre', 'delle', 'degli', 'dello', 'della', 'dei', 'delle',\n",
    "    'umane', 'generato', 'accatastati', 'rubane', 'prefato', \"all'\", 'all',\n",
    "    'da', 'in', 'con', 'su', 'per', 'tra', 'fra', 'il', 'lo', 'la', 'i', 'gli', 'le',\n",
    "    'un', 'uno', 'una'\n",
    "}\n",
    "\n",
    "ENGLISH_WORDS = {'waste', 'paper', 'plastic', 'iron', 'batterien', 'batteries', 'green'}\n",
    "\n",
    "GENERIC_TERMS = {'sacchi', 'sacchetti', 'contenitori', 'sfuso', 'animali', \n",
    "                 'ambientale', 'elettronica', 'portare', 'buttare', 'esponi', \n",
    "                 'conferito', 'essere', 'a'}\n",
    "\n",
    "DAYS_OF_WEEK = {'lunedì', 'martedì', 'mercoledì', 'giovedì', 'venerdì', 'sabato', 'domenica'}\n",
    "\n",
    "ADMIN_HEADERS = {'data', 'argomenti', 'tipologia', 'descrizione', 'ultimo aggiornamento',\n",
    "                 'a cura di', 'premesso', 'visto', 'considerato', 'ritenuto'}\n",
    "\n",
    "VALID_ACRONYMS = {'raee', 'tari', 'cam', 'cer', 'ccr', 'rup', 'aro', 'tqrif', \n",
    "                  'arera', 'isola', 'ecologica'}\n",
    "\n",
    "\n",
    "def normalize_term_format(term: str) -> str:\n",
    "    \"\"\"Normalize term formatting - remove spaces around punctuation, fix contractions.\"\"\"\n",
    "    if pd.isna(term) or not term.strip():\n",
    "        return term\n",
    "    \n",
    "    term = term.strip()\n",
    "    \n",
    "    # Remove spaces around punctuation\n",
    "    term = re.sub(r'\\s+/\\s+', '/', term)  # carta / cartone -> carta/cartone\n",
    "    term = re.sub(r'\\s+-\\s+', '-', term)  # pseudo - edili -> pseudo-edili\n",
    "    term = re.sub(r'\\s+,', ',', term)     # raccolta , trasporto -> raccolta, trasporto\n",
    "    term = re.sub(r',\\s*$', '', term)      # Remove trailing comma\n",
    "    term = re.sub(r'\\s+\\.', '', term)      # Remove space before period\n",
    "    term = re.sub(r'\\.\\s*$', '', term)     # Remove trailing period\n",
    "    \n",
    "    # Fix contractions\n",
    "    term = re.sub(r\"d'\\s+\", \"d'\", term)    # d' erba -> d'erba\n",
    "    term = re.sub(r\"dell'\\s+\", \"dell'\", term)  # dell' ambiente -> dell'ambiente\n",
    "    term = re.sub(r\"all'\\s+\", \"all'\", term)    # all' utenza -> all'utenza\n",
    "    \n",
    "    return term.strip().lower()\n",
    "\n",
    "\n",
    "def is_valid_domain_term(term: str, sentence_context: str = \"\") -> bool:\n",
    "    \"\"\"\n",
    "    Validate if term is a valid domain-specific term.\n",
    "    Filters stopwords, generic terms, days of week, administrative headers, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(term) or not term.strip():\n",
    "        return False\n",
    "    \n",
    "    term_lower = term.strip().lower()\n",
    "    \n",
    "    # Too short (unless it's a valid acronym)\n",
    "    if len(term_lower) < 3 and term_lower not in VALID_ACRONYMS:\n",
    "        return False\n",
    "    \n",
    "    # Single character\n",
    "    if len(term_lower) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Stopword\n",
    "    if term_lower in ITALIAN_STOPWORDS:\n",
    "        return False\n",
    "    \n",
    "    # English word\n",
    "    if term_lower in ENGLISH_WORDS:\n",
    "        return False\n",
    "    \n",
    "    # Generic term\n",
    "    if term_lower in GENERIC_TERMS:\n",
    "        return False\n",
    "    \n",
    "    # Day of week\n",
    "    if term_lower in DAYS_OF_WEEK:\n",
    "        return False\n",
    "    \n",
    "    # Administrative header (check if sentence is just a header)\n",
    "    if term_lower in ADMIN_HEADERS and len(sentence_context.split()) < 5:\n",
    "        return False\n",
    "    \n",
    "    # Incomplete term (starts with preposition only)\n",
    "    if re.match(r'^(del|di|a|da|in|con|su|per|tra|fra|delle|degli|dello|della|dei)\\s*$', term_lower):\n",
    "        return False\n",
    "    \n",
    "    # Incomplete term (ends with preposition - IMPROVED CHECK)\n",
    "    # Check if term ends with preposition and is incomplete\n",
    "    if re.search(r'\\s+(del|di|a|da|in|con|su|per|tra|fra|dei|del|delle|degli|dello|della)$', term_lower):\n",
    "        # If it's a 2-word term ending with preposition, it's likely incomplete\n",
    "        if len(term_lower.split()) <= 2:\n",
    "            return False\n",
    "        # For 3+ word terms, check if it looks incomplete (e.g., \"di ritiro su\")\n",
    "        # These patterns suggest incomplete extraction\n",
    "        if re.search(r'^(di|del|della|delle|degli|dello|dei)\\s+\\w+\\s+(su|di|del|della|delle|degli|dello|dei)$', term_lower):\n",
    "            return False\n",
    "    \n",
    "    # Very short incomplete fragments\n",
    "    if len(term_lower.split()) == 1 and len(term_lower) < 4 and term_lower not in VALID_ACRONYMS:\n",
    "        return False\n",
    "    \n",
    "    # Additional check: Terms that are clearly incomplete fragments\n",
    "    # Pattern: starts with preposition, has middle word, ends with preposition (e.g., \"di ritiro su\")\n",
    "    if re.match(r'^(di|del|della|delle|degli|dello|dei)\\s+\\w+\\s+(su|di|del|della|delle|degli|dello|dei)$', term_lower):\n",
    "        return False\n",
    "    \n",
    "    # Check for incomplete patterns like \"spazzamento e lavaggio delle\" (missing continuation)\n",
    "    # Terms ending with \"delle\", \"degli\", \"dello\", \"della\" that are clearly incomplete\n",
    "    if re.search(r'\\s+(delle|degli|dello|della)$', term_lower):\n",
    "        # If it's a short phrase ending with these, it's likely incomplete\n",
    "        # e.g., \"spazzamento e lavaggio delle\" should be \"spazzamento e lavaggio delle strade\"\n",
    "        if len(term_lower.split()) <= 3:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def reconstruct_terms_with_constraints(\n",
    "    tokens: List[str], \n",
    "    labels: List[str],\n",
    "    sentence_text: str = \"\",\n",
    "    enforce_no_nested: bool = True,\n",
    "    enforce_no_duplicates: bool = True,\n",
    "    filter_invalid: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Reconstruct terms from BIO labels with ATE-IT constraints and enhanced filtering.\n",
    "    \n",
    "    According to ATE-IT requirements: \"Nested terms are not permitted \n",
    "    (e.g., if 'impianto di trattamento rifiuti' is extracted, the inner \n",
    "    term 'trattamento rifiuti' must not be included, unless it also \n",
    "    appears independently).\"\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        labels: List of BIO labels\n",
    "        sentence_text: Original sentence text for context filtering\n",
    "        enforce_no_nested: Remove nested terms if parent term exists (unless they appear independently)\n",
    "        enforce_no_duplicates: Remove duplicate terms\n",
    "        filter_invalid: Filter out stopwords, generic terms, etc.\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted terms (lowercase, normalized)\n",
    "    \"\"\"\n",
    "    # First, extract all terms\n",
    "    all_terms = extract_terms_from_bio(tokens, labels)\n",
    "    \n",
    "    if not all_terms:\n",
    "        return []\n",
    "    \n",
    "    # Normalize to lowercase and format\n",
    "    all_terms = [normalize_term_format(t) for t in all_terms if t and t.strip()]\n",
    "    all_terms = [t for t in all_terms if t]\n",
    "    \n",
    "    # Filter invalid terms\n",
    "    if filter_invalid:\n",
    "        all_terms = [t for t in all_terms if is_valid_domain_term(t, sentence_text)]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    if enforce_no_duplicates:\n",
    "        seen = set()\n",
    "        unique_terms = []\n",
    "        for term in all_terms:\n",
    "            if term not in seen:\n",
    "                seen.add(term)\n",
    "                unique_terms.append(term)\n",
    "        all_terms = unique_terms\n",
    "    \n",
    "    # Enforce no nested terms (unless they appear independently)\n",
    "    if enforce_no_nested and len(all_terms) > 1:\n",
    "        # Reconstruct sentence text from tokens for independent occurrence checking\n",
    "        sentence_text_lower = sentence_text.lower() if sentence_text else ' '.join(tokens).lower()\n",
    "        \n",
    "        # Sort by length (longest first)\n",
    "        sorted_terms = sorted(all_terms, key=len, reverse=True)\n",
    "        \n",
    "        filtered_terms = []\n",
    "        for term in sorted_terms:\n",
    "            # Check if this term is nested in any already accepted term\n",
    "            is_nested_in_accepted = False\n",
    "            nested_in_terms = []\n",
    "            \n",
    "            for accepted_term in filtered_terms:\n",
    "                # Check if term appears as substring in accepted_term\n",
    "                if term in accepted_term and term != accepted_term:\n",
    "                    # Check if it's a word boundary match (more strict)\n",
    "                    pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "                    if re.search(pattern, accepted_term):\n",
    "                        is_nested_in_accepted = True\n",
    "                        nested_in_terms.append(accepted_term)\n",
    "            \n",
    "            # If term is nested, check if it also appears independently\n",
    "            if is_nested_in_accepted:\n",
    "                # Find all occurrences of the shorter term in the sentence\n",
    "                term_pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "                term_matches = list(re.finditer(term_pattern, sentence_text_lower))\n",
    "                \n",
    "                # Find all occurrences of longer terms that contain it\n",
    "                longer_term_positions = []\n",
    "                for longer_term in nested_in_terms:\n",
    "                    longer_pattern = r'\\b' + re.escape(longer_term) + r'\\b'\n",
    "                    for match in re.finditer(longer_pattern, sentence_text_lower):\n",
    "                        longer_term_positions.append((match.start(), match.end()))\n",
    "                \n",
    "                # Check if term has an independent occurrence (not covered by longer terms)\n",
    "                has_independent_occurrence = False\n",
    "                for term_match in term_matches:\n",
    "                    term_start = term_match.start()\n",
    "                    term_end = term_match.end()\n",
    "                    \n",
    "                    # Check if this occurrence is covered by any longer term\n",
    "                    is_covered = False\n",
    "                    for longer_start, longer_end in longer_term_positions:\n",
    "                        if longer_start <= term_start and term_end <= longer_end:\n",
    "                            is_covered = True\n",
    "                            break\n",
    "                    \n",
    "                    # If this occurrence is not covered, it's independent\n",
    "                    if not is_covered:\n",
    "                        has_independent_occurrence = True\n",
    "                        break\n",
    "                \n",
    "                # Only add if it appears independently\n",
    "                if has_independent_occurrence:\n",
    "                    filtered_terms.append(term)\n",
    "                # Otherwise, skip it (it's nested and doesn't appear independently)\n",
    "            else:\n",
    "                # Not nested, add it\n",
    "                filtered_terms.append(term)\n",
    "        \n",
    "        all_terms = filtered_terms\n",
    "    \n",
    "    return all_terms\n",
    "\n",
    "\n",
    "# Test term reconstruction with constraints\n",
    "test_tokens = [\"Il\", \"servizio\", \"di\", \"raccolta\", \"dei\", \"rifiuti\", \"è\", \"gestito\"]\n",
    "test_labels = [\"O\", \"B-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"O\", \"O\"]\n",
    "\n",
    "terms = reconstruct_terms_with_constraints(test_tokens, test_labels)\n",
    "print(\"Term reconstruction test:\")\n",
    "print(f\"Tokens: {test_tokens}\")\n",
    "print(f\"Labels: {test_labels}\")\n",
    "print(f\"Extracted terms: {terms}\")\n",
    "\n",
    "# Test with nested terms\n",
    "test_tokens2 = [\"Il\", \"servizio\", \"di\", \"raccolta\", \"dei\", \"rifiuti\", \"urbani\"]\n",
    "test_labels2 = [\"O\", \"B-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\", \"I-TERM\"]\n",
    "# Simulate also detecting \"raccolta\" as a separate term\n",
    "# This would require label modification, but for testing:\n",
    "print(\"\\nTesting nested term handling:\")\n",
    "print(\"If both 'servizio di raccolta dei rifiuti urbani' and 'raccolta' are detected,\")\n",
    "print(\"only the longer one should be kept (unless 'raccolta' appears standalone elsewhere)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Set Evaluation\n",
    "\n",
    "Evaluate the model on the training set to see performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SET EVALUATION\n",
      "============================================================\n",
      "\n",
      "Loading training set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running predictions on training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2308/2308 [02:36<00:00, 14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing evaluation metrics...\n",
      "\n",
      "============================================================\n",
      "TRAINING SET EVALUATION RESULTS\n",
      "============================================================\n",
      "Micro-Precision: 0.9158\n",
      "Micro-Recall: 0.8823\n",
      "Micro-F1: 0.8987\n",
      "\n",
      "Type-Precision: 0.8750\n",
      "Type-Recall: 0.8443\n",
      "Type-F1: 0.8594\n",
      "\n",
      "Gold term types: 713\n",
      "Pred term types: 688\n",
      "Intersection: 602\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING SET EVALUATION\n",
    "# ============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load training data\n",
    "print(\"\\nLoading training set...\")\n",
    "train_df_eval = pd.read_csv(TRAIN_PATH)\n",
    "train_df_eval.fillna('', inplace=True)\n",
    "\n",
    "# Group by sentence\n",
    "train_sentence_groups = train_df_eval.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "\n",
    "# Collect gold and predicted terms\n",
    "gold_terms_list = []\n",
    "pred_terms_list = []\n",
    "\n",
    "print(\"\\nRunning predictions on training set...\")\n",
    "with torch.no_grad():\n",
    "    for (doc_id, para_id, sent_id), group in tqdm(train_sentence_groups, desc=\"Evaluating\"):\n",
    "        sentence_text = group.iloc[0]['sentence_text']\n",
    "        \n",
    "        # Get gold terms\n",
    "        gold_terms = [str(t).strip().lower() for t in group['term'].dropna().tolist() if t and str(t).strip()]\n",
    "        gold_terms_list.append(gold_terms)\n",
    "        \n",
    "        # Clean and tokenize\n",
    "        cleaned_text = clean_text(sentence_text)\n",
    "        tokens = tokenize_with_spacy(cleaned_text)\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            pred_terms_list.append([])\n",
    "            continue\n",
    "        \n",
    "        # Tokenize with transformer\n",
    "        encoded = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Predict\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        pred_label_ids = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Get word_ids for alignment\n",
    "        encoded_for_words = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        word_ids = encoded_for_words.word_ids()\n",
    "        \n",
    "        # Map predictions back to tokens\n",
    "        pred_labels = []\n",
    "        previous_word_idx = None\n",
    "        for tokenizer_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            elif word_idx == previous_word_idx:\n",
    "                continue\n",
    "            else:\n",
    "                if tokenizer_idx < len(pred_label_ids):\n",
    "                    pred_labels.append(ID_TO_LABEL[pred_label_ids[tokenizer_idx]])\n",
    "                else:\n",
    "                    pred_labels.append('O')\n",
    "                previous_word_idx = word_idx\n",
    "        \n",
    "        # Ensure alignment\n",
    "        min_len = min(len(tokens), len(pred_labels))\n",
    "        tokens_aligned = tokens[:min_len]\n",
    "        pred_labels_aligned = pred_labels[:min_len]\n",
    "        \n",
    "        # Extract terms with constraints and filtering\n",
    "        pred_terms = reconstruct_terms_with_constraints(\n",
    "            tokens_aligned, \n",
    "            pred_labels_aligned,\n",
    "            sentence_text=sentence_text,  # Pass original sentence for context\n",
    "            enforce_no_nested=True,\n",
    "            enforce_no_duplicates=True,\n",
    "            filter_invalid=True  # Enable filtering\n",
    "        )\n",
    "        pred_terms_list.append(pred_terms)\n",
    "\n",
    "# Compute metrics\n",
    "print(\"\\nComputing evaluation metrics...\")\n",
    "micro_metrics = compute_micro_f1(gold_terms_list, pred_terms_list)\n",
    "type_metrics = compute_type_f1(gold_terms_list, pred_terms_list)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SET EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Micro-Precision: {micro_metrics['micro_precision']:.4f}\")\n",
    "print(f\"Micro-Recall: {micro_metrics['micro_recall']:.4f}\")\n",
    "print(f\"Micro-F1: {micro_metrics['micro_f1']:.4f}\")\n",
    "print(f\"\\nType-Precision: {type_metrics['type_precision']:.4f}\")\n",
    "print(f\"Type-Recall: {type_metrics['type_recall']:.4f}\")\n",
    "print(f\"Type-F1: {type_metrics['type_f1']:.4f}\")\n",
    "print(f\"\\nGold term types: {type_metrics['num_gold_types']}\")\n",
    "print(f\"Pred term types: {type_metrics['num_pred_types']}\")\n",
    "print(f\"Intersection: {type_metrics['num_intersection']}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Development Set Evaluation\n",
    "\n",
    "Evaluate the model on the development set to see performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEVELOPMENT SET EVALUATION\n",
      "============================================================\n",
      "\n",
      "Loading development set...\n",
      "\n",
      "Running predictions on development set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/577 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 577/577 [00:37<00:00, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing evaluation metrics...\n",
      "\n",
      "============================================================\n",
      "DEVELOPMENT SET EVALUATION RESULTS\n",
      "============================================================\n",
      "Micro-Precision: 0.7098\n",
      "Micro-Recall: 0.7051\n",
      "Micro-F1: 0.7075\n",
      "\n",
      "Type-Precision: 0.6402\n",
      "Type-Recall: 0.6322\n",
      "Type-F1: 0.6362\n",
      "\n",
      "Gold term types: 242\n",
      "Pred term types: 239\n",
      "Intersection: 153\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DEVELOPMENT SET EVALUATION\n",
    "# ============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"DEVELOPMENT SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load development data\n",
    "print(\"\\nLoading development set...\")\n",
    "dev_df_eval = pd.read_csv(DEV_PATH)\n",
    "dev_df_eval.fillna('', inplace=True)\n",
    "\n",
    "# Group by sentence\n",
    "dev_sentence_groups = dev_df_eval.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "\n",
    "# Collect gold and predicted terms\n",
    "gold_terms_list = []\n",
    "pred_terms_list = []\n",
    "\n",
    "print(\"\\nRunning predictions on development set...\")\n",
    "with torch.no_grad():\n",
    "    for (doc_id, para_id, sent_id), group in tqdm(dev_sentence_groups, desc=\"Evaluating\"):\n",
    "        sentence_text = group.iloc[0]['sentence_text']\n",
    "        \n",
    "        # Get gold terms\n",
    "        gold_terms = [str(t).strip().lower() for t in group['term'].dropna().tolist() if t and str(t).strip()]\n",
    "        gold_terms_list.append(gold_terms)\n",
    "        \n",
    "        # Clean and tokenize\n",
    "        cleaned_text = clean_text(sentence_text)\n",
    "        tokens = tokenize_with_spacy(cleaned_text)\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            pred_terms_list.append([])\n",
    "            continue\n",
    "        \n",
    "        # Tokenize with transformer\n",
    "        encoded = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Predict\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        pred_label_ids = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Get word_ids for alignment\n",
    "        encoded_for_words = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        word_ids = encoded_for_words.word_ids()\n",
    "        \n",
    "        # Map predictions back to tokens\n",
    "        pred_labels = []\n",
    "        previous_word_idx = None\n",
    "        for tokenizer_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            elif word_idx == previous_word_idx:\n",
    "                continue\n",
    "            else:\n",
    "                if tokenizer_idx < len(pred_label_ids):\n",
    "                    pred_labels.append(ID_TO_LABEL[pred_label_ids[tokenizer_idx]])\n",
    "                else:\n",
    "                    pred_labels.append('O')\n",
    "                previous_word_idx = word_idx\n",
    "        \n",
    "        # Ensure alignment\n",
    "        min_len = min(len(tokens), len(pred_labels))\n",
    "        tokens_aligned = tokens[:min_len]\n",
    "        pred_labels_aligned = pred_labels[:min_len]\n",
    "        \n",
    "        # Extract terms with constraints and filtering\n",
    "        pred_terms = reconstruct_terms_with_constraints(\n",
    "            tokens_aligned, \n",
    "            pred_labels_aligned,\n",
    "            sentence_text=sentence_text,  # Pass original sentence for context\n",
    "            enforce_no_nested=True,\n",
    "            enforce_no_duplicates=True,\n",
    "            filter_invalid=True  # Enable filtering\n",
    "        )\n",
    "        pred_terms_list.append(pred_terms)\n",
    "\n",
    "# Compute metrics\n",
    "print(\"\\nComputing evaluation metrics...\")\n",
    "micro_metrics = compute_micro_f1(gold_terms_list, pred_terms_list)\n",
    "type_metrics = compute_type_f1(gold_terms_list, pred_terms_list)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEVELOPMENT SET EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Micro-Precision: {micro_metrics['micro_precision']:.4f}\")\n",
    "print(f\"Micro-Recall: {micro_metrics['micro_recall']:.4f}\")\n",
    "print(f\"Micro-F1: {micro_metrics['micro_f1']:.4f}\")\n",
    "print(f\"\\nType-Precision: {type_metrics['type_precision']:.4f}\")\n",
    "print(f\"Type-Recall: {type_metrics['type_recall']:.4f}\")\n",
    "print(f\"Type-F1: {type_metrics['type_f1']:.4f}\")\n",
    "print(f\"\\nGold term types: {type_metrics['num_gold_types']}\")\n",
    "print(f\"Pred term types: {type_metrics['num_pred_types']}\")\n",
    "print(f\"Intersection: {type_metrics['num_intersection']}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Set Evaluation (Comprehensive)\n",
    "\n",
    "This cell evaluates the improved test predictions and computes metrics. \n",
    "If you have a gold standard file (test_ground_truth.csv), it will compute Micro-F1 and Type-F1.\n",
    "Otherwise, it will show prediction statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPREHENSIVE ATE-IT EVALUATION\n",
      "============================================================\n",
      "\n",
      " Loading files...\n",
      " Loaded 1379 prediction rows from: test_predictions_improved.csv\n",
      " Loaded 1363 gold standard rows from: test_predictions_fixed.csv\n",
      "\n",
      " Grouping by sentence...\n",
      " Found 1142 unique sentences\n",
      "\n",
      " Extracting terms per sentence...\n",
      " Processed 1142 sentences\n",
      "\n",
      " Computing Micro-level metrics...\n",
      " Computing Type-level metrics...\n",
      "\n",
      "============================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Micro-Precision: 0.0000\n",
      "Micro-Recall:    0.0000\n",
      "Micro-F1:        0.0000\n",
      "\n",
      "Type-Precision:  0.0000\n",
      "Type-Recall:     0.0000\n",
      "Type-F1:         0.0000\n",
      "\n",
      "Gold term types: 0\n",
      "Pred term types: 0\n",
      "Intersection:    0\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DETAILED ERROR ANALYSIS\n",
      "============================================================\n",
      "\n",
      " Top 20 Most Frequent False Positives:\n",
      "  No false positives found.\n",
      "\n",
      " Top 20 Most Frequent False Negatives:\n",
      "  No false negatives found.\n",
      "\n",
      " Sentence-Level Statistics:\n",
      "Metric                                             Count     \n",
      "------------------------------------------------------------\n",
      "Total sentences                                    1142      \n",
      "Sentences with gold terms                          0         \n",
      "Sentences with predicted terms                     0         \n",
      "Missed sentences (gold exists, no predictions)     0         \n",
      "False alert sentences (predictions, no gold)       0         \n",
      "Sentences with false positives                     0         \n",
      "Sentences with false negatives                     0         \n",
      "\n",
      " Confusion Matrix Summary:\n",
      "Metric                         Count          \n",
      "---------------------------------------------\n",
      "True Positives (TP)            0              \n",
      "False Positives (FP)           0              \n",
      "False Negatives (FN)           0              \n",
      "Total Gold Terms               0              \n",
      "Total Predicted Terms          0              \n",
      "\n",
      "============================================================\n",
      " EVALUATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE ATE-IT EVALUATION (OFFICIAL TASK SPECIFICATIONS)\n",
    "# ============================================================================\n",
    "# This cell implements the exact ATE-IT evaluation metrics:\n",
    "# 1. Micro-level metrics (sentence-level aggregation)\n",
    "# 2. Type-level metrics (unique term types)\n",
    "# 3. Detailed error analysis (false positives, false negatives)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "PREDICTIONS_PATH = r\"test_predictions_improved.csv\"  # Model predictions\n",
    "GOLD_TRUTH_PATH = r\"test_predictions_fixed.csv\"     # Gold standard\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE ATE-IT EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load and Normalize Data\n",
    "# ============================================================================\n",
    "print(\"\\n Loading files...\")\n",
    "\n",
    "# Load predictions\n",
    "if not os.path.exists(PREDICTIONS_PATH):\n",
    "    raise FileNotFoundError(f\"Predictions file not found: {PREDICTIONS_PATH}\")\n",
    "pred_df = pd.read_csv(PREDICTIONS_PATH)\n",
    "pred_df.fillna('', inplace=True)\n",
    "print(f\" Loaded {len(pred_df)} prediction rows from: {PREDICTIONS_PATH}\")\n",
    "\n",
    "# Load gold standard\n",
    "if not os.path.exists(GOLD_TRUTH_PATH):\n",
    "    raise FileNotFoundError(f\"Gold standard file not found: {GOLD_TRUTH_PATH}\")\n",
    "gold_df = pd.read_csv(GOLD_TRUTH_PATH)\n",
    "gold_df.fillna('', inplace=True)\n",
    "print(f\" Loaded {len(gold_df)} gold standard rows from: {GOLD_TRUTH_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Normalize Terms\n",
    "# ============================================================================\n",
    "def normalize_term(term: str) -> str:\n",
    "    \"\"\"Normalize term: lowercase, strip, unify spacing.\"\"\"\n",
    "    if pd.isna(term) or not term:\n",
    "        return ''\n",
    "    term = str(term).strip().lower()\n",
    "    term = re.sub(r'\\s+', ' ', term)  # Unify spacing (no double spaces)\n",
    "    return term.strip()\n",
    "\n",
    "# Normalize all terms\n",
    "pred_df['term_normalized'] = pred_df['term'].apply(normalize_term)\n",
    "gold_df['term_normalized'] = gold_df['term'].apply(normalize_term)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Group by Sentence\n",
    "# ============================================================================\n",
    "print(\"\\n Grouping by sentence...\")\n",
    "pred_groups = pred_df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "gold_groups = gold_df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "\n",
    "# Get all unique sentences\n",
    "all_sentences = set()\n",
    "for (doc_id, para_id, sent_id), _ in pred_groups:\n",
    "    all_sentences.add((str(doc_id), str(para_id), str(sent_id)))\n",
    "for (doc_id, para_id, sent_id), _ in gold_groups:\n",
    "    all_sentences.add((str(doc_id), str(para_id), str(sent_id)))\n",
    "\n",
    "all_sentences = sorted(all_sentences)\n",
    "print(f\" Found {len(all_sentences)} unique sentences\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Extract Terms per Sentence\n",
    "# ============================================================================\n",
    "print(\"\\n Extracting terms per sentence...\")\n",
    "gold_terms_list = []\n",
    "pred_terms_list = []\n",
    "\n",
    "# Track false positives and false negatives for analysis\n",
    "false_positives_counter = Counter()  # Term -> count\n",
    "false_negatives_counter = Counter()  # Term -> count\n",
    "sentences_with_fp = []  # Track which sentences have FPs\n",
    "sentences_with_fn = []  # Track which sentences have FNs\n",
    "\n",
    "for doc_id, para_id, sent_id in all_sentences:\n",
    "    # Get gold terms\n",
    "    if (doc_id, para_id, sent_id) in gold_groups.groups:\n",
    "        gold_group = gold_groups.get_group((doc_id, para_id, sent_id))\n",
    "        gold_terms = [normalize_term(t) for t in gold_group['term'].tolist() \n",
    "                     if t and normalize_term(t)]\n",
    "        gold_terms_set = set(gold_terms)\n",
    "    else:\n",
    "        gold_terms = []\n",
    "        gold_terms_set = set()\n",
    "    \n",
    "    # Get predicted terms\n",
    "    if (doc_id, para_id, sent_id) in pred_groups.groups:\n",
    "        pred_group = pred_groups.get_group((doc_id, para_id, sent_id))\n",
    "        pred_terms = [normalize_term(t) for t in pred_group['term'].tolist() \n",
    "                     if t and normalize_term(t)]\n",
    "        pred_terms_set = set(pred_terms)\n",
    "    else:\n",
    "        pred_terms = []\n",
    "        pred_terms_set = set()\n",
    "    \n",
    "    # Track false positives and false negatives\n",
    "    fps = pred_terms_set - gold_terms_set\n",
    "    fns = gold_terms_set - pred_terms_set\n",
    "    \n",
    "    for fp_term in fps:\n",
    "        false_positives_counter[fp_term] += 1\n",
    "    for fn_term in fns:\n",
    "        false_negatives_counter[fn_term] += 1\n",
    "    \n",
    "    if fps:\n",
    "        sentences_with_fp.append((doc_id, para_id, sent_id))\n",
    "    if fns:\n",
    "        sentences_with_fn.append((doc_id, para_id, sent_id))\n",
    "    \n",
    "    gold_terms_list.append(gold_terms)\n",
    "    pred_terms_list.append(pred_terms)\n",
    "\n",
    "print(f\" Processed {len(all_sentences)} sentences\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Compute Micro-Level Metrics (Sentence-Level Aggregation)\n",
    "# ============================================================================\n",
    "print(\"\\n Computing Micro-level metrics...\")\n",
    "\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "\n",
    "for gold_terms, pred_terms in zip(gold_terms_list, pred_terms_list):\n",
    "    gold_set = set(gold_terms)\n",
    "    pred_set = set(pred_terms)\n",
    "    \n",
    "    tp = len(gold_set & pred_set)  # True positives\n",
    "    fp = len(pred_set - gold_set)   # False positives\n",
    "    fn = len(gold_set - pred_set)   # False negatives\n",
    "    \n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "# Compute Micro metrics\n",
    "micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Compute Type-Level Metrics (Unique Term Types)\n",
    "# ============================================================================\n",
    "print(\" Computing Type-level metrics...\")\n",
    "\n",
    "# Collect all unique term types\n",
    "gold_terms_set = set()\n",
    "for term_list in gold_terms_list:\n",
    "    for term in term_list:\n",
    "        if term:\n",
    "            gold_terms_set.add(term)\n",
    "\n",
    "pred_terms_set = set()\n",
    "for term_list in pred_terms_list:\n",
    "    for term in term_list:\n",
    "        if term:\n",
    "            pred_terms_set.add(term)\n",
    "\n",
    "# Compute Type metrics\n",
    "type_tp = len(gold_terms_set & pred_terms_set)  # True positives (intersection)\n",
    "type_fp = len(pred_terms_set - gold_terms_set)   # False positives\n",
    "type_fn = len(gold_terms_set - pred_terms_set)   # False negatives\n",
    "\n",
    "type_precision = type_tp / (type_tp + type_fp) if (type_tp + type_fp) > 0 else 0.0\n",
    "type_recall = type_tp / (type_tp + type_fn) if (type_tp + type_fn) > 0 else 0.0\n",
    "type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0.0\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Additional Statistics\n",
    "# ============================================================================\n",
    "sentences_with_gold_terms = sum(1 for terms in gold_terms_list if len(terms) > 0)\n",
    "sentences_with_pred_terms = sum(1 for terms in pred_terms_list if len(terms) > 0)\n",
    "missed_sentences = sum(1 for gold_terms, pred_terms in zip(gold_terms_list, pred_terms_list) \n",
    "                      if len(gold_terms) > 0 and len(pred_terms) == 0)\n",
    "false_alert_sentences = sum(1 for gold_terms, pred_terms in zip(gold_terms_list, pred_terms_list) \n",
    "                           if len(gold_terms) == 0 and len(pred_terms) > 0)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Print Evaluation Results\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMicro-Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro-Recall:    {micro_recall:.4f}\")\n",
    "print(f\"Micro-F1:        {micro_f1:.4f}\")\n",
    "print(f\"\\nType-Precision:  {type_precision:.4f}\")\n",
    "print(f\"Type-Recall:     {type_recall:.4f}\")\n",
    "print(f\"Type-F1:         {type_f1:.4f}\")\n",
    "print(f\"\\nGold term types: {len(gold_terms_set)}\")\n",
    "print(f\"Pred term types: {len(pred_terms_set)}\")\n",
    "print(f\"Intersection:    {type_tp}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: Detailed Error Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Top 20 False Positives\n",
    "print(\"\\n Top 20 Most Frequent False Positives:\")\n",
    "if false_positives_counter:\n",
    "    print(f\"{'Rank':<6} {'Term':<40} {'Count':<10}\")\n",
    "    print(\"-\" * 56)\n",
    "    for rank, (term, count) in enumerate(false_positives_counter.most_common(20), 1):\n",
    "        print(f\"{rank:<6} {term[:38]:<40} {count:<10}\")\n",
    "else:\n",
    "    print(\"  No false positives found.\")\n",
    "\n",
    "# Top 20 False Negatives\n",
    "print(\"\\n Top 20 Most Frequent False Negatives:\")\n",
    "if false_negatives_counter:\n",
    "    print(f\"{'Rank':<6} {'Term':<40} {'Count':<10}\")\n",
    "    print(\"-\" * 56)\n",
    "    for rank, (term, count) in enumerate(false_negatives_counter.most_common(20), 1):\n",
    "        print(f\"{rank:<6} {term[:38]:<40} {count:<10}\")\n",
    "else:\n",
    "    print(\"  No false negatives found.\")\n",
    "\n",
    "# Sentence-level statistics\n",
    "print(\"\\n Sentence-Level Statistics:\")\n",
    "print(f\"{'Metric':<50} {'Count':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total sentences':<50} {len(all_sentences):<10}\")\n",
    "print(f\"{'Sentences with gold terms':<50} {sentences_with_gold_terms:<10}\")\n",
    "print(f\"{'Sentences with predicted terms':<50} {sentences_with_pred_terms:<10}\")\n",
    "print(f\"{'Missed sentences (gold exists, no predictions)':<50} {missed_sentences:<10}\")\n",
    "print(f\"{'False alert sentences (predictions, no gold)':<50} {false_alert_sentences:<10}\")\n",
    "print(f\"{'Sentences with false positives':<50} {len(sentences_with_fp):<10}\")\n",
    "print(f\"{'Sentences with false negatives':<50} {len(sentences_with_fn):<10}\")\n",
    "\n",
    "# Confusion matrix summary\n",
    "print(\"\\n Confusion Matrix Summary:\")\n",
    "print(f\"{'Metric':<30} {'Count':<15}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'True Positives (TP)':<30} {total_tp:<15}\")\n",
    "print(f\"{'False Positives (FP)':<30} {total_fp:<15}\")\n",
    "print(f\"{'False Negatives (FN)':<30} {total_fn:<15}\")\n",
    "print(f\"{'Total Gold Terms':<30} {total_tp + total_fn:<15}\")\n",
    "print(f\"{'Total Predicted Terms':<30} {total_tp + total_fp:<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Set Predictions Export (For Submission)\n",
    "\n",
    "This cell generates predictions on the unlabeled test set and exports them in the ATE-IT submission format. It performs the following steps:\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. **Model Loading**: Loads the trained model from `./ate_it_final_model` directory\n",
    "2. **Data Loading**: Reads the test dataset from `test.csv`\n",
    "3. **Prediction Generation**: \n",
    "   - Processes each sentence through the model\n",
    "   - Applies tokenization and BIO label prediction\n",
    "   - Extracts terms from predicted labels\n",
    "4. **Post-Processing**: \n",
    "   - Applies ATE-IT format requirements (lowercase, no duplicates, no nested terms)\n",
    "   - Removes invalid terms using domain-specific filters\n",
    "   - Ensures proper sentence ordering\n",
    "5. **Export**: Saves predictions to `test_predictions_improved.csv` in the required format\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Enhanced Filtering**: Applies comprehensive post-processing filters including:\n",
    "  - Stopword removal\n",
    "  - Generic term filtering\n",
    "  - Format normalization\n",
    "  - Nested term removal\n",
    "  - Duplicate removal\n",
    "\n",
    "- **ATE-IT Compliance**: Ensures output format matches submission requirements:\n",
    "  - All terms in lowercase\n",
    "  - No nested terms (unless they appear independently)\n",
    "  - No duplicate terms per sentence\n",
    "  - Proper column order: `document_id`, `paragraph_id`, `sentence_id`, `sentence_text`, `term`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST SET PREDICTION AND EXPORT\n",
      "============================================================\n",
      "\n",
      "Loading model and tokenizer...\n",
      "Model loaded from: ./ate_it_final_model\n",
      "Device: cpu\n",
      "\n",
      "Loading test set...\n",
      " Loaded 1142 rows from test CSV\n",
      " Required columns present: ['document_id', 'paragraph_id', 'sentence_id', 'sentence_text']\n",
      " Found 1142 unique sentences\n",
      "\n",
      " Running predictions on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1142/1142 [01:08<00:00, 16.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated 1379 prediction rows\n",
      "\n",
      " Organizing predictions...\n",
      "\n",
      "Applying ATE-IT output format requirements...\n",
      "Post-processing complete: 1379 rows\n",
      "Backed up old predictions to: test_predictions_backup_20251130_222936.csv\n",
      "\n",
      "Saving predictions to: test_predictions_improved.csv\n",
      "Successfully saved 1379 rows to test_predictions_improved.csv\n",
      "Format: document_id, paragraph_id, sentence_id, sentence_text, term\n",
      "This file includes improved filtering and ATE-IT format compliance\n",
      "Ready for ATE-IT submission\n",
      "Also saved to test_predictions.csv for compatibility\n",
      "\n",
      "============================================================\n",
      "TEST SET PREDICTION SUMMARY\n",
      "============================================================\n",
      "Total sentences: 1142\n",
      "Sentences with terms: 413\n",
      "Sentences without terms: 729\n",
      "Total terms extracted: 650\n",
      "Average terms per sentence: 0.57\n",
      "============================================================\n",
      "\n",
      "TEST SET PREDICTION EXPORT COMPLETE\n",
      "Improved predictions: test_predictions_improved.csv\n",
      "Also saved to: test_predictions.csv\n",
      "Ready for ATE-IT submission\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST SET PREDICTION AND EXPORT\n",
    "# ============================================================================\n",
    "# This cell:\n",
    "# 1. Loads the saved model\n",
    "# 2. Runs predictions on unlabeled test set\n",
    "# 3. Saves predictions CSV in ATE-IT submission format\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Paths\n",
    "MODEL_PATH = r\"./ate_it_final_model\"  # Updated to local writable directory\n",
    "TEST_CSV_PATH = r\"test.csv\"  # Test dataset path\n",
    "OUTPUT_PATH = r\"test_predictions_improved.csv\"  # Output file with improved filtering\n",
    "OLD_PREDICTIONS_PATH = r\"test_predictions.csv\"  # Old predictions (will be backed up)\n",
    "\n",
    "# Model configuration (must match training)\n",
    "LABEL_LIST = ['O', 'B-TERM', 'I-TERM']\n",
    "LABEL_TO_ID = {label: idx for idx, label in enumerate(LABEL_LIST)}\n",
    "ID_TO_LABEL = {idx: label for idx, label in enumerate(LABEL_LIST)}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET PREDICTION AND EXPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load Model and Tokenizer\n",
    "# ============================================================================\n",
    "print(\"\\nLoading model and tokenizer...\")\n",
    "try:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        num_labels=len(LABEL_LIST),\n",
    "        id2label=ID_TO_LABEL,\n",
    "        label2id=LABEL_TO_ID\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from: {MODEL_PATH}\")\n",
    "    print(f\"Device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Load Test Data\n",
    "# ============================================================================\n",
    "print(\"\\nLoading test set...\")\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    test_df.fillna('', inplace=True)\n",
    "    print(f\" Loaded {len(test_df)} rows from test CSV\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_columns = ['document_id', 'paragraph_id', 'sentence_id', 'sentence_text']\n",
    "    missing_columns = [col for col in required_columns if col not in test_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    print(f\" Required columns present: {required_columns}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\" Test file not found: {TEST_CSV_PATH}\")\n",
    "    print(\"  Please update TEST_CSV_PATH with the correct path to your test dataset\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\" Error loading test data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Group by sentence to get unique sentences\n",
    "sentence_groups = test_df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "print(f\" Found {len(sentence_groups)} unique sentences\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Helper Functions (from notebook)\n",
    "# ============================================================================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and lowercase text.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n",
    "    text = re.sub(r'\\{([^\\}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_with_spacy(text: str) -> list:\n",
    "    \"\"\"Tokenize using SpaCy.\"\"\"\n",
    "    if not text or text == '':\n",
    "        return []\n",
    "    if nlp is None:\n",
    "        return text.split()\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def extract_terms_from_bio(tokens: list, labels: list) -> list:\n",
    "    \"\"\"Extract terms from BIO labels.\"\"\"\n",
    "    terms = []\n",
    "    current_term = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == 'B-TERM':\n",
    "            if current_term:\n",
    "                terms.append(' '.join(current_term))\n",
    "            current_term = [token.lower()]\n",
    "        elif label == 'I-TERM':\n",
    "            if current_term:\n",
    "                current_term.append(token.lower())\n",
    "            else:\n",
    "                current_term = [token.lower()]\n",
    "        else:  # 'O'\n",
    "            if current_term:\n",
    "                terms.append(' '.join(current_term))\n",
    "                current_term = []\n",
    "    if current_term:\n",
    "        terms.append(' '.join(current_term))\n",
    "    return terms\n",
    "\n",
    "# Note: reconstruct_terms_with_constraints is defined in the preprocessing section with enhanced filtering\n",
    "# This cell uses the improved version from the main notebook\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Run Predictions on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n Running predictions on test set...\")\n",
    "prediction_rows = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (doc_id, para_id, sent_id), group in tqdm(sentence_groups, desc=\"Predicting\"):\n",
    "        sentence_text = group.iloc[0]['sentence_text']\n",
    "        \n",
    "        # Clean and tokenize\n",
    "        cleaned_text = clean_text(sentence_text)\n",
    "        tokens = tokenize_with_spacy(cleaned_text)\n",
    "        \n",
    "        # Prepare string conversions for row creation\n",
    "        doc_id_str = str(doc_id)\n",
    "        para_id_str = str(para_id)\n",
    "        sent_id_str = str(sent_id)\n",
    "        sentence_text_str = str(sentence_text) if pd.notna(sentence_text) else ''\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            # Add empty row for sentences with no tokens (to maintain order)\n",
    "            prediction_rows.append({\n",
    "                'document_id': doc_id_str,\n",
    "                'paragraph_id': para_id_str,\n",
    "                'sentence_id': sent_id_str,\n",
    "                'sentence_text': sentence_text_str,\n",
    "                'term': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Tokenize with transformer\n",
    "        encoded = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Predict\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        pred_label_ids = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Get word_ids for alignment\n",
    "        encoded_for_words = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        word_ids = encoded_for_words.word_ids()\n",
    "        \n",
    "        # Map predictions back to tokens\n",
    "        pred_labels = []\n",
    "        previous_word_idx = None\n",
    "        for tokenizer_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            elif word_idx == previous_word_idx:\n",
    "                continue\n",
    "            else:\n",
    "                if tokenizer_idx < len(pred_label_ids):\n",
    "                    pred_labels.append(ID_TO_LABEL[pred_label_ids[tokenizer_idx]])\n",
    "                else:\n",
    "                    pred_labels.append('O')\n",
    "                previous_word_idx = word_idx\n",
    "        \n",
    "        # Ensure alignment\n",
    "        min_len = min(len(tokens), len(pred_labels))\n",
    "        tokens_aligned = tokens[:min_len]\n",
    "        pred_labels_aligned = pred_labels[:min_len]\n",
    "        \n",
    "        # Extract terms with constraints and filtering\n",
    "        pred_terms = reconstruct_terms_with_constraints(\n",
    "            tokens_aligned, \n",
    "            pred_labels_aligned,\n",
    "            sentence_text=sentence_text,  # Pass original sentence for context\n",
    "            enforce_no_nested=True,\n",
    "            enforce_no_duplicates=True,\n",
    "            filter_invalid=True  # Enable filtering\n",
    "        )\n",
    "        \n",
    "        # Create rows (one per term, or one empty row if no terms)\n",
    "        if pred_terms:\n",
    "            for term in pred_terms:\n",
    "                # Terms are already normalized by reconstruct_terms_with_constraints\n",
    "                normalized_term = term.strip().lower()\n",
    "                if normalized_term:\n",
    "                    prediction_rows.append({\n",
    "                        'document_id': doc_id_str,\n",
    "                        'paragraph_id': para_id_str,\n",
    "                        'sentence_id': sent_id_str,\n",
    "                        'sentence_text': sentence_text_str,\n",
    "                        'term': normalized_term\n",
    "                    })\n",
    "        else:\n",
    "            # Add empty row for sentences with no predictions (to maintain order)\n",
    "            prediction_rows.append({\n",
    "                'document_id': doc_id_str,\n",
    "                'paragraph_id': para_id_str,\n",
    "                'sentence_id': sent_id_str,\n",
    "                'sentence_text': sentence_text_str,\n",
    "                'term': ''\n",
    "            })\n",
    "\n",
    "print(f\" Generated {len(prediction_rows)} prediction rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Create DataFrame and Sort by Sentence Order\n",
    "# ============================================================================\n",
    "print(\"\\n Organizing predictions...\")\n",
    "test_predictions_df = pd.DataFrame(prediction_rows)\n",
    "\n",
    "# ============================================================================\n",
    "# POST-PROCESSING: Apply ATE-IT Output Format Requirements\n",
    "# ============================================================================\n",
    "# 1. Lowercase all terms (no lemmatisation, stemming, or other transformations)\n",
    "# 2. Remove duplicate terms within the same sentence\n",
    "# 3. Remove nested terms (if \"impianto di trattamento rifiuti\" exists, \n",
    "#    remove \"trattamento rifiuti\" unless it appears independently)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nApplying ATE-IT output format requirements...\")\n",
    "\n",
    "# Step 1: Lowercase all terms\n",
    "test_predictions_df['term'] = test_predictions_df['term'].apply(\n",
    "    lambda x: str(x).strip().lower() if pd.notna(x) and str(x).strip() else ''\n",
    ")\n",
    "\n",
    "# Step 2 & 3: Process each sentence to remove duplicates and nested terms\n",
    "def remove_duplicates_and_nested(terms, sentence_text=\"\"):\n",
    "    \"\"\"Remove duplicates and nested terms from a list of terms.\n",
    "    \n",
    "    IMPORTANT: Checks if nested terms appear independently in the sentence.\n",
    "    Also filters invalid terms using is_valid_domain_term.\n",
    "    \"\"\"\n",
    "    if not terms:\n",
    "        return []\n",
    "    \n",
    "    # Remove empty terms and duplicates\n",
    "    unique_terms = []\n",
    "    seen = set()\n",
    "    for term in terms:\n",
    "        term_clean = term.strip().lower()\n",
    "        if term_clean and term_clean not in seen:\n",
    "            unique_terms.append(term_clean)\n",
    "            seen.add(term_clean)\n",
    "    \n",
    "    if len(unique_terms) <= 1:\n",
    "        return unique_terms\n",
    "    \n",
    "    # Filter invalid terms using is_valid_domain_term\n",
    "    valid_terms = []\n",
    "    for term in unique_terms:\n",
    "        if is_valid_domain_term(term, sentence_text):\n",
    "            valid_terms.append(term)\n",
    "    unique_terms = valid_terms\n",
    "    \n",
    "    if len(unique_terms) <= 1:\n",
    "        return unique_terms\n",
    "    \n",
    "    # Remove nested terms - but check for independent occurrences\n",
    "    sentence_text_lower = sentence_text.lower() if sentence_text else ''\n",
    "    \n",
    "    # Sort by length (longest first) to check if shorter terms are nested in longer ones\n",
    "    sorted_terms = sorted(unique_terms, key=len, reverse=True)\n",
    "    final_terms = []\n",
    "    \n",
    "    for term in sorted_terms:\n",
    "        is_nested_in_accepted = False\n",
    "        nested_in_terms = []\n",
    "        \n",
    "        # Check if this term is nested in any already accepted term\n",
    "        for accepted_term in final_terms:\n",
    "            # Check if term appears as substring in accepted_term\n",
    "            pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "            if re.search(pattern, accepted_term, re.IGNORECASE):\n",
    "                is_nested_in_accepted = True\n",
    "                nested_in_terms.append(accepted_term)\n",
    "        \n",
    "        # If term is nested, check if it also appears independently\n",
    "        if is_nested_in_accepted and sentence_text_lower:\n",
    "            # Find all occurrences of the shorter term in the sentence\n",
    "            term_pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "            term_matches = list(re.finditer(term_pattern, sentence_text_lower))\n",
    "            \n",
    "            # Find all occurrences of longer terms that contain it\n",
    "            longer_term_positions = []\n",
    "            for longer_term in nested_in_terms:\n",
    "                longer_pattern = r'\\b' + re.escape(longer_term) + r'\\b'\n",
    "                for match in re.finditer(longer_pattern, sentence_text_lower):\n",
    "                    longer_term_positions.append((match.start(), match.end()))\n",
    "            \n",
    "            # Check if term has an independent occurrence (not covered by longer terms)\n",
    "            has_independent_occurrence = False\n",
    "            for term_match in term_matches:\n",
    "                term_start = term_match.start()\n",
    "                term_end = term_match.end()\n",
    "                \n",
    "                # Check if this occurrence is covered by any longer term\n",
    "                is_covered = False\n",
    "                for longer_start, longer_end in longer_term_positions:\n",
    "                    if longer_start <= term_start and term_end <= longer_end:\n",
    "                        is_covered = True\n",
    "                        break\n",
    "                \n",
    "                # If this occurrence is not covered, it's independent\n",
    "                if not is_covered:\n",
    "                    has_independent_occurrence = True\n",
    "                    break\n",
    "            \n",
    "            # Only add if it appears independently\n",
    "            if has_independent_occurrence:\n",
    "                final_terms.append(term)\n",
    "            # Otherwise, skip it (it's nested and doesn't appear independently)\n",
    "        else:\n",
    "            # Not nested, add it\n",
    "            final_terms.append(term)\n",
    "    \n",
    "    # Return in original order (but without duplicates and nested terms)\n",
    "    result = []\n",
    "    seen_result = set()\n",
    "    for term in unique_terms:\n",
    "        if term in final_terms and term not in seen_result:\n",
    "            result.append(term)\n",
    "            seen_result.add(term)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Group by sentence and process terms\n",
    "processed_rows = []\n",
    "for (doc_id, para_id, sent_id), group in test_predictions_df.groupby(['document_id', 'paragraph_id', 'sentence_id']):\n",
    "    sentence_text = group.iloc[0]['sentence_text']\n",
    "    terms = group['term'].tolist()\n",
    "    terms = [t for t in terms if t and str(t).strip()]\n",
    "    \n",
    "    # Process terms: remove duplicates, nested, and invalid terms\n",
    "    processed_terms = remove_duplicates_and_nested(terms, sentence_text=sentence_text)\n",
    "    \n",
    "    # Add rows: one per term, or one empty row if no terms\n",
    "    if processed_terms:\n",
    "        for term in processed_terms:\n",
    "            processed_rows.append({\n",
    "                'document_id': doc_id,\n",
    "                'paragraph_id': para_id,\n",
    "                'sentence_id': sent_id,\n",
    "                'sentence_text': sentence_text,\n",
    "                'term': term\n",
    "            })\n",
    "    else:\n",
    "        processed_rows.append({\n",
    "            'document_id': doc_id,\n",
    "            'paragraph_id': para_id,\n",
    "            'sentence_id': sent_id,\n",
    "            'sentence_text': sentence_text,\n",
    "            'term': ''\n",
    "        })\n",
    "\n",
    "# Recreate DataFrame with processed terms\n",
    "test_predictions_df = pd.DataFrame(processed_rows)\n",
    "\n",
    "# Sort by document_id, paragraph_id, sentence_id to maintain order\n",
    "# Then empty terms first within each sentence (to match expected format)\n",
    "test_predictions_df['_term_empty'] = test_predictions_df['term'].str.strip() == ''\n",
    "test_predictions_df = test_predictions_df.sort_values(\n",
    "    by=['document_id', 'paragraph_id', 'sentence_id', '_term_empty', 'term'],\n",
    "    kind='stable',\n",
    "    ascending=[True, True, True, True, True]  # Empty terms first\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_predictions_df = test_predictions_df.drop(columns=['_term_empty'])\n",
    "\n",
    "print(f\"Post-processing complete: {len(test_predictions_df)} rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Ensure Correct Column Order and Save\n",
    "# ============================================================================\n",
    "column_order = ['document_id', 'paragraph_id', 'sentence_id', 'sentence_text', 'term']\n",
    "test_predictions_df = test_predictions_df[column_order]\n",
    "\n",
    "# Backup old predictions if they exist\n",
    "import shutil\n",
    "import datetime\n",
    "if os.path.exists(OLD_PREDICTIONS_PATH):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_path = OLD_PREDICTIONS_PATH.replace('.csv', f'_backup_{timestamp}.csv')\n",
    "    shutil.copy2(OLD_PREDICTIONS_PATH, backup_path)\n",
    "    print(f\"Backed up old predictions to: {backup_path}\")\n",
    "\n",
    "print(f\"\\nSaving predictions to: {OUTPUT_PATH}\")\n",
    "try:\n",
    "    test_predictions_df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    print(f\"Successfully saved {len(test_predictions_df)} rows to {OUTPUT_PATH}\")\n",
    "    print(f\"Format: document_id, paragraph_id, sentence_id, sentence_text, term\")\n",
    "    print(f\"This file includes improved filtering and ATE-IT format compliance\")\n",
    "    print(f\"Ready for ATE-IT submission\")\n",
    "    \n",
    "    # Also save to old path for compatibility\n",
    "    test_predictions_df.to_csv(OLD_PREDICTIONS_PATH, index=False, encoding='utf-8')\n",
    "    print(f\"Also saved to {OLD_PREDICTIONS_PATH} for compatibility\")\n",
    "except PermissionError:\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    alt_path = OUTPUT_PATH.replace('.csv', f'_{timestamp}.csv')\n",
    "    test_predictions_df.to_csv(alt_path, index=False, encoding='utf-8')\n",
    "    print(f\"Permission denied: {OUTPUT_PATH} is open\")\n",
    "    print(f\"Saved to: {alt_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Summary Statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET PREDICTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "total_sentences = len(test_predictions_df.groupby(['document_id', 'paragraph_id', 'sentence_id']))\n",
    "sentences_with_terms = len(test_predictions_df[\n",
    "    (test_predictions_df['term'].notna()) & \n",
    "    (test_predictions_df['term'].str.strip() != '')\n",
    "].groupby(['document_id', 'paragraph_id', 'sentence_id']))\n",
    "total_terms = len(test_predictions_df[\n",
    "    (test_predictions_df['term'].notna()) & \n",
    "    (test_predictions_df['term'].str.strip() != '')\n",
    "])\n",
    "\n",
    "print(f\"Total sentences: {total_sentences}\")\n",
    "print(f\"Sentences with terms: {sentences_with_terms}\")\n",
    "print(f\"Sentences without terms: {total_sentences - sentences_with_terms}\")\n",
    "print(f\"Total terms extracted: {total_terms}\")\n",
    "print(f\"Average terms per sentence: {total_terms / total_sentences:.2f}\" if total_sentences > 0 else \"N/A\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTEST SET PREDICTION EXPORT COMPLETE\")\n",
    "print(f\"Improved predictions: {OUTPUT_PATH}\")\n",
    "print(f\"Also saved to: {OLD_PREDICTIONS_PATH}\")\n",
    "print(\"Ready for ATE-IT submission\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Set Evaluation (Simple)\n",
    "\n",
    "This cell provides a streamlined evaluation interface for test set predictions. It loads predictions and optionally computes metrics if a gold standard is available.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "**Primary Mode - With Gold Standard:**\n",
    "- Loads predictions from `test_predictions_improved.csv` (or `test_predictions.csv` as fallback)\n",
    "- Loads gold standard from `test_ground_truth.csv`\n",
    "- Computes official ATE-IT metrics:\n",
    "  - **Micro-F1**: Term-level precision, recall, and F1 score\n",
    "  - **Type-F1**: Unique term type precision, recall, and F1 score\n",
    "- Displays additional statistics:\n",
    "  - Sentence coverage statistics\n",
    "  - Term type counts and intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST SET EVALUATION (IMPROVED PREDICTIONS)\n",
      "============================================================\n",
      "\n",
      " Loading test predictions...\n",
      " Loaded improved predictions from: test_predictions_improved.csv\n",
      " Loaded 1379 prediction rows\n",
      "\n",
      "Prediction Statistics:\n",
      "  Total sentences: 1142\n",
      "  Sentences with terms: 413 (36.2%)\n",
      "  Sentences without terms: 729\n",
      "  Total terms extracted: 650\n",
      "  Unique term types: 337\n",
      "  Average terms per sentence: 0.57\n",
      "\n",
      "Gold standard file not found: test_ground_truth.csv\n",
      "Showing prediction statistics only (no gold standard available)\n",
      "\n",
      "TEST SET EVALUATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST SET EVALUATION (WITH IMPROVED PREDICTIONS)\n",
    "# ============================================================================\n",
    "# This cell:\n",
    "# 1. Loads improved test predictions\n",
    "# 2. If gold standard exists, computes Micro-F1 and Type-F1\n",
    "# 3. Otherwise, shows prediction statistics\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "IMPROVED_PREDICTIONS_PATH = r\"test_predictions_improved.csv\"\n",
    "PREDICTIONS_PATH = r\"test_predictions.csv\"  # Fallback to old path\n",
    "GOLD_TRUTH_PATH = r\"test_ground_truth.csv\"  # Optional: manually created ground truth\n",
    "TEST_CSV_PATH = r\"test.csv\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET EVALUATION (IMPROVED PREDICTIONS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load predictions\n",
    "print(\"\\n Loading test predictions...\")\n",
    "if os.path.exists(IMPROVED_PREDICTIONS_PATH):\n",
    "    pred_df = pd.read_csv(IMPROVED_PREDICTIONS_PATH)\n",
    "    print(f\" Loaded improved predictions from: {IMPROVED_PREDICTIONS_PATH}\")\n",
    "elif os.path.exists(PREDICTIONS_PATH):\n",
    "    pred_df = pd.read_csv(PREDICTIONS_PATH)\n",
    "    print(f\" Loaded predictions from: {PREDICTIONS_PATH}\")\n",
    "else:\n",
    "    print(f\" Predictions file not found!\")\n",
    "    print(f\"  Please run the Test Set Predictions Export cell first.\")\n",
    "    raise FileNotFoundError(\"Predictions file not found\")\n",
    "\n",
    "pred_df.fillna('', inplace=True)\n",
    "print(f\" Loaded {len(pred_df)} prediction rows\")\n",
    "\n",
    "# Group predictions by sentence\n",
    "pred_groups = pred_df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "\n",
    "# Compute prediction statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "total_sentences = len(pred_groups)\n",
    "sentences_with_terms = len(pred_df[\n",
    "    (pred_df['term'].notna()) & \n",
    "    (pred_df['term'].str.strip() != '')\n",
    "].groupby(['document_id', 'paragraph_id', 'sentence_id']))\n",
    "total_terms = len(pred_df[\n",
    "    (pred_df['term'].notna()) & \n",
    "    (pred_df['term'].str.strip() != '')\n",
    "])\n",
    "unique_term_types = pred_df[\n",
    "    (pred_df['term'].notna()) & \n",
    "    (pred_df['term'].str.strip() != '')\n",
    "]['term'].str.strip().str.lower().nunique()\n",
    "\n",
    "print(f\"  Total sentences: {total_sentences}\")\n",
    "print(f\"  Sentences with terms: {sentences_with_terms} ({sentences_with_terms/total_sentences*100:.1f}%)\")\n",
    "print(f\"  Sentences without terms: {total_sentences - sentences_with_terms}\")\n",
    "print(f\"  Total terms extracted: {total_terms}\")\n",
    "print(f\"  Unique term types: {unique_term_types}\")\n",
    "print(f\"  Average terms per sentence: {total_terms / total_sentences:.2f}\" if total_sentences > 0 else \"N/A\")\n",
    "\n",
    "# Check if gold truth exists\n",
    "if os.path.exists(GOLD_TRUTH_PATH):\n",
    "    print(f\"\\nLoading gold standard from: {GOLD_TRUTH_PATH}\")\n",
    "    gold_df = pd.read_csv(GOLD_TRUTH_PATH)\n",
    "    gold_df.fillna('', inplace=True)\n",
    "    print(f\"Loaded {len(gold_df)} gold standard rows\")\n",
    "    \n",
    "    # Group gold standard by sentence\n",
    "    gold_groups = gold_df.groupby(['document_id', 'paragraph_id', 'sentence_id'])\n",
    "    \n",
    "    # Collect term lists\n",
    "    gold_terms_list = []\n",
    "    pred_terms_list = []\n",
    "    \n",
    "    # Get all unique sentences\n",
    "    all_sentences = set()\n",
    "    for (doc_id, para_id, sent_id), _ in pred_groups:\n",
    "        all_sentences.add((str(doc_id), str(para_id), str(sent_id)))\n",
    "    for (doc_id, para_id, sent_id), _ in gold_groups:\n",
    "        all_sentences.add((str(doc_id), str(para_id), str(sent_id)))\n",
    "    \n",
    "    all_sentences = sorted(all_sentences)\n",
    "    \n",
    "    print(f\"\\nComputing evaluation metrics...\")\n",
    "    # Extract terms for each sentence\n",
    "    for doc_id, para_id, sent_id in all_sentences:\n",
    "        # Get gold terms\n",
    "        if (doc_id, para_id, sent_id) in gold_groups.groups:\n",
    "            gold_group = gold_groups.get_group((doc_id, para_id, sent_id))\n",
    "            gold_terms = [str(t).strip().lower() for t in gold_group['term'].tolist() \n",
    "                         if t and str(t).strip() != '']\n",
    "        else:\n",
    "            gold_terms = []\n",
    "        \n",
    "        # Get predicted terms\n",
    "        if (doc_id, para_id, sent_id) in pred_groups.groups:\n",
    "            pred_group = pred_groups.get_group((doc_id, para_id, sent_id))\n",
    "            pred_terms = [str(t).strip().lower() for t in pred_group['term'].tolist() \n",
    "                         if t and str(t).strip() != '']\n",
    "        else:\n",
    "            pred_terms = []\n",
    "        \n",
    "        gold_terms_list.append(gold_terms)\n",
    "        pred_terms_list.append(pred_terms)\n",
    "    \n",
    "    # Compute metrics using the same functions from the notebook\n",
    "    micro_metrics = compute_micro_f1(gold_terms_list, pred_terms_list)\n",
    "    type_metrics = compute_type_f1(gold_terms_list, pred_terms_list)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST SET EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Micro-Precision: {micro_metrics['micro_precision']:.4f}\")\n",
    "    print(f\"Micro-Recall: {micro_metrics['micro_recall']:.4f}\")\n",
    "    print(f\"Micro-F1: {micro_metrics['micro_f1']:.4f}\")\n",
    "    print(f\"\\nType-Precision: {type_metrics['type_precision']:.4f}\")\n",
    "    print(f\"Type-Recall: {type_metrics['type_recall']:.4f}\")\n",
    "    print(f\"Type-F1: {type_metrics['type_f1']:.4f}\")\n",
    "    print(f\"\\nGold term types: {type_metrics['num_gold_types']}\")\n",
    "    print(f\"Pred term types: {type_metrics['num_pred_types']}\")\n",
    "    print(f\"Intersection: {type_metrics['num_intersection']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Additional statistics\n",
    "    sentences_with_gold_terms = sum(1 for terms in gold_terms_list if len(terms) > 0)\n",
    "    sentences_with_pred_terms = sum(1 for terms in pred_terms_list if len(terms) > 0)\n",
    "    \n",
    "    print(f\"\\nAdditional Statistics:\")\n",
    "    print(f\"Total sentences: {len(gold_terms_list)}\")\n",
    "    print(f\"Sentences with gold terms: {sentences_with_gold_terms}\")\n",
    "    print(f\"Sentences with predicted terms: {sentences_with_pred_terms}\")\n",
    "    print(f\"Coverage: {sentences_with_pred_terms / len(gold_terms_list) * 100:.1f}% of sentences have predictions\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nGold standard file not found: {GOLD_TRUTH_PATH}\")\n",
    "    print(\"Showing prediction statistics only (no gold standard available)\")\n",
    "\n",
    "print(\"\\nTEST SET EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete Automatic Term Extraction (ATE) system for the ATE-IT Shared Task (EVALITA 2026), Subtask A.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Hybrid Approach**: Classical NLP preprocessing (SpaCy) + Transformer-based sequence labeling\n",
    "\n",
    "2. **BIO Tagging**: Uses B-TERM, I-TERM, O labels for token classification\n",
    "\n",
    "3. **Italian Transformer Models**: Fine-tuned dbmdz/bert-base-italian-uncased for token classification task\n",
    "\n",
    "4. **ATE-IT Evaluation**: Implements exact Micro-F1 and Type-F1 metrics\n",
    "\n",
    "5. **Constraints Handling**: No nested terms, no duplicates, domain-specific filtering\n",
    "\n",
    "6. **Post-Processing**: Enhanced filtering with stopword removal, format normalization, and ATE-IT format compliance\n",
    "\n",
    "### Dependencies:\n",
    "\n",
    "- transformers\n",
    "- torch\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- spacy + it_core_news_sm\n",
    "- tqdm\n",
    "- datasets\n",
    "- seqeval\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- `./ate_it_final_model/`: Final saved model directory\n",
    "- `test_predictions_improved.csv`: Test set predictions with improved filtering (primary submission file)\n",
    "- `test_predictions.csv`: Test set predictions (backup/compatibility file)\n",
    "- `dev_predictions.csv`: Development set predictions (if generated)\n",
    "\n",
    "### Output Format:\n",
    "\n",
    "- CSV format with columns: `document_id`, `paragraph_id`, `sentence_id`, `sentence_text`, `term`\n",
    "- All terms normalized to lowercase\n",
    "- No duplicate terms within sentences\n",
    "- No nested terms (unless they appear independently)\n",
    "- One row per term per sentence\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "- **Micro-F1**: Term-level precision, recall, and F1 score\n",
    "- **Type-F1**: Unique term type precision, recall, and F1 score\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
