{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask B - Term Variants Clustering: Production Pipeline\n",
    "\n",
    "This notebook implements a comprehensive production pipeline for clustering term variants in the waste management domain.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Cluster terms that refer to the same underlying concept\n",
    "- **Domain**: Municipal waste management (Italian)\n",
    "- **Evaluation**: BCubed F1 Score\n",
    "\n",
    "## Pipeline Components\n",
    "1. Data loading and preprocessing\n",
    "2. Linguistic preprocessing (lemmatization, POS tagging)\n",
    "3. Multiple clustering approaches:\n",
    "   - LLM-based clustering (Gemini)\n",
    "   - Embedding-based semantic similarity\n",
    "   - Hybrid approach\n",
    "4. Evaluation and error analysis\n",
    "5. Output generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import codecs\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP libraries\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy.lang.it import Italian\n",
    "    SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"Warning: spaCy not available. Install with: pip install spacy && python -m spacy download it_core_news_sm\")\n",
    "\n",
    "# Embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"Warning: sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "\n",
    "# LLM - Using Hugging Face Transformers (Open Source)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"Warning: transformers not available. Install with: pip install transformers torch\")\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for the clustering pipeline\"\"\"\n",
    "    \n",
    "    # Data paths\n",
    "    TRAIN_PATH = \"subtask_b_train.csv\"\n",
    "    DEV_PATH = \"subtask_b_dev.csv\"\n",
    "    \n",
    "    # Clustering parameters\n",
    "    EMBEDDING_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"  # Multilingual model\n",
    "    SIMILARITY_THRESHOLD = 0.75  # Threshold for semantic similarity clustering\n",
    "    DBSCAN_EPS = 0.3  # DBSCAN epsilon parameter\n",
    "    DBSCAN_MIN_SAMPLES = 2  # DBSCAN min_samples parameter\n",
    "    \n",
    "    # Strict clustering parameters (for anti-monster cluster pipeline)\n",
    "    STRICT_SIMILARITY_THRESHOLD = 0.90  # Higher threshold for strict clustering\n",
    "    MONSTER_CLUSTER_THRESHOLD = 10  # Cluster size threshold for breaking\n",
    "    USE_LLM_VERIFICATION = False  # Whether to use LLM edge verification (slower)\n",
    "    \n",
    "    # LLM parameters (Open Source - Hugging Face)\n",
    "    LLM_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"  # Small, fast model. Alternative: \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    LLM_USE_LOCAL = True  # Set to False to use Hugging Face API (requires API key)\n",
    "    LLM_BATCH_SIZE = 10  # Number of terms per LLM batch (smaller for local models)\n",
    "    LLM_TEMPERATURE = 0.1  # Lower temperature for more consistent outputs\n",
    "    LLM_MAX_LENGTH = 512  # Maximum generation length\n",
    "    \n",
    "    # Linguistic preprocessing\n",
    "    USE_LEMMATIZATION = True\n",
    "    USE_POS_FILTERING = False  # Filter by POS tags if needed\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_FORMAT = \"csv\"  # \"json\" or \"csv\" - CSV is recommended for submission\n",
    "    OUTPUT_PREFIX = \"production_run\"\n",
    "    \n",
    "    # Evaluation\n",
    "    EVALUATE_ON_DEV = True\n",
    "    \n",
    "    @classmethod\n",
    "    def update(cls, **kwargs):\n",
    "        \"\"\"Update configuration parameters\"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(cls, key):\n",
    "                setattr(cls, key, value)\n",
    "            else:\n",
    "                print(f\"Warning: Unknown config parameter: {key}\")\n",
    "\n",
    "config = Config() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linguistic Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_terms(file_path: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load unique terms from CSV or JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to input file (CSV or JSON)\n",
    "    \n",
    "    Returns:\n",
    "        Set of unique terms\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.fillna('', inplace=True)\n",
    "        terms = set([t.strip() for t in df['term'] if t != ''])\n",
    "    elif file_path.endswith('.json'):\n",
    "        with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        terms = set()\n",
    "        for row in json_data.get(\"data\", []):\n",
    "            if \"term_list\" in row:\n",
    "                terms.update(row[\"term_list\"])\n",
    "            elif \"term\" in row:\n",
    "                terms.add(row[\"term\"])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only CSV and JSON files are supported.\")\n",
    "    return terms\n",
    "\n",
    "def load_gold_standard(file_path: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Load gold standard clustering from CSV or JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to gold standard file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping terms to cluster IDs\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        data = {term: int(cluster) for term, cluster in df.itertuples(index=False)}\n",
    "    elif file_path.endswith('.json'):\n",
    "        with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        data = {item[\"term\"]: item[\"cluster\"] for item in json_data.get(\"data\", [])}\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only CSV and JSON files are supported.\")\n",
    "    return data\n",
    "\n",
    "def save_output(clustering: Dict[str, int], output_path: str, format: str = \"csv\"):\n",
    "    \"\"\"\n",
    "    Save clustering results to file.\n",
    "    \n",
    "    Args:\n",
    "        clustering: Dictionary mapping terms to cluster IDs\n",
    "        output_path: Path to output file\n",
    "        format: Output format (\"json\" or \"csv\")\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.dirname(output_path) if os.path.dirname(output_path) else '.'\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Sort by cluster ID then term for consistent output\n",
    "    sorted_items = sorted(clustering.items(), key=lambda x: (x[1], x[0]))\n",
    "    \n",
    "    if format == \"json\":\n",
    "        json_data = {\"data\": []}\n",
    "        for term, cluster in sorted_items:\n",
    "            json_data[\"data\"].append({\"term\": term, \"cluster\": int(cluster)})\n",
    "        with codecs.open(output_path, 'w', 'utf-8') as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "    elif format == \"csv\":\n",
    "        df = pd.DataFrame([{\"term\": term, \"cluster\": int(cluster)}\n",
    "                          for term, cluster in sorted_items])\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {format}\")\n",
    "    \n",
    "    abs_path = os.path.abspath(output_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PREDICTION FILE SAVED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Location: {abs_path}\")\n",
    "    print(f\"  Format: {format.upper()}\")\n",
    "    print(f\"  Total terms: {len(clustering)}\")\n",
    "    print(f\"  Total clusters: {len(set(clustering.values()))}\")\n",
    "    print(f\"{'='*60}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BCubed Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimal_threshold(train_file: str, method: str = \"hybrid\",\n",
    "                             threshold_range: List[float] = None) -> float:\n",
    "    \"\"\"\n",
    "    Train optimal similarity threshold on training data.\n",
    "    \n",
    "    Args:\n",
    "        train_file: Path to training file with gold standard\n",
    "        method: Clustering method to use\n",
    "        threshold_range: List of thresholds to try (default: [0.65, 0.70, 0.75, 0.80, 0.85])\n",
    "    \n",
    "    Returns:\n",
    "        Optimal threshold value\n",
    "    \"\"\"\n",
    "    if threshold_range is None:\n",
    "        threshold_range = [0.65, 0.70, 0.75, 0.80, 0.85]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training: Finding Optimal Similarity Threshold\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_terms = load_terms(train_file)\n",
    "    train_gold = load_gold_standard(train_file)\n",
    "    train_terms_list = sorted(list(train_terms))\n",
    "    \n",
    "    print(f\"Training on {len(train_terms_list)} terms from {train_file}\")\n",
    "    print(f\"Trying thresholds: {threshold_range}\\n\")\n",
    "    \n",
    "    best_threshold = threshold_range[0]\n",
    "    best_f1 = 0.0\n",
    "    results = []\n",
    "    \n",
    "    for threshold in tqdm(threshold_range, desc=\"Testing thresholds\"):\n",
    "        # Temporarily update config\n",
    "        original_threshold = config.SIMILARITY_THRESHOLD\n",
    "        config.SIMILARITY_THRESHOLD = threshold\n",
    "        \n",
    "        try:\n",
    "            # Run clustering with this threshold\n",
    "            if method == \"hybrid\":\n",
    "                embedding_clusterer = None\n",
    "                if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "                    embedding_clusterer = EmbeddingClusterer(\n",
    "                        similarity_threshold=threshold\n",
    "                    )\n",
    "                \n",
    "                hybrid_clusterer = HybridClusterer(\n",
    "                    preprocessor=preprocessor,\n",
    "                    embedding_clusterer=embedding_clusterer,\n",
    "                    llm_clusterer=None  # No LLM during training\n",
    "                )\n",
    "                clustering = hybrid_clusterer.cluster(train_terms_list, use_llm_refinement=False)\n",
    "            elif method == \"embedding\":\n",
    "                if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "                    continue\n",
    "                embedding_clusterer = EmbeddingClusterer(similarity_threshold=threshold)\n",
    "                clustering = embedding_clusterer.cluster_by_similarity(train_terms_list)\n",
    "            else:\n",
    "                # For lemma method, threshold doesn't apply\n",
    "                continue\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = evaluate_clustering(train_gold, clustering, verbose=False)\n",
    "            f1 = metrics['f1']\n",
    "            results.append((threshold, f1, metrics['precision'], metrics['recall']))\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with threshold {threshold}: {e}\")\n",
    "            continue\n",
    "        finally:\n",
    "            config.SIMILARITY_THRESHOLD = original_threshold\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING SET EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for threshold, f1, precision, recall in results:\n",
    "        marker = \" [BEST]\" if threshold == best_threshold else \"\"\n",
    "        print(f\"{threshold:<12.2f} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f}{marker}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Training Set BCubed F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Update config with best threshold\n",
    "    config.SIMILARITY_THRESHOLD = best_threshold\n",
    "    \n",
    "    # Save trained parameters to file for use on test data\n",
    "    import os\n",
    "    import json\n",
    "    trained_params = {\n",
    "        \"optimal_threshold\": best_threshold,\n",
    "        \"method\": method,\n",
    "        \"train_file\": train_file,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"training_date\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    params_file = \"trained_parameters.json\"\n",
    "    with open(params_file, 'w') as f:\n",
    "        json.dump(trained_params, f, indent=2)\n",
    "    print(f\"Trained parameters saved to: {os.path.abspath(params_file)}\")\n",
    "    print(f\"  These parameters will be used when running on test data.\\n\")\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def load_trained_parameters(params_file: str = \"trained_parameters.json\") -> Dict:\n",
    "    \"\"\"\n",
    "    Load trained parameters from file.\n",
    "    \n",
    "    Args:\n",
    "        params_file: Path to trained parameters file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trained parameters\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    if os.path.exists(params_file):\n",
    "        with open(params_file, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        print(f\"Loaded trained parameters from: {os.path.abspath(params_file)}\")\n",
    "        print(f\"  Optimal threshold: {params['optimal_threshold']:.3f}\")\n",
    "        print(f\"  Method: {params['method']}\")\n",
    "        print(f\"  Training F1: {params['best_f1']:.4f}\\n\")\n",
    "        \n",
    "        # Update config with loaded parameters\n",
    "        config.SIMILARITY_THRESHOLD = params['optimal_threshold']\n",
    "        \n",
    "        return params\n",
    "    else:\n",
    "        print(f\"WARNING: No trained parameters found at: {params_file}\")\n",
    "        print(\"   Using default parameters. Run training first!\\n\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCubedCalculator:\n",
    "    \"\"\"BCubed metric calculator for clustering evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, gold: Dict[str, int], pred: Dict[str, int]):\n",
    "        self.gold = gold\n",
    "        self.pred = pred\n",
    "        self.gold_cluster = defaultdict(set)\n",
    "        self.pred_cluster = defaultdict(set)\n",
    "        \n",
    "        # Build cluster mappings\n",
    "        for item, clus_id in gold.items():\n",
    "            self.gold_cluster[clus_id].add(item)\n",
    "        for item, clus_id in pred.items():\n",
    "            self.pred_cluster[clus_id].add(item)\n",
    "    \n",
    "    def bc_precision_item(self, item: str) -> float:\n",
    "        \"\"\"Calculate BCubed precision for a single item\"\"\"\n",
    "        pred_id = self.pred.get(item, None)\n",
    "        gold_id = self.gold.get(item, None)\n",
    "        \n",
    "        if pred_id is None or gold_id is None:\n",
    "            return 0.0\n",
    "        \n",
    "        pred_cluster = self.pred_cluster[pred_id]\n",
    "        gold_cluster = self.gold_cluster[gold_id]\n",
    "        \n",
    "        TP = len(pred_cluster.intersection(gold_cluster))\n",
    "        FP = len(pred_cluster) - TP\n",
    "        \n",
    "        if TP + FP == 0:\n",
    "            return 0.0\n",
    "        return TP / (TP + FP)\n",
    "    \n",
    "    def bc_recall_item(self, item: str) -> float:\n",
    "        \"\"\"Calculate BCubed recall for a single item\"\"\"\n",
    "        pred_id = self.pred.get(item, None)\n",
    "        gold_id = self.gold.get(item, None)\n",
    "        \n",
    "        if pred_id is None or gold_id is None:\n",
    "            return 0.0\n",
    "        \n",
    "        pred_cluster = self.pred_cluster[pred_id]\n",
    "        gold_cluster = self.gold_cluster[gold_id]\n",
    "        \n",
    "        TP = len(pred_cluster.intersection(gold_cluster))\n",
    "        FN = len(gold_cluster) - TP\n",
    "        \n",
    "        if TP + FN == 0:\n",
    "            return 0.0\n",
    "        return TP / (TP + FN)\n",
    "\n",
    "def bcubed_precision(gold: Dict[str, int], pred: Dict[str, int]) -> float:\n",
    "    \"\"\"Calculate BCubed precision\"\"\"\n",
    "    calc = BCubedCalculator(gold, pred)\n",
    "    pred_items = list(calc.pred.keys())\n",
    "    if not pred_items:\n",
    "        return 0.0\n",
    "    return np.average([calc.bc_precision_item(item) for item in pred_items])\n",
    "\n",
    "def bcubed_recall(gold: Dict[str, int], pred: Dict[str, int]) -> float:\n",
    "    \"\"\"Calculate BCubed recall\"\"\"\n",
    "    calc = BCubedCalculator(gold, pred)\n",
    "    gold_items = list(calc.gold.keys())\n",
    "    if not gold_items:\n",
    "        return 0.0\n",
    "    return np.average([calc.bc_recall_item(item) for item in gold_items])\n",
    "\n",
    "def bcubed_f1(gold: Dict[str, int], pred: Dict[str, int]) -> float:\n",
    "    \"\"\"Calculate BCubed F1 score\"\"\"\n",
    "    precision = bcubed_precision(gold, pred)\n",
    "    recall = bcubed_recall(gold, pred)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def evaluate_clustering(gold: Dict[str, int], pred: Dict[str, int], verbose: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using BCubed metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    precision = bcubed_precision(gold, pred)\n",
    "    recall = bcubed_recall(gold, pred)\n",
    "    f1 = bcubed_f1(gold, pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"BCubed Precision: {precision:.4f}\")\n",
    "        print(f\"BCubed Recall: {recall:.4f}\")\n",
    "        print(f\"BCubed F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Italian spaCy model\n"
     ]
    }
   ],
   "source": [
    "class LinguisticPreprocessor:\n",
    "    \"\"\"Linguistic preprocessing for Italian terms\"\"\"\n",
    "    \n",
    "    def __init__(self, use_lemmatization: bool = True):\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.nlp = None\n",
    "        \n",
    "        if use_lemmatization and SPACY_AVAILABLE:\n",
    "            try:\n",
    "                self.nlp = spacy.load(\"it_core_news_sm\")\n",
    "                print(\"Loaded Italian spaCy model\")\n",
    "            except OSError:\n",
    "                print(\"Warning: Italian spaCy model not found. Install with: python -m spacy download it_core_news_sm\")\n",
    "                self.nlp = None\n",
    "    \n",
    "    def normalize_term(self, term: str) -> str:\n",
    "        \"\"\"Normalize a term (lowercase, strip)\"\"\"\n",
    "        return term.lower().strip()\n",
    "    \n",
    "    def lemmatize(self, term: str) -> str:\n",
    "        \"\"\"Lemmatize a term\"\"\"\n",
    "        if self.nlp is None:\n",
    "            return self.normalize_term(term)\n",
    "        \n",
    "        doc = self.nlp(term)\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "        return \" \".join(lemmas)\n",
    "    \n",
    "    def get_lemma_map(self, terms: Set[str]) -> Dict[str, str]:\n",
    "        \"\"\"Create mapping from terms to their lemmas\"\"\"\n",
    "        lemma_map = {}\n",
    "        for term in tqdm(terms, desc=\"Lemmatizing terms\"):\n",
    "            lemma_map[term] = self.lemmatize(term)\n",
    "        return lemma_map\n",
    "    \n",
    "    def group_by_lemma(self, terms: Set[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Group terms by their lemmas\"\"\"\n",
    "        lemma_map = self.get_lemma_map(terms)\n",
    "        lemma_groups = defaultdict(list)\n",
    "        for term, lemma in lemma_map.items():\n",
    "            lemma_groups[lemma].append(term)\n",
    "        return dict(lemma_groups)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = LinguisticPreprocessor(use_lemmatization=config.USE_LEMMATIZATION) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding-Based Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClusterer:\n",
    "    \"\"\"Clustering based on semantic embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = None, similarity_threshold: float = 0.75):\n",
    "        self.model_name = model_name or config.EMBEDDING_MODEL\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.model = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "        if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(\"Model loaded successfully\")\n",
    "        else:\n",
    "            print(\"Warning: sentence-transformers not available\")\n",
    "    \n",
    "    def compute_embeddings(self, terms: List[str]) -> np.ndarray:\n",
    "        \"\"\"Compute embeddings for a list of terms\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "        \n",
    "        print(f\"Computing embeddings for {len(terms)} terms...\")\n",
    "        embeddings = self.model.encode(terms, show_progress_bar=True, convert_to_numpy=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def cluster_by_similarity(self, terms: List[str], embeddings: np.ndarray = None) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Cluster terms based on cosine similarity threshold.\n",
    "        Uses a greedy approach to assign terms to clusters.\n",
    "        \"\"\"\n",
    "        if embeddings is None:\n",
    "            embeddings = self.compute_embeddings(terms)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Initialize clustering\n",
    "        clustering = {}\n",
    "        cluster_id = 1\n",
    "        assigned = set()\n",
    "        \n",
    "        # Greedy clustering: assign each term to the first cluster it's similar to\n",
    "        for i, term in enumerate(terms):\n",
    "            if term in assigned:\n",
    "                continue\n",
    "            \n",
    "            # Find similar terms\n",
    "            similar_indices = np.where(similarity_matrix[i] >= self.similarity_threshold)[0]\n",
    "            \n",
    "            if len(similar_indices) > 1:  # Found similar terms\n",
    "                # Assign all similar terms to the same cluster\n",
    "                for idx in similar_indices:\n",
    "                    if terms[idx] not in assigned:\n",
    "                        clustering[terms[idx]] = cluster_id\n",
    "                        assigned.add(terms[idx])\n",
    "                cluster_id += 1\n",
    "            else:  # No similar terms found, create singleton cluster\n",
    "                clustering[term] = cluster_id\n",
    "                assigned.add(term)\n",
    "                cluster_id += 1\n",
    "        \n",
    "        return clustering\n",
    "    \n",
    "    def cluster_with_dbscan(self, terms: List[str], embeddings: np.ndarray = None, \n",
    "                           eps: float = 0.3, min_samples: int = 2) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Cluster terms using DBSCAN algorithm on embeddings.\n",
    "        \"\"\"\n",
    "        if embeddings is None:\n",
    "            embeddings = self.compute_embeddings(terms)\n",
    "        \n",
    "        # Convert similarity to distance (1 - similarity)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        \n",
    "        # Apply DBSCAN\n",
    "        clustering_alg = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "        labels = clustering_alg.fit_predict(distance_matrix)\n",
    "        \n",
    "        # Convert labels to cluster IDs (DBSCAN uses -1 for noise)\n",
    "        clustering = {}\n",
    "        max_cluster_id = 0\n",
    "        \n",
    "        for term, label in zip(terms, labels):\n",
    "            if label == -1:  # Noise point, assign to new cluster\n",
    "                clustering[term] = max_cluster_id + 1\n",
    "                max_cluster_id += 1\n",
    "            else:\n",
    "                clustering[term] = int(label) + 1  # Shift to start from 1\n",
    "        \n",
    "        return clustering\n",
    "    \n",
    "    def cluster_with_hierarchical(self, terms: List[str], embeddings: np.ndarray = None,\n",
    "                                 n_clusters: int = None, linkage_method: str = 'ward') -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Cluster terms using hierarchical clustering.\n",
    "        \"\"\"\n",
    "        if embeddings is None:\n",
    "            embeddings = self.compute_embeddings(terms)\n",
    "        \n",
    "        if n_clusters is None:\n",
    "            # Estimate number of clusters (rough heuristic: ~10% of terms)\n",
    "            n_clusters = max(2, len(terms) // 10)\n",
    "        \n",
    "        clustering_alg = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters, \n",
    "            linkage=linkage_method,\n",
    "            metric='cosine'\n",
    "        )\n",
    "        labels = clustering_alg.fit_predict(embeddings)\n",
    "        \n",
    "        clustering = {term: int(label) + 1 for term, label in zip(terms, labels)}\n",
    "        return clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-Based Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClusterer:\n",
    "    \"\"\"Clustering using Large Language Models (Hugging Face Transformers - Open Source)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = None, batch_size: int = 10, use_local: bool = True):\n",
    "        self.model_name = model_name or config.LLM_MODEL\n",
    "        self.batch_size = batch_size\n",
    "        self.use_local = use_local\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                if use_local:\n",
    "                    print(f\"Loading local model: {self.model_name}\")\n",
    "                    print(\"Note: First run will download the model (~2-14GB depending on model)\")\n",
    "                    \n",
    "                    # Use a smaller, faster model for local inference\n",
    "                    if \"Phi-3\" not in self.model_name and \"mistral\" not in self.model_name.lower():\n",
    "                        # Default to a smaller model\n",
    "                        self.model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "                        print(f\"Using smaller model: {self.model_name}\")\n",
    "                    \n",
    "                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                    \n",
    "                    # Create pipeline for easier text generation\n",
    "                    self.pipeline = pipeline(\n",
    "                        \"text-generation\",\n",
    "                        model=self.model,\n",
    "                        tokenizer=self.tokenizer,\n",
    "                        device=0 if torch.cuda.is_available() else -1,\n",
    "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "                    )\n",
    "                    print(f\"Model loaded successfully on {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "                else:\n",
    "                    # Use Hugging Face Inference API (requires API key)\n",
    "                    print(\"Using Hugging Face Inference API\")\n",
    "                    try:\n",
    "                        from huggingface_hub import InferenceClient\n",
    "                        import os\n",
    "                        api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "                        if not api_key:\n",
    "                            api_key = input(\"Please enter your Hugging Face API key (or press Enter to skip LLM): \")\n",
    "                        if api_key:\n",
    "                            self.client = InferenceClient(model=self.model_name, token=api_key)\n",
    "                        else:\n",
    "                            print(\"No API key provided. LLM clustering will be skipped.\")\n",
    "                    except ImportError:\n",
    "                        print(\"huggingface_hub not installed. Install with: pip install huggingface_hub\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load LLM model: {e}\")\n",
    "                print(\"LLM clustering will be disabled. Consider using 'embedding' or 'hybrid' methods.\")\n",
    "        else:\n",
    "            print(\"Warning: transformers not available\")\n",
    "    \n",
    "    def create_system_prompt(self) -> str:\n",
    "        \"\"\"Create system prompt for clustering\"\"\"\n",
    "        return f\"\"\"You are a term clustering agent specialized in Italian municipal waste management terminology.\n",
    "\n",
    "You will receive a list of existing clusters and a list of unclustered terms.\n",
    "Your task is to cluster together terms that refer to the same underlying concept.\n",
    "\n",
    "Rules:\n",
    "1. Group terms by meaning, not form. Consider their lemmas and semantic meaning.\n",
    "2. Focus on their meaning within the municipal waste management context.\n",
    "3. Inflected forms of the same lemma should be clustered together (e.g., \"isola ecologica\" and \"isole ecologiche\").\n",
    "4. Acronyms, initialisms, and their full forms should be clustered together (e.g., \"ccr\" and \"centro comunale di raccolta\").\n",
    "5. Synonyms should be clustered together (e.g., \"isola ecologica\" and \"centro comunale di raccolta\").\n",
    "6. Hypernyms and hyponyms should NOT be clustered together (e.g., \"rifiuti\" and \"rifiuti indifferenziati\" must be in different clusters).\n",
    "7. Each cluster must represent a single, well-defined concept.\n",
    "\n",
    "Output format:\n",
    "Return the list of clusters with newly added terms. Each cluster must be on a new line.\n",
    "Terms within a cluster should be separated by semicolons.\n",
    "\n",
    "Example Output:\n",
    "centro di raccolta; ccr; isola ecologica; isole ecologiche\n",
    "indifferenziato; secco residuo\n",
    "raccolta porta a porta; raccolta domiciliare\n",
    "\n",
    "Instructions:\n",
    "- If a term does not belong to any existing cluster, create a new cluster for it.\n",
    "- Do not remove terms from existing clusters.\n",
    "- Your response must contain only the clustered terms in the specified format. Do not add any introductory text or explanations.\n",
    "- Process all {self.batch_size} unclustered terms.\n",
    "\"\"\"\n",
    "    \n",
    "    def parse_clusters(self, cluster_text: str) -> List[List[str]]:\n",
    "        \"\"\"Parse cluster text into list of clusters\"\"\"\n",
    "        clusters = []\n",
    "        for line in cluster_text.strip().split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split by semicolon and clean terms\n",
    "            terms = [t.strip() for t in line.split(';') if t.strip()]\n",
    "            if terms:\n",
    "                clusters.append(terms)\n",
    "        return clusters\n",
    "    \n",
    "    def cluster_terms(self, terms: List[str], initial_clusters: List[List[str]] = None) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Cluster terms using LLM in batches.\n",
    "        \n",
    "        Args:\n",
    "            terms: List of terms to cluster\n",
    "            initial_clusters: Optional initial clusters (from previous batches)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping terms to cluster IDs\n",
    "        \"\"\"\n",
    "        if self.pipeline is None and not hasattr(self, 'client'):\n",
    "            raise ValueError(\"LLM model not initialized\")\n",
    "        \n",
    "        # Initialize clusters\n",
    "        if initial_clusters is None:\n",
    "            clusters = []\n",
    "        else:\n",
    "            clusters = [cluster.copy() for cluster in initial_clusters]\n",
    "        \n",
    "        # Process terms in batches\n",
    "        system_prompt = self.create_system_prompt()\n",
    "        \n",
    "        for batch_start in tqdm(range(0, len(terms), self.batch_size), desc=\"Processing batches\"):\n",
    "            batch_terms = terms[batch_start:batch_start + self.batch_size]\n",
    "            \n",
    "            # Format existing clusters\n",
    "            if clusters:\n",
    "                cluster_text = \"\\n\".join([\"; \".join(cluster) for cluster in clusters])\n",
    "                user_prompt = f\"CLUSTERS:\\n{cluster_text}\\n\\nUNCLUSTERED TERMS:\\n\" + \"\\n\".join(batch_terms)\n",
    "            else:\n",
    "                user_prompt = f\"UNCLUSTERED TERMS:\\n\" + \"\\n\".join(batch_terms)\n",
    "            \n",
    "            # Get LLM response\n",
    "            try:\n",
    "                if self.pipeline is not None:\n",
    "                    # Local model inference\n",
    "                    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\\n\\nOutput:\"\n",
    "                    response = self.pipeline(\n",
    "                        full_prompt,\n",
    "                        max_new_tokens=config.LLM_MAX_LENGTH,\n",
    "                        temperature=config.LLM_TEMPERATURE,\n",
    "                        do_sample=True,\n",
    "                        return_full_text=False,\n",
    "                        truncation=True\n",
    "                    )\n",
    "                    response_text = response[0]['generated_text'].strip()\n",
    "                elif hasattr(self, 'client'):\n",
    "                    # Hugging Face API\n",
    "                    response = self.client.text_generation(\n",
    "                        f\"{system_prompt}\\n\\n{user_prompt}\\n\\nOutput:\",\n",
    "                        max_new_tokens=config.LLM_MAX_LENGTH,\n",
    "                        temperature=config.LLM_TEMPERATURE\n",
    "                    )\n",
    "                    response_text = response.strip()\n",
    "                else:\n",
    "                    raise ValueError(\"LLM model not initialized\")\n",
    "                \n",
    "                # Clean response - remove instruction text that might be included\n",
    "                lines = response_text.split('\\n')\n",
    "                cleaned_lines = []\n",
    "                skip_keywords = ['instructions:', 'example', 'output format:', 'rules:', 'return the list']\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    line_lower = line.lower()\n",
    "                    # Skip instruction lines (unless they contain semicolons, which means they're clusters)\n",
    "                    if any(keyword in line_lower for keyword in skip_keywords) and ';' not in line:\n",
    "                        continue\n",
    "                    # Skip lines that are clearly instructions\n",
    "                    if line_lower.startswith(('instructions', 'example output', 'output format', 'rules')):\n",
    "                        continue\n",
    "                    cleaned_lines.append(line)\n",
    "                \n",
    "                response_text = '\\n'.join(cleaned_lines).strip()\n",
    "                \n",
    "                # Parse response\n",
    "                new_clusters = self.parse_clusters(response_text)\n",
    "                \n",
    "                # Update clusters\n",
    "                clusters = new_clusters\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Fallback: create singleton clusters for this batch\n",
    "                for term in batch_terms:\n",
    "                    clusters.append([term])\n",
    "        \n",
    "        # Convert clusters to dictionary\n",
    "        clustering = {}\n",
    "        for cluster_id, cluster in enumerate(clusters, start=1):\n",
    "            for term in cluster:\n",
    "                clustering[term] = cluster_id\n",
    "        \n",
    "        # Verify all input terms are in clustering\n",
    "        input_terms_set = set(terms)\n",
    "        clustered_terms_set = set(clustering.keys())\n",
    "        missing_terms = input_terms_set - clustered_terms_set\n",
    "        \n",
    "        if missing_terms:\n",
    "            print(f\"\\nWARNING: {len(missing_terms)} terms not found in LLM output. Adding as singletons...\")\n",
    "            max_cluster_id = max(clustering.values()) if clustering else 0\n",
    "            for i, term in enumerate(sorted(missing_terms), start=1):\n",
    "                clustering[term] = max_cluster_id + i\n",
    "        \n",
    "        return clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridClusterer:\n",
    "    \"\"\"\n",
    "    Hybrid clustering approach combining:\n",
    "    1. Lemma-based grouping\n",
    "    2. Embedding-based similarity\n",
    "    3. LLM-based refinement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: LinguisticPreprocessor, \n",
    "                 embedding_clusterer: EmbeddingClusterer,\n",
    "                 llm_clusterer: LLMClusterer = None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.embedding_clusterer = embedding_clusterer\n",
    "        self.llm_clusterer = llm_clusterer\n",
    "    \n",
    "    def cluster(self, terms: List[str], use_llm_refinement: bool = False) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Perform hybrid clustering.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Start with lemma-based grouping\n",
    "        2. Use embeddings to merge similar clusters\n",
    "        3. Optionally refine with LLM\n",
    "        \"\"\"\n",
    "        print(f\"Clustering {len(terms)} terms using hybrid approach...\")\n",
    "        \n",
    "        # Step 1: Lemma-based initial clustering\n",
    "        print(\"Step 1: Lemma-based grouping...\")\n",
    "        lemma_groups = self.preprocessor.group_by_lemma(set(terms))\n",
    "        \n",
    "        # Create initial clustering from lemma groups\n",
    "        initial_clustering = {}\n",
    "        cluster_id = 1\n",
    "        for lemma, lemma_terms in lemma_groups.items():\n",
    "            for term in lemma_terms:\n",
    "                initial_clustering[term] = cluster_id\n",
    "            cluster_id += 1\n",
    "        \n",
    "        # Step 2: Embedding-based refinement\n",
    "        print(\"Step 2: Embedding-based refinement...\")\n",
    "        if self.embedding_clusterer.model is not None:\n",
    "            embeddings = self.embedding_clusterer.compute_embeddings(terms)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Merge clusters based on similarity\n",
    "            term_to_idx = {term: i for i, term in enumerate(terms)}\n",
    "            \n",
    "            # Build cluster representatives (average embedding per cluster)\n",
    "            cluster_embeddings = defaultdict(list)\n",
    "            for term, cluster_id in initial_clustering.items():\n",
    "                if term in term_to_idx:\n",
    "                    cluster_embeddings[cluster_id].append(embeddings[term_to_idx[term]])\n",
    "            \n",
    "            cluster_repr = {cid: np.mean(embs, axis=0) for cid, embs in cluster_embeddings.items()}\n",
    "            \n",
    "            # Merge similar clusters\n",
    "            cluster_ids = list(cluster_repr.keys())\n",
    "            merged = {cid: cid for cid in cluster_ids}  # Track merges\n",
    "            \n",
    "            for i, cid1 in enumerate(cluster_ids):\n",
    "                for cid2 in cluster_ids[i+1:]:\n",
    "                    if merged[cid1] != merged[cid2]:\n",
    "                        sim = cosine_similarity(\n",
    "                            cluster_repr[cid1].reshape(1, -1),\n",
    "                            cluster_repr[cid2].reshape(1, -1)\n",
    "                        )[0][0]\n",
    "                        \n",
    "                        if sim >= config.SIMILARITY_THRESHOLD:\n",
    "                            # Merge cid2 into cid1\n",
    "                            target = merged[cid1]\n",
    "                            source = merged[cid2]\n",
    "                            for cid in cluster_ids:\n",
    "                                if merged[cid] == source:\n",
    "                                    merged[cid] = target\n",
    "            \n",
    "            # Apply merges to clustering\n",
    "            final_clustering = {term: merged[initial_clustering[term]] for term in terms}\n",
    "            \n",
    "            # Renumber clusters to be consecutive\n",
    "            unique_clusters = sorted(set(final_clustering.values()))\n",
    "            cluster_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_clusters, start=1)}\n",
    "            final_clustering = {term: cluster_mapping[cluster_id] \n",
    "                             for term, cluster_id in final_clustering.items()}\n",
    "        else:\n",
    "            final_clustering = initial_clustering\n",
    "        \n",
    "        # Step 3: Optional LLM refinement\n",
    "        if use_llm_refinement and self.llm_clusterer is not None:\n",
    "            print(\"Step 3: LLM-based refinement...\")\n",
    "            # Convert clustering to cluster lists for LLM\n",
    "            cluster_lists = defaultdict(list)\n",
    "            for term, cluster_id in final_clustering.items():\n",
    "                cluster_lists[cluster_id].append(term)\n",
    "            \n",
    "            initial_clusters = [terms for terms in cluster_lists.values()]\n",
    "            \n",
    "            # Re-cluster with LLM (this will refine the clusters)\n",
    "            llm_clustering = self.llm_clusterer.cluster_terms(terms, initial_clusters)\n",
    "            final_clustering = llm_clustering\n",
    "        \n",
    "        return final_clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Strict Hybrid Clustering (Anti-Monster Cluster Pipeline)\n",
    "\n",
    "This refactored pipeline implements 4 strict modules to prevent \"monster clusters\" and improve precision:\n",
    "\n",
    "1. **Pre-Processing**: Acronym & Substring Solver\n",
    "2. **Refined Embedding Clustering**: Anti-Lumping Logic (threshold 0.90, domain stop-word penalty, antonym guardrail)\n",
    "3. **Post-Processing**: Monster Cluster Breaker\n",
    "4. **LLM Refinement**: Verification Only (YES/NO edge verification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class AcronymSubstringSolver:\n",
    "    \"\"\"\n",
    "    Module 1: Pre-Processing - Acronym & Substring Solver\n",
    "    Deterministic rule-based step to lock obvious matches before embedding clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Domain stop-words that should NOT trigger substring matches\n",
    "    DOMAIN_STOPWORDS = {\n",
    "        'rifiuti', 'rifiuto', 'raccolta', 'servizio', 'servizi',\n",
    "        'gestione', 'comunale', 'comune', 'municipale', 'municipio'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, preprocessor: LinguisticPreprocessor = None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.locked_pairs = set()  # Track locked term pairs\n",
    "    \n",
    "    def is_acronym(self, term: str) -> bool:\n",
    "        \"\"\"Check if a term is likely an acronym (all caps, short, no spaces)\"\"\"\n",
    "        term_clean = term.strip().upper()\n",
    "        # Acronym: 2-10 chars, all uppercase letters/numbers, no spaces\n",
    "        if len(term_clean) >= 2 and len(term_clean) <= 10:\n",
    "            if term_clean.replace(' ', '') == term_clean and term_clean.isalnum():\n",
    "                # Check if it's mostly letters (at least 50%)\n",
    "                if sum(c.isalpha() for c in term_clean) >= len(term_clean) * 0.5:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def extract_acronym_letters(self, term: str) -> str:\n",
    "        \"\"\"Extract potential acronym letters from a term (first letters of words)\"\"\"\n",
    "        words = term.upper().split()\n",
    "        # Get first letter of each significant word (skip articles/prepositions)\n",
    "        articles = {'DI', 'DE', 'DEL', 'DELLA', 'DELLE', 'DEI', 'DEGLI', 'DA', 'E', 'A', 'AL', 'ALL', 'ALLA', 'ALLE'}\n",
    "        letters = []\n",
    "        for word in words:\n",
    "            if word and word[0].isalpha() and word not in articles:\n",
    "                letters.append(word[0])\n",
    "        return ''.join(letters)\n",
    "    \n",
    "    def is_acronym_match(self, term1: str, term2: str) -> bool:\n",
    "        \"\"\"Check if term1 is an acronym of term2 or vice versa\"\"\"\n",
    "        t1_upper = term1.upper().strip()\n",
    "        t2_upper = term2.upper().strip()\n",
    "        \n",
    "        # Direct match\n",
    "        if t1_upper == t2_upper:\n",
    "            return True\n",
    "        \n",
    "        # Check if one is acronym and other is expansion\n",
    "        if self.is_acronym(t1_upper):\n",
    "            # term1 is acronym, check if it matches term2's letters\n",
    "            t2_letters = self.extract_acronym_letters(term2)\n",
    "            if t1_upper == t2_letters or t1_upper.replace(' ', '') == t2_letters:\n",
    "                return True\n",
    "        \n",
    "        if self.is_acronym(t2_upper):\n",
    "            # term2 is acronym, check if it matches term1's letters\n",
    "            t1_letters = self.extract_acronym_letters(term1)\n",
    "            if t2_upper == t1_letters or t2_upper.replace(' ', '') == t1_letters:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_strict_substring_match(self, term1: str, term2: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if terms match via exact substring (strict, excluding domain stop-words).\n",
    "        Only matches if lemmatized forms are very similar.\n",
    "        \"\"\"\n",
    "        if not self.preprocessor:\n",
    "            return False\n",
    "        \n",
    "        # Get lemmas\n",
    "        try:\n",
    "            lemma1 = self.preprocessor.lemmatize(term1)\n",
    "            lemma2 = self.preprocessor.lemmatize(term2)\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "        # Exact lemma match\n",
    "        if lemma1.lower() == lemma2.lower():\n",
    "            return True\n",
    "        \n",
    "        # Check if one is contained in the other (but not just due to stop-words)\n",
    "        lemma1_lower = lemma1.lower()\n",
    "        lemma2_lower = lemma2.lower()\n",
    "        \n",
    "        # Remove domain stop-words for comparison\n",
    "        words1 = set(w for w in lemma1_lower.split() if w not in self.DOMAIN_STOPWORDS)\n",
    "        words2 = set(w for w in lemma2_lower.split() if w not in self.DOMAIN_STOPWORDS)\n",
    "        \n",
    "        # If after removing stop-words, one is subset of other and they share significant words\n",
    "        if words1 and words2:\n",
    "            if words1.issubset(words2) or words2.issubset(words1):\n",
    "                # Check if they share at least one significant word\n",
    "                if words1 & words2:\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def solve(self, terms: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Apply acronym and substring matching to create initial locked clusters.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping terms to cluster IDs (locked clusters)\n",
    "        \"\"\"\n",
    "        print(\"Module 1: Acronym & Substring Solver\")\n",
    "        print(f\"  Processing {len(terms)} terms...\")\n",
    "        \n",
    "        clustering = {}\n",
    "        cluster_id = 1\n",
    "        assigned = set()\n",
    "        locked_clusters = []  # List of locked cluster term sets\n",
    "        \n",
    "        # Process each term\n",
    "        for i, term1 in enumerate(terms):\n",
    "            if term1 in assigned:\n",
    "                continue\n",
    "            \n",
    "            # Find matches for this term\n",
    "            matches = [term1]\n",
    "            \n",
    "            for term2 in terms[i+1:]:\n",
    "                if term2 in assigned:\n",
    "                    continue\n",
    "                \n",
    "                # Check acronym match\n",
    "                if self.is_acronym_match(term1, term2):\n",
    "                    matches.append(term2)\n",
    "                    self.locked_pairs.add((term1, term2))\n",
    "                    assigned.add(term2)\n",
    "                    continue\n",
    "                \n",
    "                # Check strict substring match\n",
    "                if self.is_strict_substring_match(term1, term2):\n",
    "                    matches.append(term2)\n",
    "                    self.locked_pairs.add((term1, term2))\n",
    "                    assigned.add(term2)\n",
    "                    continue\n",
    "            \n",
    "            # Assign cluster ID to all matches\n",
    "            for term in matches:\n",
    "                clustering[term] = cluster_id\n",
    "                assigned.add(term)\n",
    "            \n",
    "            if len(matches) > 1:\n",
    "                locked_clusters.append(matches)\n",
    "            \n",
    "            cluster_id += 1\n",
    "        \n",
    "        print(f\"  Created {len(locked_clusters)} locked clusters from acronym/substring matches\")\n",
    "        print(f\"  {sum(len(c) for c in locked_clusters)} terms locked\")\n",
    "        \n",
    "        return clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefinedEmbeddingClusterer:\n",
    "    \"\"\"\n",
    "    Module 2: Refined Embedding Clustering - Anti-Lumping Logic\n",
    "    Uses higher threshold (0.90), domain stop-word penalty, and antonym guardrail.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Domain stop-words that should be penalized in similarity calculation\n",
    "    DOMAIN_STOPWORDS = {\n",
    "        'rifiuti', 'rifiuto', 'raccolta', 'servizio', 'servizi',\n",
    "        'gestione', 'comunale', 'comune', 'municipale', 'municipio'\n",
    "    }\n",
    "    \n",
    "    # Antonym pairs that should NOT be merged\n",
    "    ANTONYM_PAIRS = [\n",
    "        ('differenziata', 'indifferenziata'),\n",
    "        ('differenziato', 'indifferenziato'),\n",
    "        ('differenziati', 'indifferenziati'),\n",
    "        ('secco', 'umido'),\n",
    "        ('secca', 'umida'),\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, embedding_clusterer: EmbeddingClusterer,\n",
    "                 similarity_threshold: float = 0.90):\n",
    "        self.embedding_clusterer = embedding_clusterer\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def has_antonym_conflict(self, term1: str, term2: str) -> bool:\n",
    "        \"\"\"Check if two terms contain antonyms\"\"\"\n",
    "        term1_lower = term1.lower()\n",
    "        term2_lower = term2.lower()\n",
    "        \n",
    "        for ant1, ant2 in self.ANTONYM_PAIRS:\n",
    "            if ant1 in term1_lower and ant2 in term2_lower:\n",
    "                return True\n",
    "            if ant2 in term1_lower and ant1 in term2_lower:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def calculate_penalized_similarity(self, term1: str, term2: str,\n",
    "                                       base_similarity: float) -> float:\n",
    "        \"\"\"\n",
    "        Apply domain stop-word penalty to similarity score.\n",
    "        If terms only match due to stop-words, penalize heavily.\n",
    "        \"\"\"\n",
    "        # Check if terms are identical (no penalty)\n",
    "        if term1.lower() == term2.lower():\n",
    "            return base_similarity\n",
    "        \n",
    "        # Extract non-stop-word tokens\n",
    "        words1 = set(w.lower() for w in term1.split() if w.lower() not in self.DOMAIN_STOPWORDS)\n",
    "        words2 = set(w.lower() for w in term2.split() if w.lower() not in self.DOMAIN_STOPWORDS)\n",
    "        \n",
    "        # If both have no significant words after removing stop-words, heavy penalty\n",
    "        if not words1 or not words2:\n",
    "            return base_similarity * 0.5  # 50% penalty\n",
    "        \n",
    "        # Calculate overlap of significant words\n",
    "        if words1 & words2:\n",
    "            # They share significant words, no penalty\n",
    "            return base_similarity\n",
    "        else:\n",
    "            # They only share stop-words, apply penalty\n",
    "            penalty = 0.3  # 70% penalty\n",
    "            return base_similarity * penalty\n",
    "    \n",
    "    def cluster_with_anti_lumping(self, terms: List[str],\n",
    "                                  initial_clustering: Dict[str, int],\n",
    "                                  embeddings: np.ndarray) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Perform refined embedding clustering with anti-lumping logic.\n",
    "        \n",
    "        Args:\n",
    "            terms: List of all terms\n",
    "            initial_clustering: Initial clustering from Module 1 (locked clusters)\n",
    "            embeddings: Pre-computed embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Updated clustering dictionary\n",
    "        \"\"\"\n",
    "        print(\"Module 2: Refined Embedding Clustering (Anti-Lumping)\")\n",
    "        print(f\"  Using strict threshold: {self.similarity_threshold:.2f}\")\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        term_to_idx = {term: i for i, term in enumerate(terms)}\n",
    "        \n",
    "        # Track which clusters are locked (from Module 1)\n",
    "        locked_clusters = set(initial_clustering.values())\n",
    "        cluster_members = defaultdict(set)\n",
    "        for term, cid in initial_clustering.items():\n",
    "            cluster_members[cid].add(term)\n",
    "        \n",
    "        # Build cluster representatives (average embedding per cluster)\n",
    "        cluster_embeddings = defaultdict(list)\n",
    "        for term, cluster_id in initial_clustering.items():\n",
    "            if term in term_to_idx:\n",
    "                cluster_embeddings[cluster_id].append(embeddings[term_to_idx[term]])\n",
    "        \n",
    "        cluster_repr = {cid: np.mean(embs, axis=0) for cid, embs in cluster_embeddings.items()}\n",
    "        \n",
    "        # Track merges (only for unlocked clusters)\n",
    "        cluster_ids = list(cluster_repr.keys())\n",
    "        merged = {cid: cid for cid in cluster_ids}\n",
    "        \n",
    "        # Try to merge clusters (but respect locked clusters and apply penalties)\n",
    "        merge_count = 0\n",
    "        for i, cid1 in enumerate(cluster_ids):\n",
    "            for cid2 in cluster_ids[i+1:]:\n",
    "                if merged[cid1] == merged[cid2]:\n",
    "                    continue\n",
    "                \n",
    "                # Don't merge if both are locked clusters (from Module 1)\n",
    "                if cid1 in locked_clusters and cid2 in locked_clusters:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate base similarity\n",
    "                base_sim = cosine_similarity(\n",
    "                    cluster_repr[cid1].reshape(1, -1),\n",
    "                    cluster_repr[cid2].reshape(1, -1)\n",
    "                )[0][0]\n",
    "                \n",
    "                # Get representative terms for penalty calculation\n",
    "                rep_term1 = list(cluster_members[cid1])[0] if cluster_members[cid1] else \"\"\n",
    "                rep_term2 = list(cluster_members[cid2])[0] if cluster_members[cid2] else \"\"\n",
    "                \n",
    "                # Check antonym conflict\n",
    "                if rep_term1 and rep_term2 and self.has_antonym_conflict(rep_term1, rep_term2):\n",
    "                    continue  # Skip merge due to antonym\n",
    "                \n",
    "                # Apply domain stop-word penalty\n",
    "                penalized_sim = self.calculate_penalized_similarity(rep_term1, rep_term2, base_sim)\n",
    "                \n",
    "                # Merge only if penalized similarity exceeds threshold\n",
    "                if penalized_sim >= self.similarity_threshold:\n",
    "                    # Merge cid2 into cid1\n",
    "                    target = merged[cid1]\n",
    "                    source = merged[cid2]\n",
    "                    for cid in cluster_ids:\n",
    "                        if merged[cid] == source:\n",
    "                            merged[cid] = target\n",
    "                    merge_count += 1\n",
    "        \n",
    "        # Apply merges to clustering\n",
    "        final_clustering = {}\n",
    "        for term in terms:\n",
    "            original_cid = initial_clustering.get(term, None)\n",
    "            if original_cid is None:\n",
    "                # Term not in initial clustering, assign to new cluster\n",
    "                max_cid = max(merged.values()) if merged else 0\n",
    "                final_clustering[term] = max_cid + 1\n",
    "            else:\n",
    "                final_clustering[term] = merged[original_cid]\n",
    "        \n",
    "        # Renumber clusters to be consecutive\n",
    "        unique_clusters = sorted(set(final_clustering.values()))\n",
    "        cluster_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_clusters, start=1)}\n",
    "        final_clustering = {term: cluster_mapping[cluster_id]\n",
    "                           for term, cluster_id in final_clustering.items()}\n",
    "        \n",
    "        print(f\"  Merged {merge_count} cluster pairs (strict threshold applied)\")\n",
    "        print(f\"  Final clusters: {len(unique_clusters)}\")\n",
    "        \n",
    "        return final_clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonsterClusterBreaker:\n",
    "    \"\"\"\n",
    "    Module 3: Post-Processing - Monster Cluster Breaker\n",
    "    Identifies clusters with size > 10 and re-clusters them with stricter criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_clusterer: EmbeddingClusterer,\n",
    "                 refined_clusterer: RefinedEmbeddingClusterer,\n",
    "                 monster_threshold: int = 10,\n",
    "                 break_threshold: float = 0.85):\n",
    "        self.embedding_clusterer = embedding_clusterer\n",
    "        self.refined_clusterer = refined_clusterer\n",
    "        self.monster_threshold = monster_threshold\n",
    "        self.break_threshold = break_threshold  # Stricter threshold for breaking\n",
    "    \n",
    "    def break_monster_clusters(self, terms: List[str],\n",
    "                              clustering: Dict[str, int],\n",
    "                              embeddings: np.ndarray) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Identify and break monster clusters (size > threshold).\n",
    "        \n",
    "        Args:\n",
    "            terms: List of all terms\n",
    "            clustering: Current clustering\n",
    "            embeddings: Pre-computed embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Updated clustering with monster clusters broken\n",
    "        \"\"\"\n",
    "        print(\"Module 3: Monster Cluster Breaker\")\n",
    "        \n",
    "        # Group terms by cluster\n",
    "        cluster_members = defaultdict(list)\n",
    "        for term in terms:\n",
    "            if term in clustering:\n",
    "                cluster_members[clustering[term]].append(term)\n",
    "        \n",
    "        # Find monster clusters\n",
    "        monster_clusters = {cid: members\n",
    "                           for cid, members in cluster_members.items()\n",
    "                           if len(members) > self.monster_threshold}\n",
    "        \n",
    "        if not monster_clusters:\n",
    "            print(f\"  No monster clusters found (all clusters  {self.monster_threshold} items)\")\n",
    "            return clustering\n",
    "        \n",
    "        print(f\"  Found {len(monster_clusters)} monster cluster(s) to break:\")\n",
    "        for cid, members in monster_clusters.items():\n",
    "            print(f\"    Cluster {cid}: {len(members)} items\")\n",
    "        \n",
    "        # Create updated clustering\n",
    "        updated_clustering = clustering.copy()\n",
    "        term_to_idx = {term: i for i, term in enumerate(terms)}\n",
    "        max_cluster_id = max(clustering.values()) if clustering else 0\n",
    "        \n",
    "        # Process each monster cluster\n",
    "        break_count = 0\n",
    "        for monster_cid, monster_terms in monster_clusters.items():\n",
    "            if len(monster_terms) <= self.monster_threshold:\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Breaking cluster {monster_cid} ({len(monster_terms)} items)...\")\n",
    "            \n",
    "            # Get embeddings for this cluster\n",
    "            monster_indices = [term_to_idx[t] for t in monster_terms if t in term_to_idx]\n",
    "            if len(monster_indices) < 2:\n",
    "                continue\n",
    "            \n",
    "            monster_embeddings = embeddings[monster_indices]\n",
    "            \n",
    "            # Use AgglomerativeClustering with stricter criteria\n",
    "            # Determine number of clusters: aim for average cluster size of 3-5\n",
    "            target_cluster_size = 4\n",
    "            n_clusters = max(2, len(monster_terms) // target_cluster_size)\n",
    "            \n",
    "            # Use cosine distance with stricter linkage\n",
    "            clustering_alg = AgglomerativeClustering(\n",
    "                n_clusters=n_clusters,\n",
    "                linkage='average',  # Average linkage is more conservative\n",
    "                metric='cosine',\n",
    "                distance_threshold=None\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                labels = clustering_alg.fit_predict(monster_embeddings)\n",
    "                \n",
    "                # Check internal similarity - if clusters are too dissimilar, break further\n",
    "                new_cluster_groups = defaultdict(list)\n",
    "                for term, label in zip(monster_terms, labels):\n",
    "                    new_cluster_groups[label].append(term)\n",
    "                \n",
    "                # For each new sub-cluster, check if it should be broken further\n",
    "                final_subclusters = {}\n",
    "                current_new_cid = max_cluster_id + 1\n",
    "                \n",
    "                for sub_label, sub_terms in new_cluster_groups.items():\n",
    "                    if len(sub_terms) <= 3:\n",
    "                        # Small cluster, keep as is\n",
    "                        for term in sub_terms:\n",
    "                            final_subclusters[term] = current_new_cid\n",
    "                        current_new_cid += 1\n",
    "                    else:\n",
    "                        # Check internal similarity\n",
    "                        sub_indices = [term_to_idx[t] for t in sub_terms if t in term_to_idx]\n",
    "                        if len(sub_indices) > 1:\n",
    "                            sub_embeddings = embeddings[sub_indices]\n",
    "                            sub_sim_matrix = cosine_similarity(sub_embeddings)\n",
    "                            \n",
    "                            # Calculate average internal similarity\n",
    "                            n = len(sub_terms)\n",
    "                            avg_sim = (sub_sim_matrix.sum() - n) / (n * (n - 1))  # Exclude diagonal\n",
    "                            \n",
    "                            if avg_sim >= self.break_threshold:\n",
    "                                # High internal similarity, keep together\n",
    "                                for term in sub_terms:\n",
    "                                    final_subclusters[term] = current_new_cid\n",
    "                                current_new_cid += 1\n",
    "                            else:\n",
    "                                # Low internal similarity, break into singletons or pairs\n",
    "                                print(f\"    Low internal similarity ({avg_sim:.3f}), breaking into smaller groups\")\n",
    "                                for term in sub_terms:\n",
    "                                    final_subclusters[term] = current_new_cid\n",
    "                                    current_new_cid += 1\n",
    "                        else:\n",
    "                            # Singleton\n",
    "                            for term in sub_terms:\n",
    "                                final_subclusters[term] = current_new_cid\n",
    "                            current_new_cid += 1\n",
    "                \n",
    "                # Update clustering\n",
    "                for term, new_cid in final_subclusters.items():\n",
    "                    updated_clustering[term] = new_cid\n",
    "                \n",
    "                break_count += 1\n",
    "                max_cluster_id = current_new_cid - 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not break cluster {monster_cid}: {e}\")\n",
    "                # Keep original cluster if breaking fails\n",
    "                continue\n",
    "        \n",
    "        # Renumber all clusters to be consecutive\n",
    "        unique_clusters = sorted(set(updated_clustering.values()))\n",
    "        cluster_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_clusters, start=1)}\n",
    "        final_clustering = {term: cluster_mapping[cluster_id]\n",
    "                           for term, cluster_id in updated_clustering.items()}\n",
    "        \n",
    "        print(f\"  Broke {break_count} monster cluster(s)\")\n",
    "        print(f\"  Final cluster count: {len(unique_clusters)}\")\n",
    "        \n",
    "        return final_clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEdgeVerifier:  \"\"\"\n",
    "Module 4: LLM Refinement - Verification Only\n",
    "Uses LLM to verify edges (term pairs) with YES/NO questions.\n",
    "Only breaks links\n",
    "if LLM says NO.  \"\"\"   def __init__(self, llm_clusterer: LLMClusterer):  self.llm_clusterer = llm_clusterer\n",
    "def verify_edge(self, term1: str, term2: str) -> bool:  \"\"\"\n",
    "Ask LLM\n",
    "if two terms are synonyms referring to the exact same concept. \n",
    "Returns:\n",
    "True\n",
    "if LLM says YES (should be in same cluster), False\n",
    "if NO  \"\"\"  if self.llm_clusterer is None or (not hasattr(self.llm_clusterer, 'pipeline') and not hasattr(self.llm_clusterer, 'client')):  # LLM not available, skip verification (assume edge is valid)  return True   prompt = f\"\"\"Are '{term1}' and '{term2}' synonyms referring to the exact same concept in Italian municipal waste management?\n",
    "Answer YES or NO only. Do not provide any explanation.\n",
    "Answer:\"\"\"   try:  if hasattr(self.llm_clusterer, 'pipeline') and self.llm_clusterer.pipeline is not None:  # Local model  response = self.llm_clusterer.pipeline(  prompt,  max_new_tokens=10,  temperature=0.1,  do_sample=False,  return_full_text=False  )  answer = response[0]['generated_text'].strip().upper()  elif hasattr(self.llm_clusterer, 'client') and self.llm_clusterer.client is not None:  # API  response = self.llm_clusterer.client.text_generation(  prompt,  max_new_tokens=10,  temperature=0.1  )  answer = response.strip().upper()  else:  return True # Default to keeping edge\n",
    "if LLM unavailable\n",
    "# Parse YES/NO\n",
    "if 'YES' in answer or 'SI' in answer:  return True\n",
    "elif 'NO' in answer:  return False\n",
    "else:  # Unclear response, default to keeping edge (conservative)  return True\n",
    "except Exception as e:  # Error in LLM call, default to keeping edge\n",
    "return True\n",
    "def verify_clustering_edges(self, terms: List[str],  clustering: Dict[str, int]) -> Dict[str, int]:  \"\"\"\n",
    "Verify edges in clustering and break links where LLM says NO. \n",
    "Strategy:  - For each cluster, check pairs of terms  - If LLM says NO, break the link (assign to different clusters)  \"\"\"  print(\"Module 4: LLM Edge Verification\")   if self.llm_clusterer is None:  print(\" WARNING: LLM not available, skipping verification\")  return clustering\n",
    "# Group terms by cluster  cluster_members = defaultdict(list)  for term in terms:  if term in clustering:  cluster_members[clustering[term]].append(term)   # Track edges to break  edges_to_break = set()  verified_count = 0\n",
    "# For each cluster, verify edges\n",
    "for cluster_id, members in cluster_members.items():  if len(members) < 2:  continue # Singleton, nothing to verify\n",
    "# Check pairs in cluster\n",
    "for i, term1 in enumerate(members):  for term2 in members[i+1:]:  verified_count += 1\n",
    "if verified_count % 10 == 0:  print(f\" Verified {verified_count} edges...\", end='\\r')   # Ask LLM  should_keep = self.verify_edge(term1, term2)   if not should_keep:  # LLM says NO, mark edge\n",
    "for breaking  edges_to_break.add((term1, term2))   print(f\"\\n Verified {verified_count} edges\")   if not edges_to_break:  print(\" All edges verified as valid\")  return clustering\n",
    "print(f\" WARNING: Breaking {len(edges_to_break)} invalid edges\")   # Break edges by reassigning terms  updated_clustering = clustering.copy()  max_cluster_id = max(clustering.values()) if clustering else 0  new_cluster_id = max_cluster_id + 1\n",
    "# For each broken edge, assign one term to a new cluster\n",
    "for term1, term2 in edges_to_break:  # Keep term1 in original cluster, move term2 to new cluster  updated_clustering[term2] = new_cluster_id  new_cluster_id += 1\n",
    "# Renumber clusters to be consecutive  unique_clusters = sorted(set(updated_clustering.values()))  cluster_mapping = {old_id: new_id\n",
    "for new_id, old_id in enumerate(unique_clusters, start=1)}  final_clustering = {term: cluster_mapping[cluster_id]  for term, cluster_id in updated_clustering.items()}   print(f\" Final cluster count: {len(unique_clusters)}\")   return final_clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrictHybridClusterer:\n",
    "    \"\"\"\n",
    "    Strict Hybrid Clustering Pipeline - Anti-Monster Cluster System\n",
    "    Orchestrates 4 modules in order:\n",
    "    1. Acronym & Substring Solver (pre-processing)\n",
    "    2. Refined Embedding Clustering (anti-lumping logic)\n",
    "    3. Monster Cluster Breaker (post-processing)\n",
    "    4. LLM Edge Verification (verification only)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 preprocessor: LinguisticPreprocessor,\n",
    "                 embedding_clusterer: EmbeddingClusterer,\n",
    "                 llm_clusterer: LLMClusterer = None,\n",
    "                 similarity_threshold: float = 0.90,\n",
    "                 monster_threshold: int = 10,\n",
    "                 use_llm_verification: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize strict hybrid clusterer.\n",
    "        \n",
    "        Args:\n",
    "            preprocessor: Linguistic preprocessor for lemmatization\n",
    "            embedding_clusterer: Embedding clusterer for semantic similarity\n",
    "            llm_clusterer: Optional LLM clusterer for edge verification\n",
    "            similarity_threshold: Strict similarity threshold (default 0.90)\n",
    "            monster_threshold: Cluster size threshold for breaking (default 10)\n",
    "            use_llm_verification: Whether to use LLM edge verification (slower)\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "        self.embedding_clusterer = embedding_clusterer\n",
    "        self.llm_clusterer = llm_clusterer\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.monster_threshold = monster_threshold\n",
    "        self.use_llm_verification = use_llm_verification\n",
    "        \n",
    "        # Initialize modules\n",
    "        self.acronym_solver = AcronymSubstringSolver(preprocessor=preprocessor)\n",
    "        self.refined_clusterer = RefinedEmbeddingClusterer(\n",
    "            embedding_clusterer=embedding_clusterer,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        self.monster_breaker = MonsterClusterBreaker(\n",
    "            embedding_clusterer=embedding_clusterer,\n",
    "            refined_clusterer=self.refined_clusterer,\n",
    "            monster_threshold=monster_threshold\n",
    "        )\n",
    "        self.llm_verifier = LLMEdgeVerifier(llm_clusterer=llm_clusterer) if llm_clusterer else None\n",
    "    \n",
    "    def cluster(self, terms: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Perform strict hybrid clustering using all 4 modules.\n",
    "        \n",
    "        Args:\n",
    "            terms: List of terms to cluster\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping terms to cluster IDs\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"STRICT HYBRID CLUSTERING PIPELINE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Clustering {len(terms)} terms with anti-monster cluster system...\")\n",
    "        print(f\"Similarity threshold: {self.similarity_threshold:.2f}\")\n",
    "        print(f\"Monster cluster threshold: {self.monster_threshold} items\")\n",
    "        print(f\"LLM verification: {'ON' if self.use_llm_verification else 'OFF'}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Module 1: Acronym & Substring Solver\n",
    "        initial_clustering = self.acronym_solver.solve(terms)\n",
    "        \n",
    "        # Ensure all terms are in clustering (singletons for unmatched terms)\n",
    "        max_cid = max(initial_clustering.values()) if initial_clustering else 0\n",
    "        for term in terms:\n",
    "            if term not in initial_clustering:\n",
    "                max_cid += 1\n",
    "                initial_clustering[term] = max_cid\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Module 2: Refined Embedding Clustering\n",
    "        if self.embedding_clusterer.model is not None:\n",
    "            print(\"Computing embeddings...\")\n",
    "            embeddings = self.embedding_clusterer.compute_embeddings(terms)\n",
    "            print()\n",
    "            \n",
    "            clustering = self.refined_clusterer.cluster_with_anti_lumping(\n",
    "                terms=terms,\n",
    "                initial_clustering=initial_clustering,\n",
    "                embeddings=embeddings\n",
    "            )\n",
    "        else:\n",
    "            print(\"WARNING: Embedding model not available, skipping Module 2\")\n",
    "            clustering = initial_clustering\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Module 3: Monster Cluster Breaker\n",
    "        if self.embedding_clusterer.model is not None:\n",
    "            clustering = self.monster_breaker.break_monster_clusters(\n",
    "                terms=terms,\n",
    "                clustering=clustering,\n",
    "                embeddings=embeddings\n",
    "            )\n",
    "        else:\n",
    "            print(\"WARNING: Embedding model not available, skipping Module 3\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Module 4: LLM Edge Verification (optional)\n",
    "        if self.use_llm_verification and self.llm_verifier is not None:\n",
    "            clustering = self.llm_verifier.verify_clustering_edges(\n",
    "                terms=terms,\n",
    "                clustering=clustering\n",
    "            )\n",
    "        else:\n",
    "            print(\"Module 4: LLM Edge Verification - SKIPPED (use_llm_verification=False)\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"CLUSTERING COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Final statistics\n",
    "        cluster_sizes = Counter(clustering.values())\n",
    "        print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "        print(f\"Average cluster size: {np.mean(list(cluster_sizes.values())):.2f}\")\n",
    "        print(f\"Largest cluster: {max(cluster_sizes.values())} items\")\n",
    "        print(f\"Singleton clusters: {sum(1 for size in cluster_sizes.values() if size == 1)}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Training on Training Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train optimal parameters on training set\n",
    "def train_and_evaluate(train_file: str, dev_file: str, method: str = \"hybrid\",\n",
    "                       optimize_threshold: bool = True, use_llm: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train on training set and evaluate on development set.\n",
    "    \n",
    "    Args:\n",
    "        train_file: Path to training file with gold standard\n",
    "        dev_file: Path to development file with gold standard\n",
    "        method: Clustering method\n",
    "        optimize_threshold: Whether to optimize similarity threshold on training set\n",
    "        use_llm: Whether to use LLM refinement\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation metrics on development set\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING AND EVALUATION PIPELINE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Step 1: Train on training set\n",
    "    train_metrics = None\n",
    "    if optimize_threshold and method in [\"hybrid\", \"embedding\"]:\n",
    "        print(\"STEP 1: Training on training set...\")\n",
    "        optimal_threshold = train_optimal_threshold(train_file, method=method)\n",
    "        print(f\"Optimal threshold learned: {optimal_threshold:.2f}\\n\")\n",
    "        \n",
    "        # Evaluate on training set with optimal threshold\n",
    "        print(\"Evaluating on training set with optimal threshold...\")\n",
    "        train_terms = load_terms(train_file)\n",
    "        train_terms_list = sorted(list(train_terms))\n",
    "        train_gold = load_gold_standard(train_file)\n",
    "        \n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "            pd.DataFrame({\"term\": train_terms_list}).to_csv(f.name, index=False)\n",
    "            temp_train_file = f.name\n",
    "        \n",
    "        train_clustering = run_pipeline(\n",
    "            input_file=temp_train_file,\n",
    "            method=method,\n",
    "            output_file=None,  # Don't save training predictions\n",
    "            gold_standard_file=None,\n",
    "            use_llm=False,\n",
    "            use_trained_params=False  # Already using optimal threshold\n",
    "        )\n",
    "        \n",
    "        train_metrics = evaluate_clustering(train_gold, train_clustering, verbose=False)\n",
    "        print(f\"Training Set - Precision: {train_metrics['precision']:.4f}, Recall: {train_metrics['recall']:.4f}, F1: {train_metrics['f1']:.4f}\\n\")\n",
    "    else:\n",
    "        print(\"STEP 1: Skipping threshold optimization (not applicable for this method)\\n\")\n",
    "    \n",
    "    # Step 2: Evaluate on development set\n",
    "    print(\"STEP 2: Evaluating on development set...\")\n",
    "    dev_terms = load_terms(dev_file)\n",
    "    dev_terms_list = sorted(list(dev_terms))\n",
    "    dev_gold = load_gold_standard(dev_file)\n",
    "    \n",
    "    print(f\"Evaluating on {len(dev_terms_list)} terms from {dev_file}\\n\")\n",
    "    \n",
    "    # Save dev terms to temp file for pipeline\n",
    "    import tempfile\n",
    "    import os\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "        pd.DataFrame({\"term\": dev_terms_list}).to_csv(f.name, index=False)\n",
    "        temp_input_file = f.name\n",
    "    \n",
    "    # Run pipeline on dev set with dev in filename\n",
    "    dev_output_file = f\"{config.OUTPUT_PREFIX}_dev.{config.OUTPUT_FORMAT}\"\n",
    "    dev_clustering = run_pipeline(\n",
    "        input_file=temp_input_file,\n",
    "        method=method,\n",
    "        output_file=dev_output_file,  # Save dev predictions with \"dev\" in name\n",
    "        gold_standard_file=None,  # We'll evaluate manually\n",
    "        use_llm=use_llm,\n",
    "        use_trained_params=True  # Use trained parameters\n",
    "    )\n",
    "    \n",
    "    # Evaluate on dev set\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DEVELOPMENT SET EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    dev_metrics = evaluate_clustering(dev_gold, dev_clustering, verbose=True)\n",
    "    \n",
    "    # Print summary of both train and dev results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING AND EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Dataset':<20} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    if train_metrics:\n",
    "        print(f\"{'Training Set':<20} {train_metrics['precision']:<12.4f} {train_metrics['recall']:<12.4f} {train_metrics['f1']:<12.4f}\")\n",
    "    print(f\"{'Development Set':<20} {dev_metrics['precision']:<12.4f} {dev_metrics['recall']:<12.4f} {dev_metrics['f1']:<12.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\"train\": train_metrics, \"dev\": dev_metrics} if train_metrics else {\"dev\": dev_metrics} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(input_file: str, method: str = \"hybrid\", output_file: str = None,\n",
    "                 gold_standard_file: str = None, use_llm: bool = False,\n",
    "                 use_trained_params: bool = True) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Run the complete clustering pipeline.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input file with terms\n",
    "        method: Clustering method (\"hybrid\", \"embedding\", \"llm\", \"lemma\", \"strict\")\n",
    "        output_file: Path to save output (optional)\n",
    "        gold_standard_file: Path to gold standard for evaluation (optional)\n",
    "        use_llm: Whether to use LLM refinement (for hybrid method)\n",
    "        use_trained_params: Whether to load trained parameters from file (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping terms to cluster IDs\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Load trained parameters if available and requested\n",
    "    if use_trained_params:\n",
    "        trained_params = load_trained_parameters()\n",
    "        if trained_params and trained_params.get('method') != method:\n",
    "            print(f\"WARNING: Trained parameters were for method '{trained_params['method']}', but using '{method}'\")\n",
    "            print(\"   Consider using the same method or retraining.\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running clustering pipeline\")\n",
    "    print(f\"Method: {method}\")\n",
    "    if use_trained_params and os.path.exists(\"trained_parameters.json\"):\n",
    "        print(f\"Using trained parameters (threshold: {config.SIMILARITY_THRESHOLD:.3f})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load terms\n",
    "    print(\"Loading terms...\")\n",
    "    terms_set = load_terms(input_file)\n",
    "    terms_list = sorted(list(terms_set))  # Sort for reproducibility\n",
    "    print(f\"Loaded {len(terms_list)} unique terms\")\n",
    "    \n",
    "    # Initialize clusterers\n",
    "    clustering = None\n",
    "    \n",
    "    if method == \"lemma\":\n",
    "        # Simple lemma-based clustering\n",
    "        print(\"Using lemma-based clustering...\")\n",
    "        lemma_groups = preprocessor.group_by_lemma(terms_set)\n",
    "        clustering = {}\n",
    "        for cluster_id, (lemma, lemma_terms) in enumerate(lemma_groups.items(), start=1):\n",
    "            for term in lemma_terms:\n",
    "                clustering[term] = cluster_id\n",
    "    \n",
    "    elif method == \"embedding\":\n",
    "        # Embedding-based clustering\n",
    "        print(\"Using embedding-based clustering...\")\n",
    "        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            raise ValueError(\"sentence-transformers required for embedding method\")\n",
    "        \n",
    "        embedding_clusterer = EmbeddingClusterer(\n",
    "            similarity_threshold=config.SIMILARITY_THRESHOLD\n",
    "        )\n",
    "        clustering = embedding_clusterer.cluster_by_similarity(terms_list)\n",
    "    \n",
    "    elif method == \"llm\":\n",
    "        # LLM-based clustering\n",
    "        print(\"Using LLM-based clustering...\")\n",
    "        if not TRANSFORMERS_AVAILABLE:\n",
    "            raise ValueError(\"transformers required for LLM method. Install with: pip install transformers torch\")\n",
    "        \n",
    "        llm_clusterer = LLMClusterer(\n",
    "            batch_size=config.LLM_BATCH_SIZE,\n",
    "            use_local=config.LLM_USE_LOCAL\n",
    "        )\n",
    "        if llm_clusterer.pipeline is None and not hasattr(llm_clusterer, 'client'):\n",
    "            raise ValueError(\"LLM model could not be loaded. Try using 'hybrid' or 'embedding' methods instead.\")\n",
    "        clustering = llm_clusterer.cluster_terms(terms_list)\n",
    "    \n",
    "    elif method == \"hybrid\":\n",
    "        # Hybrid approach\n",
    "        print(\"Using hybrid clustering...\")\n",
    "        \n",
    "        embedding_clusterer = None\n",
    "        if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            embedding_clusterer = EmbeddingClusterer(\n",
    "                similarity_threshold=config.SIMILARITY_THRESHOLD\n",
    "            )\n",
    "        \n",
    "        llm_clusterer = None\n",
    "        if use_llm and TRANSFORMERS_AVAILABLE:\n",
    "            llm_clusterer = LLMClusterer(\n",
    "                batch_size=config.LLM_BATCH_SIZE,\n",
    "                use_local=config.LLM_USE_LOCAL\n",
    "            )\n",
    "        \n",
    "        hybrid_clusterer = HybridClusterer(\n",
    "            preprocessor=preprocessor,\n",
    "            embedding_clusterer=embedding_clusterer,\n",
    "            llm_clusterer=llm_clusterer\n",
    "        )\n",
    "        clustering = hybrid_clusterer.cluster(terms_list, use_llm_refinement=use_llm)\n",
    "    \n",
    "    elif method == \"strict\":\n",
    "        # Strict hybrid clustering with anti-monster cluster system\n",
    "        print(\"Using strict hybrid clustering (anti-monster cluster pipeline)...\")\n",
    "        \n",
    "        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            raise ValueError(\"sentence-transformers required for strict method\")\n",
    "        \n",
    "        embedding_clusterer = EmbeddingClusterer(\n",
    "            similarity_threshold=config.STRICT_SIMILARITY_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        llm_clusterer = None\n",
    "        if config.USE_LLM_VERIFICATION and TRANSFORMERS_AVAILABLE:\n",
    "            llm_clusterer = LLMClusterer(\n",
    "                batch_size=config.LLM_BATCH_SIZE,\n",
    "                use_local=config.LLM_USE_LOCAL\n",
    "            )\n",
    "        \n",
    "        strict_clusterer = StrictHybridClusterer(\n",
    "            preprocessor=preprocessor,\n",
    "            embedding_clusterer=embedding_clusterer,\n",
    "            llm_clusterer=llm_clusterer,\n",
    "            similarity_threshold=config.STRICT_SIMILARITY_THRESHOLD,\n",
    "            monster_threshold=config.MONSTER_CLUSTER_THRESHOLD,\n",
    "            use_llm_verification=config.USE_LLM_VERIFICATION\n",
    "        )\n",
    "        clustering = strict_clusterer.cluster(terms_list)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Available methods: 'lemma', 'embedding', 'llm', 'hybrid', 'strict'\")\n",
    "    \n",
    "    # Ensure all terms are clustered\n",
    "    missing_terms = set(terms_list) - set(clustering.keys())\n",
    "    if missing_terms:\n",
    "        print(f\"Warning: {len(missing_terms)} terms not clustered. Adding as singletons...\")\n",
    "        max_cluster_id = max(clustering.values()) if clustering else 0\n",
    "        for i, term in enumerate(missing_terms, start=1):\n",
    "            clustering[term] = max_cluster_id + i\n",
    "    \n",
    "    # Evaluate if gold standard provided\n",
    "    if gold_standard_file:\n",
    "        print(\"\\nEvaluating results...\")\n",
    "        gold = load_gold_standard(gold_standard_file)\n",
    "        metrics = evaluate_clustering(gold, clustering)\n",
    "    \n",
    "    # Save output if output_file\n",
    "    if output_file:\n",
    "        import os\n",
    "        \n",
    "        # Check if running on dev data and add \"dev\" to filename\n",
    "        is_dev_data = False\n",
    "        if gold_standard_file and 'dev' in gold_standard_file.lower():\n",
    "            is_dev_data = True\n",
    "        elif input_file and 'dev' in input_file.lower():\n",
    "            is_dev_data = True\n",
    "        elif os.path.abspath(input_file) == os.path.abspath(config.DEV_PATH):\n",
    "            is_dev_data = True\n",
    "        \n",
    "        # Add \"dev\" to filename if running on dev data\n",
    "        if is_dev_data and 'dev' not in output_file.lower():\n",
    "            # Insert \"dev\" before the extension\n",
    "            if output_file.endswith('.csv'):\n",
    "                output_file = output_file.replace('.csv', '_dev.csv')\n",
    "            elif output_file.endswith('.json'):\n",
    "                output_file = output_file.replace('.json', '_dev.json')\n",
    "            else:\n",
    "                output_file = f\"{output_file}_dev.{config.OUTPUT_FORMAT}\"\n",
    "        \n",
    "        # Ensure file extension matches format\n",
    "        if output_file.endswith('.json') and config.OUTPUT_FORMAT == 'csv':\n",
    "            output_file = output_file.replace('.json', '.csv')\n",
    "        elif output_file.endswith('.csv') and config.OUTPUT_FORMAT == 'json':\n",
    "            output_file = output_file.replace('.csv', '.json')\n",
    "        elif not output_file.endswith(('.csv', '.json')):\n",
    "            # Add extension based on format\n",
    "            output_file = f\"{output_file}.{config.OUTPUT_FORMAT}\"\n",
    "        \n",
    "        save_output(clustering, output_file, format=config.OUTPUT_FORMAT)\n",
    "    else:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"WARNING: No output file specified!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"   Results were computed but NOT saved to file.\")\n",
    "        print(\"   To save your predictions, provide 'output_file' parameter:\")\n",
    "        print(\"   Example: run_pipeline(..., output_file='my_predictions.csv')\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nClustering Statistics:\")\n",
    "    print(f\"Total terms: {len(clustering)}\")\n",
    "    print(f\"Number of clusters: {len(set(clustering.values()))}\")\n",
    "    cluster_sizes = Counter(clustering.values())\n",
    "    print(f\"Average cluster size: {np.mean(list(cluster_sizes.values())):.2f}\")\n",
    "    print(f\"Largest cluster size: {max(cluster_sizes.values())}\")\n",
    "    print(f\"Singleton clusters: {sum(1 for size in cluster_sizes.values() if size == 1)}\")\n",
    "    \n",
    "    return clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using Strict Clustering (Anti-Monster Cluster Pipeline)\n",
    "\n",
    "The strict clustering method uses 4 modules to prevent monster clusters and improve precision:\n",
    "1. Acronym & Substring Solver\n",
    "2. Refined Embedding Clustering (threshold 0.90, domain stop-word penalty, antonym guardrail)\n",
    "3. Monster Cluster Breaker\n",
    "4. LLM Edge Verification (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Strict Clustering (Anti-Monster Cluster Pipeline)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: strict\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using strict hybrid clustering (anti-monster cluster pipeline)...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 46175de0-0dcf-47d2-88ef-8b36dc6aedf5)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "================================================================================\n",
      "STRICT HYBRID CLUSTERING PIPELINE\n",
      "================================================================================\n",
      "Clustering 242 terms with anti-monster cluster system...\n",
      "Similarity threshold: 0.90\n",
      "Monster cluster threshold: 10 items\n",
      "LLM verification: OFF\n",
      "================================================================================\n",
      "\n",
      "Module 1: Acronym & Substring Solver\n",
      "  Processing 242 terms...\n",
      "   Created 41 locked clusters from acronym/substring matches\n",
      "   137 terms locked\n",
      "\n",
      "Computing embeddings...\n",
      "Computing embeddings for 242 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83f803527324a48b3be94a345d53de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Module 2: Refined Embedding Clustering (Anti-Lumping)\n",
      "  Using strict threshold: 0.90\n",
      "   Merged 0 cluster pairs (strict threshold applied)\n",
      "   Final clusters: 146\n",
      "\n",
      "Module 3: Monster Cluster Breaker\n",
      "   No monster clusters found (all clusters  10 items)\n",
      "\n",
      "Module 4: LLM Edge Verification - SKIPPED (use_llm_verification=False)\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING COMPLETE\n",
      "================================================================================\n",
      "Total clusters: 146\n",
      "Average cluster size: 1.66\n",
      "Largest cluster: 7 items\n",
      "Singleton clusters: 105\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Evaluating results...\n",
      "BCubed Precision: 0.7113\n",
      "BCubed Recall: 0.7192\n",
      "BCubed F1 Score: 0.7152\n",
      "\n",
      "============================================================\n",
      " PREDICTION FILE SAVED!\n",
      "============================================================\n",
      "   Location: /Users/fashad/Desktop/SUBTASK-B/production_run_strict_dev.csv\n",
      "   Format: CSV\n",
      "   Total terms: 242\n",
      "   Total clusters: 146\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 146\n",
      "Average cluster size: 1.66\n",
      "Largest cluster size: 7\n",
      "Singleton clusters: 105\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: Strict vs Hybrid\n",
      "================================================================================\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: hybrid\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using hybrid clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 242 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507ffd746f434d28b63855cc51c2e19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 242 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3728a5ad63a842c08be96ec0639e3b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating results...\n",
      "BCubed Precision: 0.6481\n",
      "BCubed Recall: 0.6908\n",
      "BCubed F1 Score: 0.6688\n",
      "\n",
      "============================================================\n",
      "  WARNING: No output file specified!\n",
      "============================================================\n",
      "   Results were computed but NOT saved to file.\n",
      "   To save your predictions, provide 'output_file' parameter:\n",
      "   Example: run_pipeline(..., output_file='my_predictions.csv')\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 140\n",
      "Average cluster size: 1.73\n",
      "Largest cluster size: 24\n",
      "Singleton clusters: 112\n",
      "\n",
      "Method               Precision    Recall       F1 Score    \n",
      "--------------------------------------------------------------------------------\n",
      "Strict               0.7113       0.7192       0.7152      \n",
      "Hybrid               0.6481       0.6908       0.6688      \n",
      "================================================================================\n",
      "\n",
      "Cluster Size Statistics:\n",
      "Metric                         Strict          Hybrid         \n",
      "------------------------------------------------------------\n",
      "Total clusters                 146             140            \n",
      "Average cluster size           1.66            1.73           \n",
      "Largest cluster                7               24             \n",
      "Clusters > 10 items            0               2              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: Using Strict Clustering Method\n",
    "print(\"Example: Strict Clustering (Anti-Monster Cluster Pipeline)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run strict clustering on dev set\n",
    "clustering_strict = run_pipeline(\n",
    "    input_file=config.DEV_PATH,\n",
    "    method=\"strict\",  # Use the new strict method\n",
    "    output_file=\"production_run_strict_dev.csv\",\n",
    "    gold_standard_file=config.DEV_PATH,\n",
    "    use_llm=False,  # LLM verification is controlled by config.USE_LLM_VERIFICATION\n",
    "    use_trained_params=False  # Strict method uses its own threshold (0.90)\n",
    ")\n",
    "\n",
    "# Compare with regular hybrid method\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: Strict vs Hybrid\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run regular hybrid for comparison\n",
    "clustering_hybrid = run_pipeline(\n",
    "    input_file=config.DEV_PATH,\n",
    "    method=\"hybrid\",\n",
    "    output_file=None,  # Don't save, just for comparison\n",
    "    gold_standard_file=config.DEV_PATH,\n",
    "    use_llm=False,\n",
    "    use_trained_params=True\n",
    ")\n",
    "\n",
    "# Evaluate both\n",
    "gold = load_gold_standard(config.DEV_PATH)\n",
    "strict_metrics = evaluate_clustering(gold, clustering_strict, verbose=False)\n",
    "hybrid_metrics = evaluate_clustering(gold, clustering_hybrid, verbose=False)\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'Precision':<12} {'Recall':<12} {'F1 Score':<12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Strict':<20} {strict_metrics['precision']:<12.4f} {strict_metrics['recall']:<12.4f} {strict_metrics['f1']:<12.4f}\")\n",
    "print(f\"{'Hybrid':<20} {hybrid_metrics['precision']:<12.4f} {hybrid_metrics['recall']:<12.4f} {hybrid_metrics['f1']:<12.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show cluster size distribution\n",
    "from collections import Counter\n",
    "strict_sizes = Counter(clustering_strict.values())\n",
    "hybrid_sizes = Counter(clustering_hybrid.values())\n",
    "\n",
    "print(f\"\\nCluster Size Statistics:\")\n",
    "print(f\"{'Metric':<30} {'Strict':<15} {'Hybrid':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total clusters':<30} {len(strict_sizes):<15} {len(hybrid_sizes):<15}\")\n",
    "print(f\"{'Average cluster size':<30} {np.mean(list(strict_sizes.values())):<15.2f} {np.mean(list(hybrid_sizes.values())):<15.2f}\")\n",
    "print(f\"{'Largest cluster':<30} {max(strict_sizes.values()):<15} {max(hybrid_sizes.values()):<15}\")\n",
    "print(f\"{'Clusters > 10 items':<30} {sum(1 for s in strict_sizes.values() if s > 10):<15} {sum(1 for s in hybrid_sizes.values() if s > 10):<15}\")\n",
    "print(\"=\" * 80) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(gold: Dict[str, int], pred: Dict[str, int], top_n: int = 20):\n",
    "    \"\"\"\n",
    "    Analyze clustering errors and identify common issues.\n",
    "    \"\"\"\n",
    "    calc = BCubedCalculator(gold, pred)\n",
    "    \n",
    "    # Calculate per-item scores\n",
    "    item_scores = {}\n",
    "    for item in set(list(gold.keys()) + list(pred.keys())):\n",
    "        precision = calc.bc_precision_item(item)\n",
    "        recall = calc.bc_recall_item(item)\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        item_scores[item] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    # Find worst-performing terms\n",
    "    sorted_items = sorted(item_scores.items(), key=lambda x: x[1][\"f1\"])\n",
    "    \n",
    "    print(f\"\\nTop {top_n} worst-performing terms:\")\n",
    "    print(\"-\" * 80)\n",
    "    for term, scores in sorted_items[:top_n]:\n",
    "        gold_cluster = gold.get(term, \"N/A\")\n",
    "        pred_cluster = pred.get(term, \"N/A\")\n",
    "        print(f\"Term: {term}\")\n",
    "        print(f\"  Gold cluster: {gold_cluster}, Predicted cluster: {pred_cluster}\")\n",
    "        print(f\"  Precision: {scores['precision']:.3f}, Recall: {scores['recall']:.3f}, F1: {scores['f1']:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    return item_scores\n",
    "\n",
    "def visualize_clusters(terms: List[str], clustering: Dict[str, int],\n",
    "                      embeddings: np.ndarray = None, max_clusters: int = 50):\n",
    "    \"\"\"\n",
    "    Visualize clusters using t-SNE or PCA.\n",
    "    \"\"\"\n",
    "    if embeddings is None:\n",
    "        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            print(\"Cannot visualize: embeddings not available\")\n",
    "            return\n",
    "        \n",
    "        model = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "        embeddings = model.encode(terms, show_progress_bar=True)\n",
    "    \n",
    "    # Reduce dimensionality for visualization...\n",
    "    print(\"Reducing dimensionality for visualization...\")\n",
    "    if len(terms) > 50:\n",
    "        reducer = PCA(n_components=50)\n",
    "        embeddings_reduced = reducer.fit_transform(embeddings)\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(terms)-1))\n",
    "        embeddings_2d = tsne.fit_transform(embeddings_reduced)\n",
    "    else:\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    cluster_ids = [clustering.get(term, -1) for term in terms]\n",
    "    unique_clusters = sorted(set(cluster_ids))\n",
    "    \n",
    "    if len(unique_clusters) > max_clusters:\n",
    "        print(f\"Too many clusters ({len(unique_clusters)}). Showing top {max_clusters}...\")\n",
    "        cluster_counts = Counter(cluster_ids)\n",
    "        top_clusters = [cid for cid, _ in cluster_counts.most_common(max_clusters)]\n",
    "        mask = [cid in top_clusters for cid in cluster_ids]\n",
    "        embeddings_2d = embeddings_2d[mask]\n",
    "        cluster_ids = [cid for cid, m in zip(cluster_ids, mask) if m]\n",
    "        terms_filtered = [t for t, m in zip(terms, mask) if m]\n",
    "    else:\n",
    "        terms_filtered = terms\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "                         c=cluster_ids, cmap='tab20', alpha=0.6, s=50)\n",
    "    plt.colorbar(scatter, label='Cluster ID')\n",
    "    plt.title('Term Clusters Visualization (t-SNE)')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4. Display Current Performance Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick function to display current performance on train and dev sets\n",
    "def show_performance_scores(method: str = \"hybrid\", use_trained_params: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluate and display BCubed F1 scores on both training and development sets.\n",
    "    \n",
    "    Args:\n",
    "        method: Clustering method to use\n",
    "        use_trained_params: Whether to use trained parameters\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" PERFORMANCE EVALUATION ON TRAIN & DEV SETS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    print(\"Evaluating on Training Set...\")\n",
    "    train_terms = load_terms(config.TRAIN_PATH)\n",
    "    train_terms_list = sorted(list(train_terms))\n",
    "    train_gold = load_gold_standard(config.TRAIN_PATH)\n",
    "    \n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "        pd.DataFrame({\"term\": train_terms_list}).to_csv(f.name, index=False)\n",
    "        temp_train_file = f.name\n",
    "    \n",
    "    train_clustering = run_pipeline(\n",
    "        input_file=temp_train_file,\n",
    "        method=method,\n",
    "        output_file=None,\n",
    "        gold_standard_file=None,\n",
    "        use_llm=False,\n",
    "        use_trained_params=use_trained_params\n",
    "    )\n",
    "    \n",
    "    train_metrics = evaluate_clustering(train_gold, train_clustering, verbose=False)\n",
    "    \n",
    "    # Evaluate on development set\n",
    "    print(\"\\nEvaluating on Development Set...\")\n",
    "    dev_terms = load_terms(config.DEV_PATH)\n",
    "    dev_terms_list = sorted(list(dev_terms))\n",
    "    dev_gold = load_gold_standard(config.DEV_PATH)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "        pd.DataFrame({\"term\": dev_terms_list}).to_csv(f.name, index=False)\n",
    "        temp_dev_file = f.name\n",
    "    \n",
    "    dev_clustering = run_pipeline(\n",
    "        input_file=temp_dev_file,\n",
    "        method=method,\n",
    "        output_file=None,\n",
    "        gold_standard_file=None,\n",
    "        use_llm=False,\n",
    "        use_trained_params=use_trained_params\n",
    "    )\n",
    "    \n",
    "    dev_metrics = evaluate_clustering(dev_gold, dev_clustering, verbose=False)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" FINAL PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Dataset':<25} {'Precision':<15} {'Recall':<15} {'F1 Score':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Training Set':<25} {train_metrics['precision']:<15.4f} {train_metrics['recall']:<15.4f} {train_metrics['f1']:<15.4f}\")\n",
    "    print(f\"{'Development Set':<25} {dev_metrics['precision']:<15.4f} {dev_metrics['recall']:<15.4f} {dev_metrics['f1']:<15.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\"train\": train_metrics, \"dev\": dev_metrics}\n",
    "\n",
    "# Uncomment to see current performance:\n",
    "# scores = show_performance_scores(method=\"hybrid\", use_trained_params=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Example Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5. Training and Evaluation on Train/Dev Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Evaluation Example\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING AND EVALUATION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Training on training set...\n",
      "\n",
      "============================================================\n",
      "Training: Finding Optimal Similarity Threshold\n",
      "============================================================\n",
      "\n",
      "Training on 713 terms from subtask_b_train.csv\n",
      "Trying thresholds: [0.65, 0.7, 0.75, 0.8, 0.85]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e16e9653f1d4d1bb83b6caa22d1cd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing thresholds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32c883ebd3a4fe3b2b38cbf1e50eb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15af0795db674be4a566eb270bfbc040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ce9a2ac6bf4d2ca4be9877cfcfc7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61983648a03456ea5b9105a5d67e17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5249d9e11b774d929fb7f1b7d9b6e989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945ffeb87cfc4bf2ba665905072b5fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968964f0e8954c49ac06c34ba9816c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10f2b6a6bc84754a88a5c11ebc8af31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0035022cf0b4440dbf9158fccab448e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251325e613bb4486a9aa73dcc7732697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " TRAINING SET EVALUATION RESULTS\n",
      "================================================================================\n",
      "Threshold    Precision    Recall       F1 Score    \n",
      "--------------------------------------------------------------------------------\n",
      "0.65         0.0487       0.9752       0.0928      \n",
      "0.70         0.0971       0.9280       0.1758      \n",
      "0.75         0.1830       0.8500       0.3012      \n",
      "0.80         0.3346       0.7498       0.4627      \n",
      "0.85         0.5107       0.6186       0.5595        BEST\n",
      "\n",
      "================================================================================\n",
      " Best threshold: 0.850\n",
      " Training Set BCubed F1 Score: 0.5595\n",
      "================================================================================\n",
      "\n",
      " Trained parameters saved to: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  These parameters will be used when running on test data.\n",
      "\n",
      " Optimal threshold learned: 0.85\n",
      "\n",
      "Evaluating on training set with optimal threshold...\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: hybrid\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 713 unique terms\n",
      "Using hybrid clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf5f93c44d34be8a8b8fca1d23b02ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5603ee0c54ec436fb0809f28362ffdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  WARNING: No output file specified!\n",
      "============================================================\n",
      "   Results were computed but NOT saved to file.\n",
      "   To save your predictions, provide 'output_file' parameter:\n",
      "   Example: run_pipeline(..., output_file='my_predictions.csv')\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 713\n",
      "Number of clusters: 273\n",
      "Average cluster size: 2.61\n",
      "Largest cluster size: 146\n",
      "Singleton clusters: 208\n",
      "Training Set - Precision: 0.5107, Recall: 0.6186, F1: 0.5595\n",
      "\n",
      "STEP 2: Evaluating on development set...\n",
      "Evaluating on 242 terms from subtask_b_dev.csv\n",
      "\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: hybrid\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using hybrid clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 242 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdadaf57ae84030928da99665b5890e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 242 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09da916d25945f385a6690207bcb728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PREDICTION FILE SAVED!\n",
      "============================================================\n",
      "   Location: /Users/fashad/Desktop/SUBTASK-B/production_run_dev.csv\n",
      "   Format: CSV\n",
      "   Total terms: 242\n",
      "   Total clusters: 140\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 140\n",
      "Average cluster size: 1.73\n",
      "Largest cluster size: 24\n",
      "Singleton clusters: 112\n",
      "\n",
      "================================================================================\n",
      " DEVELOPMENT SET EVALUATION\n",
      "================================================================================\n",
      "BCubed Precision: 0.6481\n",
      "BCubed Recall: 0.6908\n",
      "BCubed F1 Score: 0.6688\n",
      "\n",
      "================================================================================\n",
      " TRAINING AND EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Dataset              Precision    Recall       F1 Score    \n",
      "--------------------------------------------------------------------------------\n",
      "Training Set         0.5107       0.6186       0.5595      \n",
      "Development Set      0.6481       0.6908       0.6688      \n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      " FINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      " Training Set BCubed F1:   0.5595\n",
      " Development Set BCubed F1: 0.6688\n",
      "\n",
      "Detailed Metrics:\n",
      "  Training Set   - Precision: 0.5107, Recall: 0.6186\n",
      "  Development Set - Precision: 0.6481, Recall: 0.6908\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Train on training set and evaluate on development set\n",
    "print(\"Training and Evaluation Example\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train on training set and evaluate on dev set\n",
    "results = train_and_evaluate(\n",
    "    train_file=config.TRAIN_PATH,\n",
    "    dev_file=config.DEV_PATH,\n",
    "    method=\"hybrid\",\n",
    "    optimize_threshold=True,  # Optimize similarity threshold on training set\n",
    "    use_llm=False  # Set to True if you want LLM refinement\n",
    ")\n",
    "\n",
    "# Results summary (detailed summary already displayed in train_and_evaluate)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" FINAL RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "if isinstance(results, dict) and 'train' in results:\n",
    "    print(f\" Training Set BCubed F1: {results['train']['f1']:.4f}\")\n",
    "    print(f\" Development Set BCubed F1: {results['dev']['f1']:.4f}\")\n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(f\" Training Set - Precision: {results['train']['precision']:.4f}, Recall: {results['train']['recall']:.4f}\")\n",
    "    print(f\" Development Set - Precision: {results['dev']['precision']:.4f}, Recall: {results['dev']['recall']:.4f}\")\n",
    "elif isinstance(results, dict) and 'dev' in results:\n",
    "    print(f\" Development Set BCubed F1: {results['dev']['f1']:.4f}\")\n",
    "    print(f\" Precision: {results['dev']['precision']:.4f}\")\n",
    "    print(f\" Recall: {results['dev']['recall']:.4f}\")\n",
    "else:\n",
    "    print(f\" Development Set BCubed F1: {results['f1']:.4f}\")\n",
    "    print(f\" Precision: {results['precision']:.4f}\")\n",
    "    print(f\" Recall: {results['recall']:.4f}\")\n",
    "print(f\"{'='*80}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Hybrid Clustering on Dev Set\n",
      "============================================================\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: hybrid\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using hybrid clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 242 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581327b520b44019a8061a6a5c85987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 242 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56504b095984e9baf72c32967176428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating results...\n",
      "BCubed Precision: 0.6481\n",
      "BCubed Recall: 0.6908\n",
      "BCubed F1 Score: 0.6688\n",
      "\n",
      "============================================================\n",
      " PREDICTION FILE SAVED!\n",
      "============================================================\n",
      "   Location: /Users/fashad/Desktop/SUBTASK-B/production_run_hybrid_dev.csv\n",
      "   Format: CSV\n",
      "   Total terms: 242\n",
      "   Total clusters: 140\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 140\n",
      "Average cluster size: 1.73\n",
      "Largest cluster size: 24\n",
      "Singleton clusters: 112\n",
      "\n",
      "Clustering complete!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Run hybrid clustering on dev set\n",
    "print(\"Example 1: Hybrid Clustering on Dev Set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For this example, we'll use the dev set as input (terms only)\n",
    "# In practice, you would extract unique terms from Subtask A output\n",
    "\n",
    "# Load dev set to get terms\n",
    "dev_terms = load_terms(config.DEV_PATH)\n",
    "dev_terms_list = sorted(list(dev_terms))\n",
    "\n",
    "# Save terms to a temporary file (simulating Subtask A output)\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "    pd.DataFrame({\"term\": dev_terms_list}).to_csv(f.name, index=False)\n",
    "    temp_input_file = f.name\n",
    "\n",
    "# Run pipeline\n",
    "clustering_result = run_pipeline(\n",
    "    input_file=temp_input_file,\n",
    "    method=\"hybrid\",\n",
    "    output_file=f\"{config.OUTPUT_PREFIX}_hybrid.json\",\n",
    "    gold_standard_file=config.DEV_PATH if config.EVALUATE_ON_DEV else None,\n",
    "    use_llm=False  # Set to True if you want to use LLM (requires transformers and model download)\n",
    ")\n",
    "\n",
    "print(\"\\nClustering complete!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2: Comparing Different Methods\n",
      "============================================================\n",
      "\n",
      "Running lemma method...\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "  Warning: Trained parameters were for method 'hybrid', but using 'lemma'\n",
      "   Consider using the same method or retraining.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: lemma\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using lemma-based clustering...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b8d94357b74dc39f4a72b91710642e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating results...\n",
      "BCubed Precision: 0.9862\n",
      "BCubed Recall: 0.6218\n",
      "BCubed F1 Score: 0.7627\n",
      "\n",
      "============================================================\n",
      "  WARNING: No output file specified!\n",
      "============================================================\n",
      "   Results were computed but NOT saved to file.\n",
      "   To save your predictions, provide 'output_file' parameter:\n",
      "   Example: run_pipeline(..., output_file='my_predictions.csv')\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 232\n",
      "Average cluster size: 1.04\n",
      "Largest cluster size: 3\n",
      "Singleton clusters: 224\n",
      "lemma - F1: 0.7627\n",
      "\n",
      "Running embedding method...\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "  Warning: Trained parameters were for method 'hybrid', but using 'embedding'\n",
      "   Consider using the same method or retraining.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: embedding\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using embedding-based clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Computing embeddings for 242 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471cd7186ce946189424614bec2b169a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating results...\n",
      "BCubed Precision: 0.7652\n",
      "BCubed Recall: 0.6675\n",
      "BCubed F1 Score: 0.7130\n",
      "\n",
      "============================================================\n",
      "  WARNING: No output file specified!\n",
      "============================================================\n",
      "   Results were computed but NOT saved to file.\n",
      "   To save your predictions, provide 'output_file' parameter:\n",
      "   Example: run_pipeline(..., output_file='my_predictions.csv')\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 167\n",
      "Average cluster size: 1.45\n",
      "Largest cluster size: 6\n",
      "Singleton clusters: 128\n",
      "embedding - F1: 0.7130\n",
      "\n",
      "Method Comparison:\n",
      "------------------------------------------------------------\n",
      "lemma           - Precision: 0.9862, Recall: 0.6218, F1: 0.7627\n",
      "embedding       - Precision: 0.7652, Recall: 0.6675, F1: 0.7130\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Compare different methods\n",
    "print(\"\\nExample 2: Comparing Different Methods\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "methods = [\"lemma\", \"embedding\"]  # Add \"llm\" if you have transformers installed\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nRunning {method} method...\")\n",
    "    try:\n",
    "        clustering = run_pipeline(\n",
    "            input_file=temp_input_file,\n",
    "            method=method,\n",
    "            gold_standard_file=config.DEV_PATH if config.EVALUATE_ON_DEV else None,\n",
    "            use_llm=False\n",
    "        )\n",
    "        \n",
    "        if config.EVALUATE_ON_DEV:\n",
    "            gold = load_gold_standard(config.DEV_PATH)\n",
    "            metrics = evaluate_clustering(gold, clustering, verbose=False)\n",
    "            results[method] = metrics\n",
    "            print(f\"{method} - F1: {metrics['f1']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {method}: {e}\")\n",
    "\n",
    "# Print comparison\n",
    "if results:\n",
    "    print(\"\\nMethod Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    for method, metrics in results.items():\n",
    "        print(f\"{method:15s} - Precision: {metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {metrics['recall']:.4f}, F1: {metrics['f1']:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3: Error Analysis\n",
      "============================================================\n",
      "\n",
      "Top 10 worst-performing terms:\n",
      "--------------------------------------------------------------------------------\n",
      "Term: depositare i rifiuti\n",
      "  Gold cluster: 88, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.167, F1: 0.067\n",
      "\n",
      "Term: servizio di raccolta dei rifiuti\n",
      "  Gold cluster: 22, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.200, F1: 0.069\n",
      "\n",
      "Term: conferimento separato dei rifiuti\n",
      "  Gold cluster: 3, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.250, F1: 0.071\n",
      "\n",
      "Term: rifiuti\n",
      "  Gold cluster: 37, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.333, F1: 0.074\n",
      "\n",
      "Term: rifiuti urbani pericolosi\n",
      "  Gold cluster: 55, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.500, F1: 0.077\n",
      "\n",
      "Term: rifiuti ingombranti\n",
      "  Gold cluster: 47, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.500, F1: 0.077\n",
      "\n",
      "Term: gestore dello spazzamento e lavaggio\n",
      "  Gold cluster: 305, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.500, F1: 0.077\n",
      "\n",
      "Term: disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato\n",
      "  Gold cluster: 191, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.500, F1: 0.077\n",
      "\n",
      "Term: rifiuti conferibili\n",
      "  Gold cluster: 123, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.500, F1: 0.077\n",
      "\n",
      "Term: tassa sui rifiuti solidi urbani\n",
      "  Gold cluster: 257, Predicted cluster: 94\n",
      "  Precision: 0.042, Recall: 0.500, F1: 0.077\n",
      "\n",
      "\n",
      "Generating visualization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736823e6171b40a49b4b7b6b603c0bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dimensionality for visualization...\n",
      "Too many clusters (60). Showing top 50...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 3: Error Analysis\n",
    "if config.EVALUATE_ON_DEV:\n",
    "    print(\"\\nExample 3: Error Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gold = load_gold_standard(config.DEV_PATH)\n",
    "    item_scores = analyze_errors(gold, clustering_result, top_n=10)\n",
    "    \n",
    "    # Visualize if embeddings available\n",
    "    if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        print(\"\\nGenerating visualization...\")\n",
    "        try:\n",
    "            visualize_clusters(dev_terms_list[:100], clustering_result)  # Limit for visualization\n",
    "        except Exception as e:\n",
    "            print(f\"Visualization error: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Development Dataset Evaluation\n",
    "\n",
    "This section evaluates the pipeline on the development dataset and displays the BCubed F1 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEVELOPMENT DATASET EVALUATION\n",
      "================================================================================\n",
      "\n",
      "This section evaluates the pipeline on the development set.\n",
      "Make sure you have trained on the training set first (Section 11.5).\n",
      "\n",
      "Development Set Statistics:\n",
      "  Total terms: 242\n",
      "  Total gold clusters: 147\n",
      "\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: hybrid\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 242 unique terms\n",
      "Using hybrid clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 242 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf21f2af56f438c81e45219a47f6cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 242 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb79fea3acf843438db9c92334c85385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PREDICTION FILE SAVED!\n",
      "============================================================\n",
      "   Location: /Users/fashad/Desktop/SUBTASK-B/production_run_dev.csv\n",
      "   Format: CSV\n",
      "   Total terms: 242\n",
      "   Total clusters: 140\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 242\n",
      "Number of clusters: 140\n",
      "Average cluster size: 1.73\n",
      "Largest cluster size: 24\n",
      "Singleton clusters: 112\n",
      "\n",
      "================================================================================\n",
      " DEVELOPMENT SET EVALUATION RESULTS\n",
      "================================================================================\n",
      "BCubed Precision: 0.6481\n",
      "BCubed Recall: 0.6908\n",
      "BCubed F1 Score: 0.6688\n",
      "\n",
      "================================================================================\n",
      " DEVELOPMENT SET SUMMARY\n",
      "================================================================================\n",
      "BCubed Precision: 0.6481\n",
      "BCubed Recall:    0.6908\n",
      "BCubed F1 Score:  0.6688\n",
      "================================================================================\n",
      "\n",
      "Comparison with Baseline:\n",
      "  Baseline (from task description): F1 = 0.260\n",
      "  Your Model:                      F1 = 0.6688\n",
      "  Improvement:                      +157.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Development Dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"DEVELOPMENT DATASET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis section evaluates the pipeline on the development set.\")\n",
    "print(\"Make sure you have trained on the training set first (Section 11.5).\\n\")\n",
    "\n",
    "# Load development data\n",
    "dev_terms = load_terms(config.DEV_PATH)\n",
    "dev_terms_list = sorted(list(dev_terms))\n",
    "dev_gold = load_gold_standard(config.DEV_PATH)\n",
    "\n",
    "print(f\"Development Set Statistics:\")\n",
    "print(f\"  Total terms: {len(dev_terms_list)}\")\n",
    "print(f\"  Total gold clusters: {len(set(dev_gold.values()))}\\n\")\n",
    "\n",
    "# Save dev terms to temp file\n",
    "import tempfile\n",
    "import os\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "    pd.DataFrame({\"term\": dev_terms_list}).to_csv(f.name, index=False)\n",
    "    temp_dev_file = f.name\n",
    "\n",
    "# Run pipeline on development set\n",
    "dev_output_file = f\"{config.OUTPUT_PREFIX}_dev.{config.OUTPUT_FORMAT}\"\n",
    "dev_clustering = run_pipeline(\n",
    "    input_file=temp_dev_file,\n",
    "    method=\"hybrid\",  # Use the same method as training\n",
    "    output_file=dev_output_file,\n",
    "    gold_standard_file=None,  # We'll evaluate manually\n",
    "    use_llm=False,\n",
    "    use_trained_params=True  # Use trained parameters\n",
    ")\n",
    "\n",
    "# Evaluate on development set\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" DEVELOPMENT SET EVALUATION RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "dev_metrics = evaluate_clustering(dev_gold, dev_clustering, verbose=True)\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" DEVELOPMENT SET SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"BCubed Precision: {dev_metrics['precision']:.4f}\")\n",
    "print(f\"BCubed Recall: {dev_metrics['recall']:.4f}\")\n",
    "print(f\"BCubed F1 Score: {dev_metrics['f1']:.4f}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Compare with baseline if available\n",
    "print(\"Comparison with Baseline:\")\n",
    "print(\"  Baseline (from task description): F1 = 0.260\")\n",
    "print(f\"  Your Model: F1 = {dev_metrics['f1']:.4f}\")\n",
    "improvement = ((dev_metrics['f1'] - 0.260) / 0.260) * 100\n",
    "print(f\"  Improvement: {improvement:+.1f}%\")\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE PERFORMANCE SUMMARY (TRAIN + DEV)\n",
      "================================================================================\n",
      " Loaded trained parameters from: /Users/fashad/Desktop/SUBTASK-B/trained_parameters.json\n",
      "  Optimal threshold: 0.850\n",
      "  Method: hybrid\n",
      "  Training F1: 0.5595\n",
      "\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: hybrid\n",
      "Using trained parameters (threshold: 0.850)\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 713 unique terms\n",
      "Using hybrid clustering...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "Clustering 713 terms using hybrid approach...\n",
      "Step 1: Lemma-based grouping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfffc2525d847608c34cad5a909e511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing terms:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Embedding-based refinement...\n",
      "Computing embeddings for 713 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d3de10fb3d4a32ab72bab6bec9a0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  WARNING: No output file specified!\n",
      "============================================================\n",
      "   Results were computed but NOT saved to file.\n",
      "   To save your predictions, provide 'output_file' parameter:\n",
      "   Example: run_pipeline(..., output_file='my_predictions.csv')\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 713\n",
      "Number of clusters: 273\n",
      "Average cluster size: 2.61\n",
      "Largest cluster size: 146\n",
      "Singleton clusters: 208\n",
      "\n",
      "Dataset                   Precision       Recall          F1 Score       \n",
      "--------------------------------------------------------------------------------\n",
      "Training Set              0.5107          0.6186          0.5595         \n",
      "Development Set           0.6481          0.6908          0.6688         \n",
      "================================================================================\n",
      "\n",
      " Development Set BCubed F1: 0.6688\n",
      " Training Set BCubed F1:   0.5595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional: Show performance on both train and dev sets side by side\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE PERFORMANCE SUMMARY (TRAIN + DEV)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load training results if available\n",
    "train_metrics = None\n",
    "if os.path.exists(\"trained_parameters.json\"):\n",
    "    import json\n",
    "    with open(\"trained_parameters.json\", 'r') as f:\n",
    "        params = json.load(f)\n",
    "    train_f1 = params.get('best_f1', None)\n",
    "    if train_f1:\n",
    "        # Re-run on training set to get full metrics\n",
    "        train_terms = load_terms(config.TRAIN_PATH)\n",
    "        train_terms_list = sorted(list(train_terms))\n",
    "        train_gold = load_gold_standard(config.TRAIN_PATH)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "            pd.DataFrame({\"term\": train_terms_list}).to_csv(f.name, index=False)\n",
    "            temp_train_file = f.name\n",
    "        \n",
    "        train_clustering = run_pipeline(\n",
    "            input_file=temp_train_file,\n",
    "            method=\"hybrid\",\n",
    "            output_file=None,\n",
    "            gold_standard_file=None,\n",
    "            use_llm=False,\n",
    "            use_trained_params=True\n",
    "        )\n",
    "        \n",
    "        train_metrics = evaluate_clustering(train_gold, train_clustering, verbose=False)\n",
    "\n",
    "# Display comparison table\n",
    "print(f\"\\n{'Dataset':<25} {'Precision':<15} {'Recall':<15} {'F1 Score':<15}\")\n",
    "print(\"-\" * 80)\n",
    "if train_metrics:\n",
    "    print(f\"{'Training Set':<25} {train_metrics['precision']:<15.4f} {train_metrics['recall']:<15.4f} {train_metrics['f1']:<15.4f}\")\n",
    "print(f\"{'Development Set':<25} {dev_metrics['precision']:<15.4f} {dev_metrics['recall']:<15.4f} {dev_metrics['f1']:<15.4f}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n Development Set BCubed F1: {dev_metrics['f1']:.4f}\")\n",
    "if train_metrics:\n",
    "    print(f\" Training Set BCubed F1: {train_metrics['f1']:.4f}\")\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Dataset Evaluation and Prediction Generation\n",
    "\n",
    "This section evaluates the pipeline on the test dataset (`test_data.csv`), generates predictions, and saves them to a CSV file. The test data has a different format (with document metadata), so we extract unique terms first.\n",
    "\n",
    "**Note**: If you have a gold standard file for the test set, you can provide it for evaluation. Otherwise, this section will only generate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST DATASET EVALUATION AND PREDICTION GENERATION\n",
      "================================================================================\n",
      "\n",
      " Loading test data from: test_data.csv\n",
      "   Total rows in test file: 1368\n",
      "   Unique terms extracted: 335\n",
      "\n",
      "   Sample terms (first 10): ['a-porta', 'abbandono', 'abbandono dei rifiuti', 'abbandono di materiali', 'abbandono irregolare dei rifiuti', 'alluminio', \"appalto d'igiene urbana e servizi\", 'aree attrezzate per la raccolta dei rifiuti differenziati', 'aree di raccolta', 'aree di sosta temporanea']\n",
      "\n",
      " Created temporary input file with 335 terms\n",
      "\n",
      "================================================================================\n",
      "RUNNING PIPELINE ON TEST DATA\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Running clustering pipeline\n",
      "Method: strict\n",
      "============================================================\n",
      "\n",
      "Loading terms...\n",
      "Loaded 335 unique terms\n",
      "Using strict hybrid clustering (anti-monster cluster pipeline)...\n",
      "Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Model loaded successfully\n",
      "\n",
      "================================================================================\n",
      "STRICT HYBRID CLUSTERING PIPELINE\n",
      "================================================================================\n",
      "Clustering 335 terms with anti-monster cluster system...\n",
      "Similarity threshold: 0.90\n",
      "Monster cluster threshold: 10 items\n",
      "LLM verification: OFF\n",
      "================================================================================\n",
      "\n",
      "Module 1: Acronym & Substring Solver\n",
      "  Processing 335 terms...\n",
      "   Created 61 locked clusters from acronym/substring matches\n",
      "   196 terms locked\n",
      "\n",
      "Computing embeddings...\n",
      "Computing embeddings for 335 terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f8451d802e4de195caa1e6b166cd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Module 2: Refined Embedding Clustering (Anti-Lumping)\n",
      "  Using strict threshold: 0.90\n",
      "   Merged 0 cluster pairs (strict threshold applied)\n",
      "   Final clusters: 200\n",
      "\n",
      "Module 3: Monster Cluster Breaker\n",
      "   No monster clusters found (all clusters  10 items)\n",
      "\n",
      "Module 4: LLM Edge Verification - SKIPPED (use_llm_verification=False)\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING COMPLETE\n",
      "================================================================================\n",
      "Total clusters: 200\n",
      "Average cluster size: 1.68\n",
      "Largest cluster: 9 items\n",
      "Singleton clusters: 139\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      " PREDICTION FILE SAVED!\n",
      "============================================================\n",
      "   Location: /Users/fashad/Desktop/SUBTASK-B/production_run_test.csv\n",
      "   Format: CSV\n",
      "   Total terms: 335\n",
      "   Total clusters: 200\n",
      "============================================================\n",
      "\n",
      "\n",
      "Clustering Statistics:\n",
      "Total terms: 335\n",
      "Number of clusters: 200\n",
      "Average cluster size: 1.68\n",
      "Largest cluster size: 9\n",
      "Singleton clusters: 139\n",
      "\n",
      "================================================================================\n",
      " TEST SET PREDICTIONS GENERATED\n",
      "================================================================================\n",
      " Predictions saved to: /Users/fashad/Desktop/SUBTASK-B/production_run_test.csv\n",
      " Total terms clustered: 335\n",
      " Total clusters: 200\n",
      "\n",
      "  Note: No gold standard file found for test data.\n",
      "   Predictions have been generated but cannot be evaluated.\n",
      "   To evaluate, provide a gold standard file with one of these names:\n",
      "     - subtask_b_test.csv\n",
      "     - test_gold.csv\n",
      "     - test_data_gold.csv\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Dataset Evaluation and Prediction Generation\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST DATASET EVALUATION AND PREDICTION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Load test data and extract unique terms\n",
    "test_file = \"test_data.csv\"\n",
    "print(f\"\\nLoading test data from: {test_file}\")\n",
    "\n",
    "if not os.path.exists(test_file):\n",
    "    print(f\"ERROR: {test_file} not found!\")\n",
    "    print(\"Please ensure test_data.csv is in the current directory.\")\n",
    "else:\n",
    "    # Read test data\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    print(f\"Total rows in test file: {len(test_df)}\")\n",
    "    \n",
    "    # Extract unique terms (filter out empty/NaN terms)\n",
    "    test_terms = test_df['term'].dropna().unique().tolist()\n",
    "    test_terms = [term.strip() for term in test_terms if term.strip()]\n",
    "    test_terms = sorted(list(set(test_terms)))  # Remove duplicates and sort\n",
    "    \n",
    "    print(f\"Unique terms extracted: {len(test_terms)}\")\n",
    "    print(f\"\\nSample terms (first 10): {test_terms[:10]}\")\n",
    "    \n",
    "    # Step 2: Create temporary input file with just terms\n",
    "    temp_test_input = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "    temp_test_df = pd.DataFrame({\"term\": test_terms})\n",
    "    temp_test_df.to_csv(temp_test_input.name, index=False)\n",
    "    temp_test_input.close()\n",
    "    print(f\"\\nCreated temporary input file with {len(test_terms)} terms\")\n",
    "    \n",
    "    # Step 3: Run pipeline on test data\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RUNNING PIPELINE ON TEST DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate output filename\n",
    "    test_output_file = f\"{config.OUTPUT_PREFIX}_test.{config.OUTPUT_FORMAT}\"\n",
    "    \n",
    "    # Check if gold standard exists for test data\n",
    "    test_gold_file = None\n",
    "    possible_gold_files = [\n",
    "        \"subtask_b_test.csv\",\n",
    "        \"test_gold.csv\",\n",
    "        \"test_data_gold.csv\"\n",
    "    ]\n",
    "    for gold_file in possible_gold_files:\n",
    "        if os.path.exists(gold_file):\n",
    "            test_gold_file = gold_file\n",
    "            print(f\"Found gold standard file: {gold_file}\")\n",
    "            break\n",
    "    \n",
    "    # Run pipeline\n",
    "    test_clustering = run_pipeline(\n",
    "        input_file=temp_test_input.name,\n",
    "        method=\"strict\",  # Use strict method (can be changed)\n",
    "        output_file=test_output_file,\n",
    "        gold_standard_file=test_gold_file,\n",
    "        use_llm=False,  # Set to True if you want LLM refinement (slower)\n",
    "        use_trained_params=True  # Use trained parameters from training set\n",
    "    )\n",
    "    \n",
    "    # Step 4: Evaluate if gold standard is available\n",
    "    if test_gold_file:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\" TEST SET EVALUATION RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        test_gold = load_gold_standard(test_gold_file)\n",
    "        test_metrics = evaluate_clustering(test_gold, test_clustering, verbose=True)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\" FINAL TEST SET RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"BCubed Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"BCubed Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"BCubed F1 Score: {test_metrics['f1']:.4f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Compare with train/dev if available\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\" COMPLETE PERFORMANCE SUMMARY (TRAIN + DEV + TEST)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'Dataset':<25} {'Precision':<15} {'Recall':<15} {'F1 Score':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Try to load train metrics\n",
    "        try:\n",
    "            train_terms = load_terms(config.TRAIN_PATH)\n",
    "            train_terms_list = sorted(list(train_terms))\n",
    "            train_gold = load_gold_standard(config.TRAIN_PATH)\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "                pd.DataFrame({\"term\": train_terms_list}).to_csv(f.name, index=False)\n",
    "                temp_train_file = f.name\n",
    "            train_clustering = run_pipeline(\n",
    "                input_file=temp_train_file,\n",
    "                method=\"hybrid\",\n",
    "                output_file=None,\n",
    "                gold_standard_file=None,\n",
    "                use_llm=False,\n",
    "                use_trained_params=True\n",
    "            )\n",
    "            train_metrics = evaluate_clustering(train_gold, train_clustering, verbose=False)\n",
    "            print(f\"{'Training Set':<25} {train_metrics['precision']:<15.4f} {train_metrics['recall']:<15.4f} {train_metrics['f1']:<15.4f}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Try to load dev metrics\n",
    "        try:\n",
    "            dev_terms = load_terms(config.DEV_PATH)\n",
    "            dev_terms_list = sorted(list(dev_terms))\n",
    "            dev_gold = load_gold_standard(config.DEV_PATH)\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "                pd.DataFrame({\"term\": dev_terms_list}).to_csv(f.name, index=False)\n",
    "                temp_dev_file = f.name\n",
    "            dev_clustering = run_pipeline(\n",
    "                input_file=temp_dev_file,\n",
    "                method=\"hybrid\",\n",
    "                output_file=None,\n",
    "                gold_standard_file=None,\n",
    "                use_llm=False,\n",
    "                use_trained_params=True\n",
    "            )\n",
    "            dev_metrics = evaluate_clustering(dev_gold, dev_clustering, verbose=False)\n",
    "            print(f\"{'Development Set':<25} {dev_metrics['precision']:<15.4f} {dev_metrics['recall']:<15.4f} {dev_metrics['f1']:<15.4f}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(f\"{'Test Set':<25} {test_metrics['precision']:<15.4f} {test_metrics['recall']:<15.4f} {test_metrics['f1']:<15.4f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\" TEST SET PREDICTIONS GENERATED\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Predictions saved to: {os.path.abspath(test_output_file)}\")\n",
    "        print(f\"Total terms clustered: {len(test_clustering)}\")\n",
    "        print(f\"Total clusters: {len(set(test_clustering.values()))}\")\n",
    "        print(f\"\\nWARNING: No gold standard file found for test data.\")\n",
    "        print(\"Predictions have been generated but cannot be evaluated.\")\n",
    "        print(\"To evaluate, provide a gold standard file with one of these names:\")\n",
    "        for gold_file in possible_gold_files:\n",
    "            print(f\"  - {gold_file}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    try:\n",
    "        os.unlink(temp_test_input.name)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
